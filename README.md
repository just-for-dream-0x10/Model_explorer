### 🏗️ 架构设计工作台（Architecture Designer）

**全新模板系统**：22+ 种预设神经网络架构，一键加载即用

#### 📚 模板库（22个模板，5大分类）

**🖼️ CNN 类 (11个)** - 卷积神经网络
- 📱 MNIST CNN - 入门级手写数字识别 🟢
- 🖼️ CIFAR-10 CNN - 彩色图像分类 🟡
- 📜 LeNet-5 - 经典CNN架构 🟢
- 🏆 AlexNet-like - ImageNet冠军网络 🟡
- 🏛️ VGG-like - 深度小卷积核网络 🟡
- 🐣 微型CNN - 超轻量级网络 🟢
- 📐 宽型CNN - 高容量网络 🟡
- 🔗 残差网络块 - ResNet风格 🔴
- 📱 MobileNet - 移动端优化 🟡
- 🎯 U-Net编码器 - 图像分割 🔴
- ⚡ EfficientNet - 高效架构 🔴

**🤖 Transformer 类 (5个)** - 注意力机制
- 🤖 Transformer编码器 - 自注意力机制 🔴
- 👁️ Vision Transformer (ViT) - 视觉Transformer 🔴
- 📖 BERT风格 - 双向编码器 🔴
- ✍️ GPT风格 - 自回归解码器 🔴
- 🔄 Seq2Seq - 编码器-解码器 🔴

**🧠 MLP 类 (2个)** - 全连接网络
- 🧠 简单MLP - 基础全连接网络 🟢
- 🧬 深度MLP - 深层全连接网络 🟡

**🔄 Autoencoder 类 (2个)** - 自编码器
- 🔄 自编码器 - 特征学习和降维 🟡
- 🔁 卷积自编码器 - 图像重建 🔴

**🎨 GAN 类 (2个)** - 生成对抗网络
- 🎨 GAN生成器 - 图像生成 🔴
- 🔍 GAN判别器 - 真假鉴别 🔴

**难度标记**: 🟢 入门 | 🟡 中级 | 🔴 高级

#### 🎯 核心功能

# Neural Network Math Explorer 🔬

**神经网络架构的计算解剖台**

**开发者**: Just For Dream Lab

> 💡 **核心理念**：不是告诉你"CNN是什么"，而是让你看到"Conv2d(3,64,3)这一层到底算了什么、有多少参数、消耗多少内存"

---

## 🎯 项目定位

### 与其他可视化工具的差异

本项目深入到**单个神经元和层级计算细节**，专注于具体网络架构的数值推导和计算细节。

| 项目 | 定位 | 目标用户 | 核心问题 |
|------|------|----------|----------|
| **Neural_Network_Math_Explorer** | **单个神经元/层级计算细节** | 想理解代码实现的人 | "这一层到底算了什么？" |
| Transformer_Explorer | Transformer宏观架构 | 想理解Transformer的人 | "Attention为什么work？" |
| vision | 通用数学理论 | 想理解数学基础的人 | "梯度下降为什么收敛？" |

---

## 项目目标

本项目致力于为深度学习工程师和研究者提供：
- **逐层计算追踪**：真正的"单步调试"体验，看到每个神经元的数值变化
- **参数量/FLOPs分析**：给定输入尺寸，自动计算每层的计算复杂度和内存占用
- **架构对比实验**：同一任务，多种架构并排对比性能和效率
- **数值稳定性检测**：自动发现梯度消失/爆炸问题，给出优化建议

## ✨ 核心特性

### 1️⃣ **逐层计算追踪**（真正的"单步调试"）
输入一个具体样本，逐层展示每个神经元的计算过程：
- ✅ 每个神经元的加权和计算
- ✅ 激活函数的输出
- ✅ 梯度反向传播的数值
- ✅ 参数更新过程

**示例**：输入一张 32×32 图像，看每个卷积核输出的具体数值

### 2️⃣ **参数量与计算量分析**（工程实用）
给定输入尺寸，自动计算每层的：
- 📊 参数量（Params）
- 📈 浮点运算量（FLOPs）
- 💾 内存占用（Forward/Backward）
- ⏱️ 理论计算时间

**示例输出**：
```
输入: [1, 3, 224, 224]
Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
├─ 参数量: 9,408
├─ FLOPs: 118,013,952
├─ 输出形状: [1, 64, 112, 112]
└─ 前向内存: 3.06 MB
```

### 3️⃣ **架构对比实验室**（差异化核心）
同一任务，多种架构并排对比：
- **图像任务**：CNN vs ViT
- **序列任务**：RNN vs LSTM vs GRU vs Transformer
- **图任务**：GCN vs GAT vs GraphSAGE
- **实时指标**：参数量、FLOPs、内存、训练时间

### 4️⃣ **数值稳定性检测**
- ⚠️ 自动检测梯度消失/爆炸
- 📉 可视化梯度流在各层的变化
- 🔧 推荐初始化/归一化方案

---

## 📚 当前实现的架构模块

### 🖼️ **卷积神经网络（CNN）**
- ✅ 卷积操作的逐像素计算
- ✅ 输出尺寸计算公式 $((W - K + 2P) / S) + 1$ 的推导
- ✅ 反向传播中的梯度计算（对输入、卷积核、偏置）
- ✅ 多通道卷积的数学推导
- ✅ 池化层的降采样细节
- 🎯 **实用场景**：理解 PyTorch `nn.Conv2d` 的实现细节

### 🕸️ **图神经网络（GNN）**
- ✅ 图卷积网络（GCN）的消息传递机制
- ✅ 对称归一化邻接矩阵 $D^{-1/2} A D^{-1/2}$ 的数学意义
- ✅ 图拉普拉斯矩阵的特征值分解
- ✅ 谱图卷积的理论基础
- 🎯 **实用场景**：对比 GCN、GAT、GraphSAGE 的计算复杂度

### 🔁 **循环神经网络（RNN/LSTM/GRU）**
- ✅ RNN的时间反向传播（BPTT）算法
- ✅ 梯度消失和梯度爆炸的数值演示
- ✅ LSTM门控机制的逐门计算
- ✅ 长期依赖问题的解决方案
- 🎯 **实用场景**：理解 `nn.LSTM` 的内部门控计算

### 🔬 **反向传播原理**
- ✅ 全连接层的完整梯度推导
- ✅ 计算图和自动微分机制
- ✅ 数值稳定性问题（Softmax的log-sum-exp技巧）
- ✅ 梯度检验方法（数值梯度 vs 解析梯度）
- 🎯 **实用场景**：验证自定义层的梯度实现正确性

### 🏛️ **失败案例博物馆** ✨ **NEW**
- ✅ 100层MLP梯度消失的数值证明
- ✅ 参数爆炸案例（卷积接全连接层）
- ✅ 无归一化网络的训练不稳定性
- ✅ 学习率过大导致的梯度爆炸演示
- 🎯 **实用场景**：不是告诉你"这样不好"，而是让你看到"梯度真的变成1e-10了"

### 🏗️ **ResNet残差连接分析** ✨ **NEW**
- ✅ 普通网络 vs ResNet梯度流对比
- ✅ 数值证明"梯度高速公路"机制
- ✅ 残差连接的数学原理推导（y = F(x) + x）
- ✅ 支持简化版（全连接）和完整版（卷积）
- 🎯 **实用场景**：理解为什么现代深度网络都用残差连接

### 🔧 **归一化层对比** ✨ **NEW**
- ✅ BatchNorm、LayerNorm、GroupNorm三种方法对比
- ✅ 可视化"在哪个维度归一化"的差异
- ✅ Batch Size敏感性分析（为什么小batch时BatchNorm效果差）
- ✅ 适用场景决策树（CNN用BatchNorm，Transformer用LayerNorm）
- 🎯 **实用场景**：选择合适的归一化方法，理解Transformer为什么用LayerNorm

### 🔍 **Vision Transformer (ViT) 分析** ✨ **NEW**
- ✅ Patch Embedding可视化（如何把图像变成序列）
- ✅ Self-Attention权重热力图（全局感受野）
- ✅ Position Encoding原理（为什么Transformer需要位置信息）
- ✅ ViT vs CNN参数量对比（ViT-Base有86M参数）
- ✅ 计算复杂度分析（O(N²)的Self-Attention）
- ✅ 数据效率分析（为什么ViT需要大规模预训练）
- 🎯 **实用场景**：理解Transformer在视觉领域的应用，学会选择ViT还是CNN

### 🔬 **架构对比实验室** ✨ **NEW**
- ✅ CNN vs Transformer训练曲线对比（Loss、Accuracy）
- ✅ 收敛速度分析（达到目标精度需要多少epoch）
- ✅ 数据效率实验（不同数据量下的性能对比）
- ✅ 参数量、FLOPs、计算复杂度全方位对比
- ✅ 智能决策助手（根据数据量和计算资源推荐模型）
- 🎯 **实用场景**：用数据说话，回答"何时用CNN，何时用Transformer"

## 安装与使用

### 环境要求
- Python 3.8+
- 建议使用虚拟环境

### 安装步骤
```bash
# 克隆项目
git clone <repository-url>
cd Neural_Network_Math_Explorer

# 安装依赖
pip install -r requirements.txt

# 运行应用
streamlit run app.py
```

### 核心依赖
```
streamlit      # Web界面框架
torch          # 深度学习框架
numpy          # 数值计算
plotly         # 交互式可视化
networkx       # 图论算法
sympy          # 符号计算
scipy          # 科学计算
scikit-learn   # 机器学习工具
```

## 🎓 适合人群

- ✅ **想理解 PyTorch/TensorFlow 代码实现细节的开发者**
  - "为什么 `nn.Conv2d(3, 64, 3)` 有 1,728 个参数？"
  - "LSTM 的三个门到底怎么算的？"
  
- ✅ **需要设计自定义网络架构的研究者**
  - "我的模型参数量太大了，瓶颈在哪？"
  - "为什么训练不稳定？梯度哪里出问题了？"
  
- ✅ **模型优化工程师**
  - 参数压缩、模型加速
  - 内存优化、算子融合
  
- ✅ **教学演示**
  - "全连接层为什么参数这么多？"
  - "残差连接如何解决梯度消失？"

---

## ⚠️ 明确的边界

### ❌ 本项目 **不做** 的事情

1. **通用数学理论教学**
   - ❌ 不讲"梯度下降为什么收敛"（这是 `vision` 项目的职责）
   - ❌ 不讲"Adam 优化器的数学原理"（同上）
   - ✅ 只讲"这一层的梯度具体怎么算"

2. **Transformer 架构详解**
   - ❌ 不讲"Multi-Head Attention 为什么有效"（这是 `Transformer_Explorer` 的职责）
   - ✅ 如果涉及 ViT，只讲"Patch Embedding 的计算细节"

3. **扩散模型原理**
   - ❌ 已从本项目移除（`vision` 项目有更完整的扩散模型数学推导）
   - ✅ 可能在未来加入"UNet 架构的参数分析"

### ✅ 本项目 **专注** 的事情

- ✅ 具体网络层的计算细节（逐神经元追踪）
- ✅ 参数量/FLOPs/内存的工程分析
- ✅ 不同架构在相同任务上的对比
- ✅ 数值稳定性问题的检测和诊断

## 🗺️ 开发路线图

### ✅ 已完成（v1.0 - 当前版本）
- ✅ CNN 卷积操作的逐像素计算
- ✅ RNN/LSTM 门控机制详解
- ✅ GNN 消息传递可视化
- ✅ 反向传播梯度追踪

---

### ✅ Phase 1: 清理与重构（v1.5 - 已完成大部分）

#### 立即移除（避免重复）
- [ ] ❌ **移除扩散模型模块** → 转移到 `vision` 项目
- [ ] ❌ **移除通用数学推导工具** → `vision` 已覆盖
- [ ] ❌ **移除优化器对比实验** → `Transformer_Explorer` 已有

#### 核心功能增强
- [x] 🔧 **增强反向传播追踪** ✨ **已完成**
  - ✅ 逐层、逐神经元的数值展示
  - ✅ 梯度流可视化（哪一层梯度变小了？）
  - ✅ 支持全连接、CNN、RNN三种网络类型
  - ✅ 梯度数值验证（Gradient Checking）
  - ✅ 梯度消失/爆炸检测

- [x] 🎮 **交互实验室** ✨ **已完成**
  - ✅ CNN特征图实时可视化
  - ✅ GNN节点分类演示
  - ✅ 激活函数交互式对比
  - ✅ 优化器轨迹可视化
  - ✅ 损失函数3D地形图
  - ✅ 批量参数对比实验

- [x] 📊 **参数量/FLOPs 计算器增强** ✨ **已完成并大幅扩展**
  
  **支持的层类型 (8种)**:
  - ✅ Conv2d (标准卷积)
  - ✅ DepthwiseConv2d (深度可分离卷积，MobileNet核心)
  - ✅ Linear (全连接层)
  - ✅ MultiHeadAttention (多头注意力，Transformer核心)
  - ✅ LSTM (长短期记忆网络)
  - ✅ Embedding (嵌入层)
  - ✅ BatchNorm2d (批归一化)
  - ✅ LayerNorm (层归一化，Transformer使用)
  
  **预定义网络 (7个架构)**:
  - ✅ ResNet-18/50 (CNN经典)
  - ✅ VGG-16 (CNN经典)
  - ✅ MobileNetV2 (轻量级CNN)
  - ✅ BERT-base (Transformer编码器，110M参数)
  - ✅ GPT-2 small (Transformer解码器，117M参数)
  - ✅ ViT-Base (Vision Transformer，86M参数)
  
  **功能特性**:
  - ✅ 单层详细分析：参数量、FLOPs、内存占用、输出形状
  - ✅ 完整网络分析：逐层统计 + 总体指标
  - ✅ 自定义网络构建器：交互式添加层并实时计算
  - ✅ 对比分析：Depthwise vs 标准卷积、LSTM vs GRU、LayerNorm vs BatchNorm
  - ✅ 复杂度分析：注意力机制的O(n²)复杂度警告
  - ✅ 可视化：参数分布、FLOPs分布、饼图
  - ✅ Markdown报告生成：可下载详细分析报告

---

### 🚀 Phase 2: 核心差异化功能（v2.0）

#### 架构对比实验室
- [x] 📈 **图像分类任务对比** ✨ **已完成**
  - ✅ CNN (ResNet-18/50, MobileNet-V2) vs ViT (Tiny/Small/Base)
  - ✅ 参数量、FLOPs、训练曲线、收敛速度全方位对比
  - ✅ 数据效率分析（10%到100%数据量对比）
  - ✅ 智能决策助手（根据数据量和算力推荐模型）
  - ✅ 6种常用模型的详细信息对比表

- [ ] 🔄 **序列建模任务对比**（可选扩展）
  - RNN vs LSTM vs GRU
  - 梯度流对比（哪个最稳定？）
  - 计算效率对比（哪个最快？）

- [ ] 🌐 **图任务对比**（可选扩展）
  - GCN vs GAT vs GraphSAGE
  - 不同图规模下的性能
  - 消息传递复杂度分析

#### 现代架构组件
- [x] 🆕 **Vision Transformer (ViT)** ✨ **已完成**
  - ✅ Patch Embedding 的计算细节和可视化
  - ✅ Position Embedding 的数值和原理
  - ✅ Self-Attention权重热力图
  - ✅ 与 CNN 的参数量、FLOPs、数据效率对比
  - ✅ 计算复杂度分析（O(N²)）

- [x] 🆕 **残差网络（ResNet）** ✨ **已完成**
  - ✅ 残差连接的梯度高速公路验证
  - ✅ 有无残差的梯度对比（数值证明）
  - ✅ 不同深度的梯度流分析
  - ✅ 数学原理详细推导（∂L/∂x = ∂L/∂y · (∂F/∂x + 1)）

- [x] 🆕 **归一化层对比** ✨ **已完成**
  - ✅ BatchNorm vs LayerNorm vs GroupNorm三维对比
  - ✅ 数值稳定性分析和可视化
  - ✅ Batch Size敏感性实验
  - ✅ 适用场景决策指南

- [x] 🆕 **Vision Transformer (ViT)** ✨ **已完成**
  - ✅ Patch Embedding可视化（图像切片过程）
  - ✅ Self-Attention权重可视化
  - ✅ Position Encoding原理说明
  - ✅ ViT vs CNN对比（参数量、计算复杂度、数据需求）
  - ✅ 适用场景决策指南

---

### 🔬 Phase 3: 高级工程工具（v3.0）

#### 架构设计工作台
- [ ] 🧱 **拖拽式网络搭建**
  - 可视化界面拖拽层模块
  - 实时计算整体复杂度
  - 自动检测不合理设计（如全连接层太大）

#### 内存分析器
- [ ] 💾 **前向/反向传播内存追踪**
  - 每层的激活值内存
  - 梯度存储内存
  - 峰值内存预测

#### 数值稳定性诊断
- [ ] ⚠️ **自动检测问题**
  - 梯度消失/爆炸检测
  - 数值溢出预警
  - 推荐初始化方案

#### 失败案例博物馆
- [x] 🏛️ **经典设计错误展示** ✨ **已完成**
  - ✅ "100 层的普通 MLP"（梯度消失，真实数值证明）
  - ✅ "卷积层直接接全连接"（参数爆炸，32亿参数演示）
  - ✅ "没有归一化的深度网络"（训练不稳定）
  - ✅ "超大学习率"（梯度爆炸，Loss变NaN演示）

---

### 📅 长期愿景（v4.0+）

- [ ] 🆕 **MoE (Mixture of Experts)** 参数量分析
- [ ] 🆕 **模型剪枝可视化**：哪些参数可以安全移除？
- [ ] 🆕 **量化模拟**：INT8 vs FP16 vs FP32 的精度损失
- [ ] 🆕 **算子融合优化**：Conv+BN+ReLU 融合后的加速
- [ ] 🆕 **分布式训练**：参数如何在多卡上分配？

## 🤝 贡献指南

我们欢迎以下形式的贡献：

### 优先需求
- 🆕 **新的架构模块**：Attention 变体、MoE、新型 GNN
- 🐛 **计算错误修正**：参数量/FLOPs 计算错误
- 📊 **可视化改进**：更清晰的梯度流图、更直观的对比界面
- 🔧 **工程优化**：性能提升、内存优化

### 提交前检查清单
- [ ] 代码符合项目定位（专注于**计算细节**，不涉及通用数学理论）
- [ ] 与 `vision` 和 `Transformer_Explorer` 无功能重复
- [ ] 包含数值示例和测试用例
- [ ] 代码注释完整（尤其是参数量/FLOPs 计算公式）

---

## 💡 设计哲学

> **"Tell me and I forget, teach me and I may remember, involve me and I learn."**  
> —— Benjamin Franklin

我们相信：**看到具体的数字计算，胜过阅读千行原理说明。**

这个工具的目标不是替代教科书，而是成为你理解 PyTorch 源码时的**"显微镜"**。

---

## 📖 相关项目

### Just For Dream Lab 的其他可视化工具

| 项目 | 定位 | 适合场景 |
|------|------|----------|
| **[Neural_Network_Math_Explorer](.)** | 网络层计算细节 | "Conv2d 有多少参数？" |
| **[Transformer_Explorer](../Transformer_Explorer)** | Transformer 架构 | "Multi-Head Attention 怎么工作？" |
| **[vision](../vision)** | 通用 ML 数学 | "梯度下降为什么收敛？" |

选择合适的工具学习不同层次的知识！

---

## 📄 引用

如果本项目对您的工作有帮助，欢迎引用：
```
Neural Network Math Explorer - The Computational Anatomy Lab
Just For Dream Lab
https://github.com/just-for-dream-0x10/Model_explorer
```

---

## 📧 联系方式

- **开发团队**: Just For Dream Lab
- **问题反馈**: [GitHub Issues]
- **功能建议**: 欢迎通过 Issue 或 PR 提出

---

**免责声明**：本项目专注于教学和原理展示，代码优先考虑可读性而非生产性能。使用者需自行评估代码适用性。