# Neural Network Math Explorer 🔬

**神经网络架构的计算解剖台**

**开发者**: Just For Dream Lab

> 💡 **核心理念**：不是告诉你"CNN是什么"，而是让你看到"Conv2d(3,64,3)这一层到底算了什么、有多少参数、消耗多少内存"

---

## 🎯 项目定位

### 与其他可视化工具的差异

本项目深入到**单个神经元和层级计算细节**，专注于具体网络架构的数值推导和计算细节。

| 项目 | 定位 | 目标用户 | 核心问题 |
|------|------|----------|----------|
| **Neural_Network_Math_Explorer** | **单个神经元/层级计算细节** | 想理解代码实现的人 | "这一层到底算了什么？" |
| Transformer_Explorer | Transformer宏观架构 | 想理解Transformer的人 | "Attention为什么work？" |
| vision | 通用数学理论 | 想理解数学基础的人 | "梯度下降为什么收敛？" |

---

## 项目目标

本项目致力于为深度学习工程师和研究者提供：
- **逐层计算追踪**：真正的"单步调试"体验，看到每个神经元的数值变化
- **参数量/FLOPs分析**：给定输入尺寸，自动计算每层的计算复杂度和内存占用
- **架构对比实验**：同一任务，多种架构并排对比性能和效率
- **数值稳定性检测**：自动发现梯度消失/爆炸问题，给出优化建议

## ✨ 核心特性

### 1️⃣ **逐层计算追踪**（真正的"单步调试"）
输入一个具体样本，逐层展示每个神经元的计算过程：
- ✅ 每个神经元的加权和计算
- ✅ 激活函数的输出
- ✅ 梯度反向传播的数值
- ✅ 参数更新过程

**示例**：输入一张 32×32 图像，看每个卷积核输出的具体数值

### 2️⃣ **参数量与计算量分析**（工程实用）
给定输入尺寸，自动计算每层的：
- 📊 参数量（Params）
- 📈 浮点运算量（FLOPs）
- 💾 内存占用（Forward/Backward）
- ⏱️ 理论计算时间

**示例输出**：
```
输入: [1, 3, 224, 224]
Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
├─ 参数量: 9,408
├─ FLOPs: 118,013,952
├─ 输出形状: [1, 64, 112, 112]
└─ 前向内存: 3.06 MB
```

### 3️⃣ **架构对比实验室**（差异化核心）
同一任务，多种架构并排对比：
- **图像任务**：CNN vs ViT
- **序列任务**：RNN vs LSTM vs GRU vs Transformer
- **图任务**：GCN vs GAT vs GraphSAGE
- **实时指标**：参数量、FLOPs、内存、训练时间

### 4️⃣ **数值稳定性检测**
- ⚠️ 自动检测梯度消失/爆炸
- 📉 可视化梯度流在各层的变化
- 🔧 推荐初始化/归一化方案

---

## 📚 当前实现的架构模块

### 🖼️ **卷积神经网络（CNN）**
- ✅ 卷积操作的逐像素计算
- ✅ 输出尺寸计算公式 $((W - K + 2P) / S) + 1$ 的推导
- ✅ 反向传播中的梯度计算（对输入、卷积核、偏置）
- ✅ 多通道卷积的数学推导
- ✅ 池化层的降采样细节
- 🎯 **实用场景**：理解 PyTorch `nn.Conv2d` 的实现细节

### 🕸️ **图神经网络（GNN）**
- ✅ 图卷积网络（GCN）的消息传递机制
- ✅ 对称归一化邻接矩阵 $D^{-1/2} A D^{-1/2}$ 的数学意义
- ✅ 图拉普拉斯矩阵的特征值分解
- ✅ 谱图卷积的理论基础
- 🎯 **实用场景**：对比 GCN、GAT、GraphSAGE 的计算复杂度

### 🔁 **循环神经网络（RNN/LSTM/GRU）**
- ✅ RNN的时间反向传播（BPTT）算法
- ✅ 梯度消失和梯度爆炸的数值演示
- ✅ LSTM门控机制的逐门计算
- ✅ 长期依赖问题的解决方案
- 🎯 **实用场景**：理解 `nn.LSTM` 的内部门控计算

### 🔬 **反向传播原理**
- ✅ 全连接层的完整梯度推导
- ✅ 计算图和自动微分机制
- ✅ 数值稳定性问题（Softmax的log-sum-exp技巧）
- ✅ 梯度检验方法（数值梯度 vs 解析梯度）
- 🎯 **实用场景**：验证自定义层的梯度实现正确性

## 安装与使用

### 环境要求
- Python 3.8+
- 建议使用虚拟环境

### 安装步骤
```bash
# 克隆项目
git clone <repository-url>
cd Neural_Network_Math_Explorer

# 安装依赖
pip install -r requirements.txt

# 运行应用
streamlit run app.py
```

### 核心依赖
```
streamlit      # Web界面框架
torch          # 深度学习框架
numpy          # 数值计算
plotly         # 交互式可视化
networkx       # 图论算法
sympy          # 符号计算
scipy          # 科学计算
scikit-learn   # 机器学习工具
```

## 🎓 适合人群

- ✅ **想理解 PyTorch/TensorFlow 代码实现细节的开发者**
  - "为什么 `nn.Conv2d(3, 64, 3)` 有 1,728 个参数？"
  - "LSTM 的三个门到底怎么算的？"
  
- ✅ **需要设计自定义网络架构的研究者**
  - "我的模型参数量太大了，瓶颈在哪？"
  - "为什么训练不稳定？梯度哪里出问题了？"
  
- ✅ **模型优化工程师**
  - 参数压缩、模型加速
  - 内存优化、算子融合
  
- ✅ **教学演示**
  - "全连接层为什么参数这么多？"
  - "残差连接如何解决梯度消失？"

---

## ⚠️ 明确的边界

### ❌ 本项目 **不做** 的事情

1. **通用数学理论教学**
   - ❌ 不讲"梯度下降为什么收敛"（这是 `vision` 项目的职责）
   - ❌ 不讲"Adam 优化器的数学原理"（同上）
   - ✅ 只讲"这一层的梯度具体怎么算"

2. **Transformer 架构详解**
   - ❌ 不讲"Multi-Head Attention 为什么有效"（这是 `Transformer_Explorer` 的职责）
   - ✅ 如果涉及 ViT，只讲"Patch Embedding 的计算细节"

3. **扩散模型原理**
   - ❌ 已从本项目移除（`vision` 项目有更完整的扩散模型数学推导）
   - ✅ 可能在未来加入"UNet 架构的参数分析"

### ✅ 本项目 **专注** 的事情

- ✅ 具体网络层的计算细节（逐神经元追踪）
- ✅ 参数量/FLOPs/内存的工程分析
- ✅ 不同架构在相同任务上的对比
- ✅ 数值稳定性问题的检测和诊断

## 🗺️ 开发路线图

### ✅ 已完成（v1.0 - 当前版本）
- ✅ CNN 卷积操作的逐像素计算
- ✅ RNN/LSTM 门控机制详解
- ✅ GNN 消息传递可视化
- ✅ 反向传播梯度追踪

---

### 🚧 Phase 1: 清理与重构（v1.5 - 进行中）

#### 立即移除（避免重复）
- [ ] ❌ **移除扩散模型模块** → 转移到 `vision` 项目
- [ ] ❌ **移除通用数学推导工具** → `vision` 已覆盖
- [ ] ❌ **移除优化器对比实验** → `Transformer_Explorer` 已有

#### 核心功能增强
- [ ] 🔧 **增强反向传播追踪**
  - 逐层、逐神经元的数值展示
  - 梯度流可视化（哪一层梯度变小了？）
  - 支持自定义网络结构

- [ ] 📊 **参数量/FLOPs 计算器**
  - 输入网络配置 → 自动计算 Params/FLOPs/Memory
  - 支持常见层：Conv2d, Linear, BatchNorm, LayerNorm, Attention
  - 生成详细报告（类似 `torchinfo`）

---

### 🚀 Phase 2: 核心差异化功能（v2.0）

#### 架构对比实验室
- [ ] 📈 **图像分类任务对比**
  - CNN (ResNet-18) vs ViT-Tiny
  - 参数量、FLOPs、训练时间、收敛曲线
  - 数据效率分析（小数据集上谁更好？）

- [ ] 🔄 **序列建模任务对比**
  - RNN vs LSTM vs GRU
  - 梯度流对比（哪个最稳定？）
  - 计算效率对比（哪个最快？）

- [ ] 🌐 **图任务对比**
  - GCN vs GAT vs GraphSAGE
  - 不同图规模下的性能
  - 消息传递复杂度分析

#### 现代架构组件
- [ ] 🆕 **Vision Transformer (ViT)**
  - Patch Embedding 的计算细节
  - Position Embedding 的数值
  - 与 CNN 的参数量对比

- [ ] 🆕 **残差网络（ResNet）**
  - 残差连接的梯度高速公路验证
  - 有无残差的梯度对比
  - 不同深度的梯度流分析

- [ ] 🆕 **归一化层对比**
  - BatchNorm vs LayerNorm vs GroupNorm
  - 数值稳定性分析
  - 内存占用对比

---

### 🔬 Phase 3: 高级工程工具（v3.0）

#### 架构设计工作台
- [ ] 🧱 **拖拽式网络搭建**
  - 可视化界面拖拽层模块
  - 实时计算整体复杂度
  - 自动检测不合理设计（如全连接层太大）

#### 内存分析器
- [ ] 💾 **前向/反向传播内存追踪**
  - 每层的激活值内存
  - 梯度存储内存
  - 峰值内存预测

#### 数值稳定性诊断
- [ ] ⚠️ **自动检测问题**
  - 梯度消失/爆炸检测
  - 数值溢出预警
  - 推荐初始化方案

#### 失败案例博物馆
- [ ] 🏛️ **经典设计错误展示**
  - "100 层的普通 MLP"（梯度消失）
  - "全连接层接 Conv"（参数爆炸）
  - "没有归一化的深度网络"（训练不稳定）

---

### 📅 长期愿景（v4.0+）

- [ ] 🆕 **MoE (Mixture of Experts)** 参数量分析
- [ ] 🆕 **模型剪枝可视化**：哪些参数可以安全移除？
- [ ] 🆕 **量化模拟**：INT8 vs FP16 vs FP32 的精度损失
- [ ] 🆕 **算子融合优化**：Conv+BN+ReLU 融合后的加速
- [ ] 🆕 **分布式训练**：参数如何在多卡上分配？

## 🤝 贡献指南

我们欢迎以下形式的贡献：

### 优先需求
- 🆕 **新的架构模块**：Attention 变体、MoE、新型 GNN
- 🐛 **计算错误修正**：参数量/FLOPs 计算错误
- 📊 **可视化改进**：更清晰的梯度流图、更直观的对比界面
- 🔧 **工程优化**：性能提升、内存优化

### 提交前检查清单
- [ ] 代码符合项目定位（专注于**计算细节**，不涉及通用数学理论）
- [ ] 与 `vision` 和 `Transformer_Explorer` 无功能重复
- [ ] 包含数值示例和测试用例
- [ ] 代码注释完整（尤其是参数量/FLOPs 计算公式）

---

## 💡 设计哲学

> **"Tell me and I forget, teach me and I may remember, involve me and I learn."**  
> —— Benjamin Franklin

我们相信：**看到具体的数字计算，胜过阅读千行原理说明。**

这个工具的目标不是替代教科书，而是成为你理解 PyTorch 源码时的**"显微镜"**。

---

## 📖 相关项目

### Just For Dream Lab 的其他可视化工具

| 项目 | 定位 | 适合场景 |
|------|------|----------|
| **[Neural_Network_Math_Explorer](.)** | 网络层计算细节 | "Conv2d 有多少参数？" |
| **[Transformer_Explorer](../Transformer_Explorer)** | Transformer 架构 | "Multi-Head Attention 怎么工作？" |
| **[vision](../vision)** | 通用 ML 数学 | "梯度下降为什么收敛？" |

选择合适的工具学习不同层次的知识！

---

## 📄 引用

如果本项目对您的工作有帮助，欢迎引用：
```
Neural Network Math Explorer - The Computational Anatomy Lab
Just For Dream Lab
https://github.com/just-for-dream-0x10/Model_explorer
```

---

## 📧 联系方式

- **开发团队**: Just For Dream Lab
- **问题反馈**: [GitHub Issues]
- **功能建议**: 欢迎通过 Issue 或 PR 提出

---

**免责声明**：本项目专注于教学和原理展示，代码优先考虑可读性而非生产性能。使用者需自行评估代码适用性。