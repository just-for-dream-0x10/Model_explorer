# 神经网络数学原理探索器

**开发者**: Just For Dream Lab

一个面向学者和研究者的交互式工具，用于深入理解深度学习模型的内部工作原理。本项目通过严格的数学推导和可视化，帮助用户建立对神经网络从基础到前沿技术的系统性认知。

## 项目目标

本项目致力于为深度学习研究者和学习者提供：
- **透明化的数学原理**：完整展示模型背后的数学推导过程
- **可验证的计算实现**：通过数值实验验证理论公式
- **直观的可视化工具**：将抽象的数学概念转化为可交互的视觉呈现
- **系统化的知识体系**：从经典网络到前沿生成模型的完整覆盖

## 当前功能模块

### 🔄 卷积神经网络（CNN）
- **核心内容**：
  - 卷积运算的离散数学定义
  - 输出尺寸计算公式 $((W - K + 2P) / S) + 1$ 的推导
  - 反向传播中的梯度计算（包括对输入、卷积核、偏置的梯度）
  - 数值梯度检验方法
- **适用场景**：理解卷积操作的本质、验证自定义卷积实现的正确性

### 🕸️ 图神经网络（GNN）
- **核心内容**：
  - 图卷积网络（GCN）的消息传递机制
  - 对称归一化邻接矩阵 $D^(-1/2) A D^(-1/2)$ 的数学意义
  - 图拉普拉斯矩阵的特征值分解
  - 谱图卷积的理论基础
- **适用场景**：理解图结构数据的深度学习方法、研究图神经网络的理论性质

### 🔁 循环神经网络（RNN/LSTM）
- **核心内容**：
  - RNN的时间反向传播（BPTT）算法
  - 梯度消失和梯度爆炸的数学解释
  - LSTM门控机制的设计原理
  - 长期依赖问题的解决方案
- **适用场景**：理解序列模型的训练难点、研究时序数据建模方法

### 🌊 扩散模型（Diffusion Models）
- **核心内容**：
  - 前向扩散过程的马尔可夫链定义
  - 重参数化技巧：$ x_t = \sqrt{ᾱ_t} \cdot x_0 + \sqrt{1-ᾱ_t} \cdot ε$
  - 反向去噪过程的变分下界（ELBO）推导
  - DDPM vs DDIM采样算法对比
  - Score-based模型的理论联系
  - 2D数据分布的扩散可视化
- **适用场景**：理解生成式AI（如Stable Diffusion、DALL-E）的数学基础

### 📐 数学推导工具
- **核心内容**：
  - 卷积定理的傅里叶视角
  - 梯度下降及其变体（SGD、Momentum、Adam）的数学分析
  - 链式法则在深度网络中的应用
  - 常见激活函数的导数计算
- **适用场景**：复习和验证基础数学工具

### 🔬 反向传播原理
- **核心内容**：
  - 全连接层的完整梯度推导
  - 计算图和自动微分
  - 数值稳定性问题（例如Softmax的log-sum-exp技巧）
  - 梯度检验方法
- **适用场景**：深入理解自动微分框架的工作机制

### 🧪 交互实验室
- **核心内容**：
  - 可调参数的实时实验
  - 多种优化器和学习率策略的对比
  - 实际训练过程的可视化监控
- **适用场景**：快速验证理论假设、探索超参数影响

## 安装与使用

### 环境要求
- Python 3.8+
- 建议使用虚拟环境

### 安装步骤
```bash
# 克隆项目
git clone <repository-url>
cd Neural_Network_Math_Explorer

# 安装依赖
pip install -r requirements.txt

# 运行应用
streamlit run app.py
```

### 核心依赖
```
streamlit      # Web界面框架
torch          # 深度学习框架
numpy          # 数值计算
plotly         # 交互式可视化
networkx       # 图论算法
sympy          # 符号计算
scipy          # 科学计算
scikit-learn   # 机器学习工具
```

## 适用人群

- **研究生和博士生**：深入理解论文中的数学细节
- **算法工程师**：验证自定义模块的数学正确性
- **深度学习讲师**：用于课堂教学和演示
- **自学者**：系统学习神经网络的数学基础

## 当前限制

本项目专注于**教学和原理展示**，存在以下限制：
- **计算资源**：演示模型尺寸较小，不适合生产级训练
- **数据集**：使用简化的数据集，未集成大规模真实数据
- **性能优化**：代码优先考虑可读性而非执行效率
- **模型训练**：扩散模型等复杂模型仅展示采样算法，未提供完整训练流程

这些限制是有意为之，以确保项目保持教学清晰性和可维护性。

## 未来计划

### 短期目标（3-6个月）
- [ ] **优化算法模块增强**
  - 二阶优化方法（牛顿法、BFGS）
  - 自适应学习率的理论分析
  - 学习率调度策略对比

- [ ] **正则化技术**
  - Dropout的集成学习解释
  - 权重衰减（L1/L2）的贝叶斯视角
  - 数据增强的数学等价性

### 中期目标（6-12个月）
- [ ] **生成对抗网络（GAN）**
  - Minimax博弈的数学表述
  - Wasserstein距离与WGAN
  - 训练稳定性问题分析

- [ ] **变分自编码器（VAE）**
  - ELBO推导与重参数化技巧
  - 潜空间的几何性质
  - VAE与扩散模型的联系

- [ ] **强化学习基础**
  - 策略梯度定理
  - 值函数近似
  - Actor-Critic架构

### 长期愿景
- [ ] **大语言模型（LLM）原理**
  - 因果语言建模的数学框架
  - Scaling Laws的经验规律
  - In-Context Learning的理论解释
  - 提示工程的数学视角

- [ ] **多模态模型**
  - 视觉-语言对齐机制（CLIP、ALIGN）
  - 跨模态注意力
  - 统一表示学习

- [ ] **模型压缩与效率**
  - 知识蒸馏的理论基础
  - 量化与剪枝的数学分析
  - LoRA等参数高效微调方法

- [ ] **可解释性工具**
  - 梯度归因方法（Integrated Gradients、Saliency Maps）
  - 注意力权重分析
  - 特征可视化

- [ ] **交互式论文复现**
  - 经典论文的核心算法实现
  - 消融实验工具
  - 理论与实验的对照验证

## 贡献指南

我们欢迎以下形式的贡献：
- **Bug修复**：报告或修复现有功能的错误
- **文档改进**：纠正数学公式、补充说明
- **功能建议**：提出新的模块或可视化想法
- **代码优化**：在保持可读性的前提下提升性能

提交贡献前，请确保：
1. 数学推导的严格性和正确性
2. 代码注释的完整性
3. 与现有风格的一致性

## 引用

如果本项目对您的研究或教学有帮助，欢迎引用：
```
Neural Network Math Explorer
Just For Dream Lab
https://github.com/just-for-dream-0x10/Model_explorer
```


## 联系方式

- **开发团队**: Just For Dream Lab
- **问题反馈**: [GitHub Issues]

---

**免责声明**：本项目仅供教育和研究使用。代码实现优先考虑教学清晰性，未针对生产环境优化。使用者需自行评估代码适用性。