# Neural Network Math Explorer 🔬

**神经网络架构的计算解剖台**

> 💡 **核心理念**：不是告诉你"CNN是什么"，而是让你看到"`Conv2d(3,64,3)`这一层到底算了什么、有多少参数、消耗多少内存"

---

## 🎯 项目定位

### 与其他项目的"视角差异"

虽然三个项目有部分**功能重复**，但**视角和深度完全不同**：

| 维度 | Neural_Network_Math_Explorer | Transformer_Explorer | Basic_Math |
|------|------------------------------|----------------------|------------|
| **核心视角** | 🔬 **"如何实现"** - 工程实现细节 | 🏗️ **"为什么设计"** - 架构原理 | 📐 **"数学本质"** - 理论基础 |
| **典型问题** | "Conv2d(3,64,7) 有多少参数？如何计算？" | "Multi-Head 为什么比 Single-Head 好？" | "卷积定理的傅里叶变换证明？" |
| **用户群体** | PyTorch 实现者、模型优化工程师 | 架构设计者、研究者 | 数学研究者、理论学习者 |
| **深度层次** | 代码级（逐行计算、参数分析） | 设计级（模块组合、架构对比） | 理论级（数学推导、严格证明） |
| **Git地址** | [Model_explorer](https://github.com/just-for-dream-0x10/Model_explorer) | [Transformer_explorer](https://github.com/just-for-dream-0x10/Transformer_explorer) | [Basic_math](https://github.com/just-for-dream-0x10/Basic_math) |

### 📚 具体差异举例

#### 示例 1：卷积操作

| 项目 | 视角 | 内容侧重 |
|------|------|----------|
| **Neural（本项目）** | 🔧 实现细节 | • 3×3卷积核滑动时每个位置的乘加操作<br>• 输出形状计算：$(W-K+2P)/S+1$<br>• 参数量：$C_{out} \times C_{in} \times K^2$<br>• FLOPs/内存分析 |
| **Basic_Math** | 📐 数学理论 | • 卷积定理的傅里叶证明<br>• 卷积的交换律、结合律<br>• 离散卷积与连续卷积的关系 |
| **Transformer_Explorer** | 🚫 不涉及 | - |

#### 示例 2：图神经网络 (GNN)

| 项目 | 视角 | 内容侧重 |
|------|------|----------|
| **Neural（本项目）** | 🔧 实现细节 | • GCN 消息传递的具体计算步骤<br>• $D^{-1/2}AD^{-1/2}$ 的矩阵乘法<br>• 不同 GNN 架构的参数量对比 |
| **Basic_Math** | 📐 数学理论 | • 图拉普拉斯矩阵的谱分解<br>• 切比雪夫多项式逼近<br>• 图傅里叶变换理论 |
| **Transformer_Explorer** | 🚫 不涉及 | - |

#### 示例 3：优化器

| 项目 | 视角 | 内容侧重 |
|------|------|----------|
| **Neural（本项目）** | 🔧 实现细节 | • Adam 的 m 和 v 更新公式<br>• 学习率调度的实际效果<br>• 不同优化器的收敛速度对比 |
| **Basic_Math** | 📐 数学理论 | • Adam 的收敛性证明<br>• 动量的指数加权移动平均推导<br>• 自适应学习率的理论基础 |
| **Transformer_Explorer** | 🏗️ 架构选择 | • AdamW vs Adam 在 Transformer 中的差异<br>• 为什么解耦权重衰减<br>• Warmup + Cosine Decay 策略 |

### 🔗 推荐学习路径

```
第一步：Basic_Math（理解数学本质）
   ↓  "为什么卷积有效？傅里叶视角"
   
第二步：Neural_Network_Math_Explorer（理解实现细节）← 你在这里！
   ↓  "Conv2d 怎么算？参数量多少？"
   
第三步：动手编码
   ↓  "用 PyTorch 实现自定义卷积层"
   
第四步：Transformer_Explorer（理解架构设计）
   ↓  "为什么 ViT 用 Patch Embedding？"
```

### 4个核心问题

每个模块都必须能够回答：

1. **"这一步到底算了什么？"** - 具体的数值计算过程
2. **"为什么是这个公式？"** - 数学推导和原理
3. **"数值如何变化？"** - 输入到输出的数值变化
4. **"什么时候会出问题？"** - 数值稳定性的边界条件

---

## ✨ 核心特性

### 1️⃣ 逐层计算追踪（详细的数值展示）
- ✅ 每个神经元的加权和计算
- ✅ 激活函数的输出
- ✅ 梯度反向传播的数值
- ✅ 参数更新过程

**示例**：输入一张 32×32 图像，看每个卷积核输出的具体数值

### 2️⃣ 参数量与计算量分析（工程实用）
- ✅ **支持8种层类型**（业界最全）：
  - Conv2d, DepthwiseConv2d（MobileNet）
  - Linear, MultiHeadAttention（Transformer）
  - LSTM, Embedding, BatchNorm2d, LayerNorm
- ✅ **支持7个完整网络**：
  - CNN经典：ResNet-18/50, VGG-16, MobileNetV2
  - Transformer：BERT-base (110M), GPT-2 small (117M), ViT-Base (86M)
- ✅ 参数量、FLOPs、内存占用详细分析
- ✅ 自动诊断：参数过大、FLOPs过高、配置问题
- ✅ 优化建议：剪枝、量化、蒸馏等策略

### 3️⃣ 架构对比实验室（差异化核心）
- ✅ 图像任务：CNN vs ViT
- ✅ 序列任务：RNN vs LSTM vs GRU vs Transformer
- ✅ 图任务：GCN vs GAT vs GraphSAGE
- ✅ 理论分析：参数量、FLOPs、内存占用
- ✅ 模拟训练曲线：基于真实实验规律的收敛过程对比
- ⚠️ **说明**：训练曲线为模拟数据，但基于真实实验的收敛规律

### 4️⃣ 数值稳定性检测
- ⚠️ 自动检测梯度消失/爆炸
- ⚠️ 卷积配置问题诊断
- ⚠️ 图结构问题检测
- 📊 常见问题速查表
- 🔧 具体解决方案

---

## 📚 实现的功能模块

### 基础架构（3个）
- 🖼️ **CNN卷积神经网络**：逐像素卷积计算、特征图可视化、配置诊断
- 🕸️ **GNN图神经网络**：消息传递机制、谱图卷积、图结构诊断
- 🔁 **RNN/LSTM/GRU**：门控机制、BPTT算法、梯度流分析

### 现代架构（3个）
- 🔍 **Vision Transformer (ViT)**：Patch Embedding、Self-Attention、位置编码
- 🧠 **MoE专家混合**：路由机制、稀疏激活、FLOPs优势分析
- 🏗️ **ResNet残差网络**：跳跃连接、梯度高速公路、深度网络训练

### 训练技术（4个）
- 🔬 **反向传播原理**：链式法则、梯度计算、数值验证
- 🔧 **归一化层对比**：BatchNorm vs LayerNorm vs GroupNorm
- ✂️ **模型剪枝**：权重剪枝、结构化剪枝、梯度影响分析
- 🏛️ **失败案例博物馆**：梯度消失/爆炸、参数爆炸、训练不稳定

### 工具模块（4个）
- 🔢 **参数量计算器**：8种层类型、7个网络、优化建议
- 🎮 **交互实验室**：特征图可视化、激活函数对比、优化器轨迹
- 🧬 **单神经元分析**：Sigmoid、Tanh、ReLU、LSTM、GRU神经元
- 🔬 **架构对比实验室**：多架构并排对比、智能推荐

---

## 🚀 快速开始

### 安装

```bash
# 克隆项目
git clone https://github.com/just-for-dream-0x10/Model_explorer.git

# 安装依赖
pip install -r requirements.txt

# 运行应用
streamlit run app.py
```

### 核心依赖
- Python 3.8+
- streamlit（Web界面）
- torch（深度学习框架）
- numpy, pandas（数值计算）
- plotly（交互式可视化）

---

## 🎓 适合人群

### ✅ 适合你，如果你：
- 想理解 `nn.Conv2d`、`nn.LSTM`、`nn.MultiheadAttention` 的内部计算
- 需要快速对比不同架构的参数量和FLOPs
- 想看到梯度消失/爆炸的数值证明
- 需要理解为什么ResNet、LayerNorm、ViT这些技术work
- 想知道何时用CNN、何时用Transformer

### ❌ 不适合你，如果你：
- 想学习"梯度下降为什么收敛"等通用数学理论 → 推荐 `Basic_Math` 项目
- 想深入理解"Attention机制为什么有效" → 推荐 `Transformer_Explorer` 项目
- 想要生产级的模型训练工具 → 推荐 PyTorch/TensorFlow

---

## 📊 项目统计

- **总代码量**：~25,000行
- **模块数量**：18个功能模块
- **支持架构**：CNN, Transformer, RNN, GNN, MoE
- **计算器支持**：8种层类型 + 7个完整网络

---

## 📖 学习路径推荐

### 🟢 初学者（刚接触深度学习）
1. 🧬 单神经元分析 → 理解最基本的计算单元
2. 🖼️ CNN数学 → 从卷积开始
3. 🔬 反向传播原理 → 理解训练过程
4. 🏛️ 失败案例博物馆 → 避开常见坑

### 🟡 中级用户（熟悉基础但想深入）
1. 🔢 参数计算器 → 对比不同架构
2. 🏗️ ResNet/归一化对比 → 理解现代技巧
3. 🔍 ViT分析 → 理解Transformer在CV中的应用
4. 🔬 架构对比实验室 → 学会选择架构

### 🔴 高级用户（想解决具体问题）
1. ⚠️ 稳定性诊断 → 解决训练不稳定
2. ✂️ 模型剪枝 → 模型压缩部署
3. 🧠 MoE分析 → 大模型扩展技术
4. 🎮 交互实验室 → 自定义实验

---

## 🤝 贡献

欢迎贡献！请确保新增模块符合项目定位：

1. **必须回答4个核心问题**（特别是第4个"什么时候会出问题？"）
2. **专注于数值计算细节**，而非宏观理论
3. **提供具体的、可操作的示例**

---

## 📄 许可证

本项目采用 MIT 许可证。

---

## 👨‍💻 开发者

**Just For Dream Lab**

---

**免责声明**：本项目专注于教学和原理展示，代码优先考虑可读性而非生产性能。
