"""
å‚æ•°é‡ä¸è®¡ç®—é‡åˆ†æå·¥å…·
ä¸“æ³¨äºå…·ä½“ç½‘ç»œå±‚çš„è®¡ç®—ç»†èŠ‚åˆ†æ
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from typing import Dict, List, Tuple


class LayerAnalyzer:
    """ç½‘ç»œå±‚åˆ†æå™¨ï¼šè®¡ç®—å‚æ•°é‡ã€FLOPsã€å†…å­˜å ç”¨"""
    
    @staticmethod
    def conv2d_analysis(in_channels: int, out_channels: int, kernel_size: int, 
                       stride: int, padding: int, input_shape: Tuple[int, int, int],
                       use_bias: bool = True) -> Dict:
        """
        åˆ†æ Conv2d å±‚çš„è®¡ç®—ç»†èŠ‚
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•°
            out_channels: è¾“å‡ºé€šé“æ•°
            kernel_size: å·ç§¯æ ¸å¤§å°
            stride: æ­¥é•¿
            padding: å¡«å……
            input_shape: (C, H, W)
            use_bias: æ˜¯å¦ä½¿ç”¨åç½®
            
        Returns:
            åŒ…å«å‚æ•°é‡ã€FLOPsã€å†…å­˜ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        C_in, H_in, W_in = input_shape
        
        # è¾“å‡ºå°ºå¯¸è®¡ç®—
        H_out = (H_in + 2 * padding - kernel_size) // stride + 1
        W_out = (W_in + 2 * padding - kernel_size) // stride + 1
        
        # å‚æ•°é‡è®¡ç®—
        # æƒé‡å‚æ•°: out_channels Ã— in_channels Ã— kernel_size Ã— kernel_size
        weight_params = out_channels * in_channels * kernel_size * kernel_size
        # åç½®å‚æ•°: out_channels (å¦‚æœä½¿ç”¨)
        bias_params = out_channels if use_bias else 0
        total_params = weight_params + bias_params
        
        # FLOPs è®¡ç®—
        # æ¯ä¸ªè¾“å‡ºä½ç½®éœ€è¦: kernel_sizeÂ² Ã— in_channels æ¬¡ä¹˜æ³•
        # è¾“å‡ºä½ç½®æ€»æ•°: out_channels Ã— H_out Ã— W_out
        macs_per_position = kernel_size * kernel_size * in_channels  # ä¹˜åŠ æ“ä½œ
        total_macs = macs_per_position * out_channels * H_out * W_out
        # 1 MAC = 2 FLOPs (1ä¸ªä¹˜æ³• + 1ä¸ªåŠ æ³•)
        total_flops = 2 * total_macs
        
        # å¦‚æœæœ‰åç½®ï¼Œæ¯ä¸ªè¾“å‡ºä½ç½®è¿˜éœ€è¦1æ¬¡åŠ æ³•
        if use_bias:
            total_flops += out_channels * H_out * W_out
        
        # å†…å­˜å ç”¨ (å‡è®¾ FP32, æ¯ä¸ªå‚æ•° 4 bytes)
        param_memory_mb = (total_params * 4) / (1024 ** 2)
        
        # å‰å‘ä¼ æ’­æ¿€æ´»å€¼å†…å­˜
        input_memory = C_in * H_in * W_in * 4 / (1024 ** 2)  # MB
        output_memory = out_channels * H_out * W_out * 4 / (1024 ** 2)  # MB
        forward_memory_mb = input_memory + output_memory
        
        # åå‘ä¼ æ’­éœ€è¦å­˜å‚¨è¾“å…¥å’Œè¾“å‡ºçš„æ¢¯åº¦ï¼Œå†…å­˜ç¿»å€
        backward_memory_mb = forward_memory_mb * 2
        
        return {
            'layer_type': 'Conv2d',
            'input_shape': (C_in, H_in, W_in),
            'output_shape': (out_channels, H_out, W_out),
            'kernel_size': kernel_size,
            'stride': stride,
            'padding': padding,
            'parameters': {
                'weight': weight_params,
                'bias': bias_params,
                'total': total_params
            },
            'flops': {
                'macs': total_macs,
                'total': total_flops,
                'macs_readable': f"{total_macs / 1e6:.2f}M" if total_macs > 1e6 else f"{total_macs / 1e3:.2f}K",
                'flops_readable': f"{total_flops / 1e9:.2f}G" if total_flops > 1e9 else f"{total_flops / 1e6:.2f}M"
            },
            'memory_mb': {
                'parameters': param_memory_mb,
                'forward': forward_memory_mb,
                'backward': backward_memory_mb,
                'total': param_memory_mb + backward_memory_mb
            }
        }
    
    @staticmethod
    def linear_analysis(in_features: int, out_features: int, use_bias: bool = True) -> Dict:
        """
        åˆ†æ Linear (å…¨è¿æ¥) å±‚çš„è®¡ç®—ç»†èŠ‚
        
        Args:
            in_features: è¾“å…¥ç‰¹å¾æ•°
            out_features: è¾“å‡ºç‰¹å¾æ•°
            use_bias: æ˜¯å¦ä½¿ç”¨åç½®
            
        Returns:
            åŒ…å«å‚æ•°é‡ã€FLOPsã€å†…å­˜ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        # å‚æ•°é‡è®¡ç®—
        weight_params = in_features * out_features
        bias_params = out_features if use_bias else 0
        total_params = weight_params + bias_params
        
        # FLOPs è®¡ç®—
        # y = Wx + b
        # çŸ©é˜µä¹˜æ³•: in_features Ã— out_features æ¬¡ä¹˜åŠ æ“ä½œ
        total_macs = in_features * out_features
        total_flops = 2 * total_macs
        if use_bias:
            total_flops += out_features
        
        # å†…å­˜å ç”¨ (FP32)
        param_memory_mb = (total_params * 4) / (1024 ** 2)
        
        return {
            'layer_type': 'Linear',
            'input_features': in_features,
            'output_features': out_features,
            'parameters': {
                'weight': weight_params,
                'bias': bias_params,
                'total': total_params
            },
            'flops': {
                'macs': total_macs,
                'total': total_flops,
                'macs_readable': f"{total_macs / 1e6:.2f}M" if total_macs > 1e6 else f"{total_macs / 1e3:.2f}K",
                'flops_readable': f"{total_flops / 1e9:.2f}G" if total_flops > 1e9 else f"{total_flops / 1e6:.2f}M"
            },
            'memory_mb': {
                'parameters': param_memory_mb
            }
        }
    
    @staticmethod
    def attention_analysis(d_model: int, num_heads: int, seq_len: int, 
                          has_qkv_bias: bool = True) -> Dict:
        """
        åˆ†æ Multi-Head Self-Attention å±‚çš„è®¡ç®—ç»†èŠ‚
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            seq_len: åºåˆ—é•¿åº¦
            has_qkv_bias: QKVæŠ•å½±æ˜¯å¦ä½¿ç”¨åç½®
            
        Returns:
            åŒ…å«å‚æ•°é‡ã€FLOPsã€å†…å­˜ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        # å‚æ•°é‡è®¡ç®—
        # Q, K, V æŠ•å½±: 3 Ã— (d_model Ã— d_model)
        qkv_params = 3 * d_model * d_model
        qkv_bias = 3 * d_model if has_qkv_bias else 0
        
        # è¾“å‡ºæŠ•å½±: d_model Ã— d_model
        out_params = d_model * d_model
        out_bias = d_model if has_qkv_bias else 0
        
        total_params = qkv_params + qkv_bias + out_params + out_bias
        
        # FLOPs è®¡ç®—
        # 1. QKV æŠ•å½±: 3 Ã— seq_len Ã— d_model Ã— d_model Ã— 2 (çŸ©é˜µä¹˜æ³•)
        qkv_flops = 3 * seq_len * d_model * d_model * 2
        
        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q @ K^T
        #    æ¯ä¸ªå¤´: seq_len Ã— seq_len Ã— (d_model/num_heads)
        #    æ‰€æœ‰å¤´: num_heads Ã— seq_len Ã— seq_len Ã— (d_model/num_heads) Ã— 2
        attn_score_flops = num_heads * seq_len * seq_len * (d_model // num_heads) * 2
        
        # 3. Softmax: çº¦ seq_len Ã— seq_len Ã— num_heads Ã— 5 (exp, sum, divç­‰)
        softmax_flops = seq_len * seq_len * num_heads * 5
        
        # 4. æ³¨æ„åŠ›åŠ æƒ: attn @ V
        attn_value_flops = num_heads * seq_len * seq_len * (d_model // num_heads) * 2
        
        # 5. è¾“å‡ºæŠ•å½±: seq_len Ã— d_model Ã— d_model Ã— 2
        out_proj_flops = seq_len * d_model * d_model * 2
        
        total_flops = qkv_flops + attn_score_flops + softmax_flops + attn_value_flops + out_proj_flops
        
        # å†…å­˜å ç”¨
        param_memory_mb = (total_params * 4) / (1024 ** 2)
        
        # æ³¨æ„åŠ›çŸ©é˜µ: num_heads Ã— seq_len Ã— seq_len
        attn_matrix_memory = (num_heads * seq_len * seq_len * 4) / (1024 ** 2)
        
        return {
            'layer_type': 'MultiHeadAttention',
            'd_model': d_model,
            'num_heads': num_heads,
            'seq_len': seq_len,
            'parameters': {
                'qkv_weight': qkv_params,
                'qkv_bias': qkv_bias,
                'out_weight': out_params,
                'out_bias': out_bias,
                'total': total_params
            },
            'flops': {
                'qkv_proj': qkv_flops,
                'attn_score': attn_score_flops,
                'softmax': softmax_flops,
                'attn_value': attn_value_flops,
                'out_proj': out_proj_flops,
                'total': total_flops,
                'flops_readable': f"{total_flops / 1e9:.2f}G" if total_flops > 1e9 else f"{total_flops / 1e6:.2f}M"
            },
            'memory_mb': {
                'parameters': param_memory_mb,
                'attention_matrix': attn_matrix_memory,
                'total': param_memory_mb + attn_matrix_memory
            }
        }
    
    @staticmethod
    def depthwise_conv2d_analysis(in_channels: int, kernel_size: int, 
                                  stride: int, padding: int, 
                                  input_shape: Tuple[int, int, int],
                                  use_bias: bool = True) -> Dict:
        """
        åˆ†æ Depthwise Convolution çš„è®¡ç®—ç»†èŠ‚
        (MobileNetä¸­ä½¿ç”¨çš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„ç¬¬ä¸€æ­¥)
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•° (ä¹Ÿæ˜¯è¾“å‡ºé€šé“æ•°)
            kernel_size: å·ç§¯æ ¸å¤§å°
            stride: æ­¥é•¿
            padding: å¡«å……
            input_shape: (C, H, W)
            use_bias: æ˜¯å¦ä½¿ç”¨åç½®
            
        Returns:
            åŒ…å«å‚æ•°é‡ã€FLOPsã€å†…å­˜ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        C_in, H_in, W_in = input_shape
        
        # è¾“å‡ºå°ºå¯¸
        H_out = (H_in + 2 * padding - kernel_size) // stride + 1
        W_out = (W_in + 2 * padding - kernel_size) // stride + 1
        
        # å‚æ•°é‡: æ¯ä¸ªè¾“å…¥é€šé“ä¸€ä¸ªç‹¬ç«‹çš„å·ç§¯æ ¸
        weight_params = in_channels * kernel_size * kernel_size
        bias_params = in_channels if use_bias else 0
        total_params = weight_params + bias_params
        
        # FLOPs: ç›¸æ¯”æ ‡å‡†å·ç§¯å¤§å¹…å‡å°‘
        total_macs = in_channels * kernel_size * kernel_size * H_out * W_out
        total_flops = 2 * total_macs
        if use_bias:
            total_flops += in_channels * H_out * W_out
        
        param_memory_mb = (total_params * 4) / (1024 ** 2)
        
        return {
            'layer_type': 'DepthwiseConv2d',
            'input_shape': (C_in, H_in, W_in),
            'output_shape': (in_channels, H_out, W_out),
            'kernel_size': kernel_size,
            'stride': stride,
            'padding': padding,
            'parameters': {
                'weight': weight_params,
                'bias': bias_params,
                'total': total_params
            },
            'flops': {
                'macs': total_macs,
                'total': total_flops,
                'flops_readable': f"{total_flops / 1e9:.2f}G" if total_flops > 1e9 else f"{total_flops / 1e6:.2f}M"
            },
            'memory_mb': {
                'parameters': param_memory_mb
            }
        }
    
    @staticmethod
    def lstm_analysis(input_size: int, hidden_size: int, num_layers: int = 1,
                     bias: bool = True, bidirectional: bool = False) -> Dict:
        """
        åˆ†æ LSTM å±‚çš„è®¡ç®—ç»†èŠ‚
        
        Args:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_size: éšè—å±‚ç»´åº¦
            num_layers: LSTMå±‚æ•°
            bias: æ˜¯å¦ä½¿ç”¨åç½®
            bidirectional: æ˜¯å¦åŒå‘
            
        Returns:
            åŒ…å«å‚æ•°é‡ã€FLOPsã€å†…å­˜ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        # LSTMæœ‰4ä¸ªé—¨: input, forget, cell, output
        num_gates = 4
        num_directions = 2 if bidirectional else 1
        
        # ç¬¬ä¸€å±‚å‚æ•°é‡
        # input-to-hidden: input_size Ã— hidden_size Ã— 4
        # hidden-to-hidden: hidden_size Ã— hidden_size Ã— 4
        first_layer_params = num_gates * (input_size * hidden_size + hidden_size * hidden_size)
        if bias:
            first_layer_params += num_gates * hidden_size * 2  # ihå’Œhhçš„bias
        
        # å…¶ä»–å±‚å‚æ•°é‡
        other_layers_params = 0
        if num_layers > 1:
            input_size_other = hidden_size * num_directions
            other_layer_params = num_gates * (input_size_other * hidden_size + hidden_size * hidden_size)
            if bias:
                other_layer_params += num_gates * hidden_size * 2
            other_layers_params = other_layer_params * (num_layers - 1)
        
        # æ€»å‚æ•°é‡
        params_per_direction = first_layer_params + other_layers_params
        total_params = params_per_direction * num_directions
        
        # FLOPsè®¡ç®— (per timestep)
        # æ¯ä¸ªæ—¶é—´æ­¥: 4ä¸ªé—¨ Ã— (input_mm + hidden_mm + pointwise_ops)
        first_layer_flops = num_gates * (2 * input_size * hidden_size + 2 * hidden_size * hidden_size + 3 * hidden_size)
        
        other_layers_flops = 0
        if num_layers > 1:
            input_size_other = hidden_size * num_directions
            other_layer_flops = num_gates * (2 * input_size_other * hidden_size + 2 * hidden_size * hidden_size + 3 * hidden_size)
            other_layers_flops = other_layer_flops * (num_layers - 1)
        
        flops_per_timestep = (first_layer_flops + other_layers_flops) * num_directions
        
        param_memory_mb = (total_params * 4) / (1024 ** 2)
        
        return {
            'layer_type': 'LSTM',
            'input_size': input_size,
            'hidden_size': hidden_size,
            'num_layers': num_layers,
            'bidirectional': bidirectional,
            'parameters': {
                'total': total_params,
                'per_layer': total_params // (num_layers * num_directions)
            },
            'flops': {
                'per_timestep': flops_per_timestep,
                'flops_readable': f"{flops_per_timestep / 1e6:.2f}M per timestep"
            },
            'memory_mb': {
                'parameters': param_memory_mb
            }
        }
    
    @staticmethod
    def layernorm_analysis(normalized_shape: int, input_shape: Tuple) -> Dict:
        """
        åˆ†æ LayerNorm å±‚çš„è®¡ç®—ç»†èŠ‚
        
        Args:
            normalized_shape: å½’ä¸€åŒ–çš„ç»´åº¦
            input_shape: è¾“å…¥å½¢çŠ¶
            
        Returns:
            åŒ…å«å‚æ•°é‡ã€FLOPsã€å†…å­˜ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        # å‚æ•°é‡: gamma å’Œ beta
        total_params = 2 * normalized_shape
        
        # FLOPs: æ¯ä¸ªå…ƒç´ éœ€è¦è®¡ç®—å‡å€¼ã€æ–¹å·®ã€å½’ä¸€åŒ–ã€scaleå’Œshift
        total_elements = np.prod(input_shape)
        total_flops = 5 * total_elements
        
        param_memory_mb = (total_params * 4) / (1024 ** 2)
        
        return {
            'layer_type': 'LayerNorm',
            'normalized_shape': normalized_shape,
            'input_shape': input_shape,
            'parameters': {
                'gamma': normalized_shape,
                'beta': normalized_shape,
                'total': total_params
            },
            'flops': {
                'total': total_flops,
                'flops_readable': f"{total_flops / 1e6:.2f}M" if total_flops > 1e6 else f"{total_flops / 1e3:.2f}K"
            },
            'memory_mb': {
                'parameters': param_memory_mb
            }
        }
    
    @staticmethod
    def embedding_analysis(num_embeddings: int, embedding_dim: int) -> Dict:
        """
        åˆ†æ Embedding å±‚çš„è®¡ç®—ç»†èŠ‚
        
        Args:
            num_embeddings: è¯è¡¨å¤§å°
            embedding_dim: åµŒå…¥ç»´åº¦
            
        Returns:
            åŒ…å«å‚æ•°é‡ã€FLOPsã€å†…å­˜ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        # å‚æ•°é‡
        total_params = num_embeddings * embedding_dim
        
        # FLOPs: æŸ¥è¡¨æ“ä½œï¼Œå‡ ä¹ä¸º0
        total_flops = 0
        
        param_memory_mb = (total_params * 4) / (1024 ** 2)
        
        return {
            'layer_type': 'Embedding',
            'num_embeddings': num_embeddings,
            'embedding_dim': embedding_dim,
            'parameters': {
                'total': total_params
            },
            'flops': {
                'total': total_flops,
                'flops_readable': "~0 (lookup)"
            },
            'memory_mb': {
                'parameters': param_memory_mb
            }
        }
    
    @staticmethod
    def batchnorm2d_analysis(num_features: int, input_shape: Tuple[int, int, int]) -> Dict:
        """
        åˆ†æ BatchNorm2d å±‚çš„è®¡ç®—ç»†èŠ‚
        
        Args:
            num_features: é€šé“æ•°
            input_shape: (C, H, W)
            
        Returns:
            åŒ…å«å‚æ•°é‡ã€FLOPsã€å†…å­˜ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        C, H, W = input_shape
        
        # å‚æ•°é‡: gamma (scale) å’Œ beta (shift)
        total_params = 2 * num_features
        
        # FLOPs è®¡ç®—
        # æ¯ä¸ªå…ƒç´ : (x - mean) / sqrt(var + eps) * gamma + beta
        # = å‡æ³• + é™¤æ³• + ä¹˜æ³• + åŠ æ³• = 4 ops per element
        total_elements = C * H * W
        total_flops = 4 * total_elements
        
        param_memory_mb = (total_params * 4) / (1024 ** 2)
        
        return {
            'layer_type': 'BatchNorm2d',
            'num_features': num_features,
            'input_shape': input_shape,
            'parameters': {
                'gamma': num_features,
                'beta': num_features,
                'total': total_params
            },
            'flops': {
                'total': total_flops,
                'flops_readable': f"{total_flops / 1e6:.2f}M" if total_flops > 1e6 else f"{total_flops / 1e3:.2f}K"
            },
            'memory_mb': {
                'parameters': param_memory_mb
            }
        }


def params_calculator_tab():
    """å‚æ•°é‡ä¸FLOPsè®¡ç®—å™¨æ ‡ç­¾é¡µ"""
    
    st.header("ğŸ”¢ å‚æ•°é‡ä¸FLOPsè®¡ç®—å™¨")
    
    st.markdown("""
    ### æ ¸å¿ƒåŠŸèƒ½ï¼šé€å±‚åˆ†æç½‘ç»œè®¡ç®—ç»†èŠ‚
    
    è¾“å…¥ç½‘ç»œå±‚çš„é…ç½®ï¼Œè‡ªåŠ¨è®¡ç®—ï¼š
    - ğŸ“Š å‚æ•°é‡ï¼ˆParamsï¼‰
    - ğŸ“ˆ æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPs / MACsï¼‰
    - ğŸ’¾ å†…å­˜å ç”¨ï¼ˆå‰å‘/åå‘ä¼ æ’­ï¼‰
    - ğŸ” è¾“å‡ºç‰¹å¾å›¾å°ºå¯¸
    
    **ä¸ torchinfo çš„åŒºåˆ«**ï¼šæˆ‘ä»¬ä¸ä»…ç»™å‡ºæ•°å­—ï¼Œè¿˜å±•ç¤º**æ¯ä¸ªæ•°å­—èƒŒåçš„è®¡ç®—å…¬å¼**ï¼
    """)
    
    # é€‰æ‹©å±‚ç±»å‹
    layer_type = st.selectbox(
        "é€‰æ‹©ç½‘ç»œå±‚ç±»å‹",
        [
            "Conv2d (æ ‡å‡†å·ç§¯å±‚)",
            "DepthwiseConv2d (æ·±åº¦å¯åˆ†ç¦»å·ç§¯)",
            "Linear (å…¨è¿æ¥å±‚)",
            "MultiHeadAttention (å¤šå¤´æ³¨æ„åŠ›)",
            "LSTM (é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ)",
            "Embedding (åµŒå…¥å±‚)",
            "BatchNorm2d (æ‰¹å½’ä¸€åŒ–)",
            "LayerNorm (å±‚å½’ä¸€åŒ–)"
        ]
    )
    
    analyzer = LayerAnalyzer()
    
    if "Conv2d" in layer_type:
        st.markdown("### ğŸ–¼ï¸ Conv2d å·ç§¯å±‚åˆ†æ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**è¾“å…¥é…ç½®**")
            C_in = st.number_input("è¾“å…¥é€šé“æ•° (in_channels)", min_value=1, value=3, step=1)
            H_in = st.number_input("è¾“å…¥é«˜åº¦ (H)", min_value=1, value=224, step=1)
            W_in = st.number_input("è¾“å…¥å®½åº¦ (W)", min_value=1, value=224, step=1)
        
        with col2:
            st.markdown("**å±‚å‚æ•°é…ç½®**")
            C_out = st.number_input("è¾“å‡ºé€šé“æ•° (out_channels)", min_value=1, value=64, step=1)
            kernel_size = st.number_input("å·ç§¯æ ¸å¤§å° (kernel_size)", min_value=1, value=7, step=1)
            stride = st.number_input("æ­¥é•¿ (stride)", min_value=1, value=2, step=1)
            padding = st.number_input("å¡«å…… (padding)", min_value=0, value=3, step=1)
            use_bias = st.checkbox("ä½¿ç”¨åç½® (bias)", value=True)
        
        # è®¡ç®—åˆ†æ
        result = analyzer.conv2d_analysis(
            in_channels=C_in,
            out_channels=C_out,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            input_shape=(C_in, H_in, W_in),
            use_bias=use_bias
        )
        
        # æ˜¾ç¤ºç»“æœ
        st.markdown("---")
        st.markdown("### ğŸ“Š åˆ†æç»“æœ")
        
        # è¾“å‡ºå½¢çŠ¶
        st.markdown("#### 1ï¸âƒ£ è¾“å‡ºç‰¹å¾å›¾å°ºå¯¸")
        C_out_calc, H_out, W_out = result['output_shape']
        
        st.latex(r"H_{out} = \left\lfloor \frac{H_{in} + 2 \times padding - kernel\_size}{stride} \right\rfloor + 1")
        st.latex(r"W_{out} = \left\lfloor \frac{W_{in} + 2 \times padding - kernel\_size}{stride} \right\rfloor + 1")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("è¾“å…¥å½¢çŠ¶", f"[{C_in}, {H_in}, {W_in}]")
        with col2:
            st.metric("è¾“å‡ºå½¢çŠ¶", f"[{C_out_calc}, {H_out}, {W_out}]")
        with col3:
            reduction = ((H_in * W_in) - (H_out * W_out)) / (H_in * W_in) * 100
            st.metric("ç©ºé—´é™é‡‡æ ·", f"{reduction:.1f}%")
        
        # å‚æ•°é‡
        st.markdown("#### 2ï¸âƒ£ å‚æ•°é‡è®¡ç®—")
        st.latex(r"Params_{weight} = C_{out} \times C_{in} \times K_h \times K_w")
        if use_bias:
            st.latex(r"Params_{bias} = C_{out}")
            st.latex(r"Params_{total} = Params_{weight} + Params_{bias}")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æƒé‡å‚æ•°", f"{result['parameters']['weight']:,}")
        with col2:
            st.metric("åç½®å‚æ•°", f"{result['parameters']['bias']:,}")
        with col3:
            st.metric("æ€»å‚æ•°é‡", f"{result['parameters']['total']:,}")
        
        # è¯¦ç»†è®¡ç®—è¿‡ç¨‹
        with st.expander("ğŸ“– æŸ¥çœ‹è¯¦ç»†è®¡ç®—è¿‡ç¨‹"):
            st.code(f"""
è®¡ç®—è¿‡ç¨‹ï¼š
æƒé‡å‚æ•° = {C_out} Ã— {C_in} Ã— {kernel_size} Ã— {kernel_size}
        = {result['parameters']['weight']:,}

åç½®å‚æ•° = {C_out if use_bias else 0}

æ€»å‚æ•°é‡ = {result['parameters']['weight']:,} + {result['parameters']['bias']}
        = {result['parameters']['total']:,}
            """)
        
        # FLOPs
        st.markdown("#### 3ï¸âƒ£ æµ®ç‚¹è¿ç®—é‡ (FLOPs)")
        st.latex(r"MACs = K_h \times K_w \times C_{in} \times C_{out} \times H_{out} \times W_{out}")
        st.latex(r"FLOPs = 2 \times MACs" + (r" + C_{out} \times H_{out} \times W_{out}" if use_bias else ""))
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("MACs", result['flops']['macs_readable'])
        with col2:
            st.metric("FLOPs", result['flops']['flops_readable'])
        
        with st.expander("ğŸ“– æŸ¥çœ‹è¯¦ç»†è®¡ç®—è¿‡ç¨‹"):
            st.code(f"""
è®¡ç®—è¿‡ç¨‹ï¼š
æ¯ä¸ªè¾“å‡ºä½ç½®çš„ä¹˜åŠ æ“ä½œæ•° (MACs per position):
    = {kernel_size} Ã— {kernel_size} Ã— {C_in}
    = {kernel_size * kernel_size * C_in}

è¾“å‡ºä½ç½®æ€»æ•°:
    = {C_out} Ã— {H_out} Ã— {W_out}
    = {C_out * H_out * W_out:,}

æ€» MACs:
    = {kernel_size * kernel_size * C_in} Ã— {C_out * H_out * W_out:,}
    = {result['flops']['macs']:,}

æ€» FLOPs (1 MAC = 2 FLOPs):
    = 2 Ã— {result['flops']['macs']:,}{' + ' + str(C_out * H_out * W_out) if use_bias else ''}
    = {result['flops']['total']:,}
            """)
        
        # å†…å­˜å ç”¨
        st.markdown("#### 4ï¸âƒ£ å†…å­˜å ç”¨ (FP32)")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å‚æ•°å†…å­˜", f"{result['memory_mb']['parameters']:.4f} MB")
        with col2:
            st.metric("å‰å‘ä¼ æ’­", f"{result['memory_mb']['forward']:.4f} MB")
        with col3:
            st.metric("åå‘ä¼ æ’­", f"{result['memory_mb']['backward']:.4f} MB")
        
        # å¯è§†åŒ–å¯¹æ¯”
        st.markdown("#### 5ï¸âƒ£ ç›´è§‚å¯¹æ¯”")
        
        # åˆ›å»ºé¥¼å›¾
        fig = go.Figure(data=[go.Pie(
            labels=['æƒé‡å‚æ•°', 'åç½®å‚æ•°'] if use_bias else ['æƒé‡å‚æ•°'],
            values=[result['parameters']['weight'], result['parameters']['bias']] if use_bias else [result['parameters']['weight']],
            hole=.3
        )])
        fig.update_layout(title_text="å‚æ•°é‡åˆ†å¸ƒ", height=400)
        st.plotly_chart(fig, width='stretch')
        
    elif "Linear" in layer_type:
        st.markdown("### ğŸ”— Linear å…¨è¿æ¥å±‚åˆ†æ")
        
        col1, col2 = st.columns(2)
        with col1:
            in_features = st.number_input("è¾“å…¥ç‰¹å¾æ•° (in_features)", min_value=1, value=512, step=1)
        with col2:
            out_features = st.number_input("è¾“å‡ºç‰¹å¾æ•° (out_features)", min_value=1, value=1000, step=1)
        
        use_bias = st.checkbox("ä½¿ç”¨åç½® (bias)", value=True, key="linear_bias")
        
        result = analyzer.linear_analysis(in_features, out_features, use_bias)
        
        st.markdown("---")
        st.markdown("### ğŸ“Š åˆ†æç»“æœ")
        
        # å‚æ•°é‡
        st.markdown("#### å‚æ•°é‡è®¡ç®—")
        st.latex(r"Params_{weight} = in\_features \times out\_features")
        if use_bias:
            st.latex(r"Params_{total} = Params_{weight} + out\_features")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æƒé‡å‚æ•°", f"{result['parameters']['weight']:,}")
        with col2:
            st.metric("åç½®å‚æ•°", f"{result['parameters']['bias']:,}")
        with col3:
            st.metric("æ€»å‚æ•°é‡", f"{result['parameters']['total']:,}")
        
        # FLOPs
        st.markdown("#### æµ®ç‚¹è¿ç®—é‡")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("MACs", result['flops']['macs_readable'])
        with col2:
            st.metric("FLOPs", result['flops']['flops_readable'])
        
        # è­¦å‘Šï¼šå…¨è¿æ¥å±‚å‚æ•°é‡é—®é¢˜
        if result['parameters']['total'] > 1e6:
            st.warning(f"""
            âš ï¸ **å‚æ•°é‡è­¦å‘Š**
            
            è¯¥å…¨è¿æ¥å±‚æœ‰ **{result['parameters']['total']:,}** ä¸ªå‚æ•°ï¼ˆ>{result['parameters']['total']/1e6:.1f}Mï¼‰ï¼
            
            **å¸¸è§é—®é¢˜**ï¼š
            - å…¨è¿æ¥å±‚é€šå¸¸æ˜¯ç½‘ç»œä¸­å‚æ•°é‡æœ€å¤šçš„éƒ¨åˆ†
            - è€ƒè™‘ä½¿ç”¨ Global Average Pooling æ›¿ä»£
            - æˆ–è€…å‡å°‘è¾“å…¥ç‰¹å¾æ•°
            """)
    
    elif "BatchNorm" in layer_type:
        st.markdown("### ğŸ“ BatchNorm2d æ‰¹å½’ä¸€åŒ–å±‚åˆ†æ")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            num_features = st.number_input("é€šé“æ•° (num_features)", min_value=1, value=64, step=1)
        with col2:
            H = st.number_input("ç‰¹å¾å›¾é«˜åº¦ (H)", min_value=1, value=56, step=1)
        with col3:
            W = st.number_input("ç‰¹å¾å›¾å®½åº¦ (W)", min_value=1, value=56, step=1)
        
        result = analyzer.batchnorm2d_analysis(num_features, (num_features, H, W))
        
        st.markdown("---")
        st.markdown("### ğŸ“Š åˆ†æç»“æœ")
        
        st.markdown("#### å‚æ•°é‡")
        st.latex(r"Params_{total} = 2 \times num\_features \quad (\gamma \text{ å’Œ } \beta)")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Gamma (scale)", f"{result['parameters']['gamma']}")
        with col2:
            st.metric("Beta (shift)", f"{result['parameters']['beta']}")
        
        st.info("""
        ğŸ’¡ **BatchNorm å‚æ•°é‡å¾ˆå°**
        
        BatchNorm åªæœ‰ 2 Ã— é€šé“æ•° ä¸ªå¯å­¦ä¹ å‚æ•°ï¼Œä¸»è¦å¼€é”€åœ¨äºè®¡ç®—å‡å€¼å’Œæ–¹å·®ã€‚
        """)
    
    elif "DepthwiseConv2d" in layer_type:
        st.markdown("### ğŸ“± DepthwiseConv2d æ·±åº¦å¯åˆ†ç¦»å·ç§¯åˆ†æ")
        
        st.info("""
        ğŸ’¡ **MobileNetçš„æ ¸å¿ƒæŠ€æœ¯**
        
        æ·±åº¦å¯åˆ†ç¦»å·ç§¯å°†æ ‡å‡†å·ç§¯åˆ†è§£ä¸ºï¼š
        1. Depthwise Convolution (é€é€šé“å·ç§¯)
        2. Pointwise Convolution (1Ã—1å·ç§¯)
        
        å¤§å¹…å‡å°‘å‚æ•°é‡å’Œè®¡ç®—é‡ï¼
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**è¾“å…¥é…ç½®**")
            C_in = st.number_input("è¾“å…¥é€šé“æ•°", min_value=1, value=64, step=1, key="dw_cin")
            H_in = st.number_input("è¾“å…¥é«˜åº¦", min_value=1, value=56, step=1, key="dw_hin")
            W_in = st.number_input("è¾“å…¥å®½åº¦", min_value=1, value=56, step=1, key="dw_win")
        
        with col2:
            st.markdown("**å±‚å‚æ•°é…ç½®**")
            kernel_size = st.number_input("å·ç§¯æ ¸å¤§å°", min_value=1, value=3, step=1, key="dw_kernel")
            stride = st.number_input("æ­¥é•¿", min_value=1, value=1, step=1, key="dw_stride")
            padding = st.number_input("å¡«å……", min_value=0, value=1, step=1, key="dw_padding")
            use_bias = st.checkbox("ä½¿ç”¨åç½®", value=True, key="dw_bias")
        
        # è®¡ç®—Depthwiseå·ç§¯
        result_dw = analyzer.depthwise_conv2d_analysis(
            C_in, kernel_size, stride, padding, (C_in, H_in, W_in), use_bias
        )
        
        # è®¡ç®—æ ‡å‡†å·ç§¯ä½œä¸ºå¯¹æ¯”
        result_std = analyzer.conv2d_analysis(
            C_in, C_in, kernel_size, stride, padding, (C_in, H_in, W_in), use_bias
        )
        
        st.markdown("---")
        st.markdown("### ğŸ“Š å¯¹æ¯”åˆ†æï¼šDepthwise vs æ ‡å‡†å·ç§¯")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**å‚æ•°é‡å¯¹æ¯”**")
            dw_params = result_dw['parameters']['total']
            std_params = result_std['parameters']['total']
            reduction = (1 - dw_params / std_params) * 100
            
            st.metric("Depthwise", f"{dw_params:,}")
            st.metric("æ ‡å‡†å·ç§¯", f"{std_params:,}")
            st.metric("å‚æ•°å‡å°‘", f"{reduction:.1f}%", delta=f"-{std_params - dw_params:,}")
        
        with col2:
            st.markdown("**FLOPså¯¹æ¯”**")
            dw_flops = result_dw['flops']['total']
            std_flops = result_std['flops']['total']
            flops_reduction = (1 - dw_flops / std_flops) * 100
            
            st.metric("Depthwise", result_dw['flops']['flops_readable'])
            st.metric("æ ‡å‡†å·ç§¯", result_std['flops']['flops_readable'])
            st.metric("è®¡ç®—å‡å°‘", f"{flops_reduction:.1f}%")
        
        with col3:
            st.markdown("**è¾“å‡ºå½¢çŠ¶**")
            st.metric("è¾“å…¥", f"{(C_in, H_in, W_in)}")
            st.metric("è¾“å‡º", f"{result_dw['output_shape']}")
        
        # è¯¦ç»†è¯´æ˜
        with st.expander("ğŸ“– ä¸ºä»€ä¹ˆå‚æ•°é‡å¤§å¹…å‡å°‘ï¼Ÿ"):
            st.markdown(f"""
            **æ ‡å‡†å·ç§¯å‚æ•°é‡**:
            ```
            C_out Ã— C_in Ã— K Ã— K
            = {C_in} Ã— {C_in} Ã— {kernel_size} Ã— {kernel_size}
            = {std_params:,}
            ```
            
            **Depthwiseå·ç§¯å‚æ•°é‡**:
            ```
            C_in Ã— K Ã— K  (æ¯ä¸ªé€šé“ç‹¬ç«‹çš„å·ç§¯æ ¸)
            = {C_in} Ã— {kernel_size} Ã— {kernel_size}
            = {dw_params:,}
            ```
            
            **å‡å°‘å› å­**: çº¦ **1/{C_in}** = 1/{C_in} â‰ˆ {std_params/dw_params:.1f}x
            """)
    
    elif "MultiHeadAttention" in layer_type:
        st.markdown("### ğŸ¯ Multi-Head Self-Attention åˆ†æ")
        
        st.info("""
        ğŸ’¡ **Transformerçš„æ ¸å¿ƒç»„ä»¶**
        
        å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®©æ¨¡å‹å…³æ³¨è¾“å…¥çš„ä¸åŒè¡¨ç¤ºå­ç©ºé—´ã€‚
        """)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            d_model = st.number_input("æ¨¡å‹ç»´åº¦ (d_model)", min_value=64, max_value=2048, value=512, step=64, key="attn_d")
        with col2:
            num_heads = st.number_input("æ³¨æ„åŠ›å¤´æ•°", min_value=1, max_value=32, value=8, step=1, key="attn_heads")
        with col3:
            seq_len = st.number_input("åºåˆ—é•¿åº¦", min_value=1, max_value=2048, value=128, step=1, key="attn_seq")
        
        has_qkv_bias = st.checkbox("QKVæŠ•å½±ä½¿ç”¨åç½®", value=True, key="attn_bias")
        
        # æ£€æŸ¥d_modelæ˜¯å¦èƒ½è¢«num_headsæ•´é™¤
        if d_model % num_heads != 0:
            st.error(f"âš ï¸ d_model ({d_model}) å¿…é¡»èƒ½è¢« num_heads ({num_heads}) æ•´é™¤ï¼")
            st.stop()
        
        result = analyzer.attention_analysis(d_model, num_heads, seq_len, has_qkv_bias)
        
        st.markdown("---")
        st.markdown("### ğŸ“Š åˆ†æç»“æœ")
        
        # åŸºæœ¬ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ¯ä¸ªå¤´çš„ç»´åº¦", f"{d_model // num_heads}")
        with col2:
            st.metric("æ€»å‚æ•°é‡", f"{result['parameters']['total']:,}")
        with col3:
            st.metric("æ€»FLOPs", result['flops']['flops_readable'])
        
        # å‚æ•°é‡åˆ†è§£
        st.markdown("#### å‚æ•°é‡åˆ†è§£")
        
        params_breakdown = pd.DataFrame({
            'ç»„ä»¶': ['QæŠ•å½±', 'KæŠ•å½±', 'VæŠ•å½±', 'QKVåç½®', 'è¾“å‡ºæŠ•å½±', 'è¾“å‡ºåç½®'],
            'å‚æ•°é‡': [
                d_model * d_model,
                d_model * d_model,
                d_model * d_model,
                3 * d_model if has_qkv_bias else 0,
                d_model * d_model,
                d_model if has_qkv_bias else 0
            ],
            'å æ¯”': [
                f"{d_model * d_model / result['parameters']['total'] * 100:.1f}%",
                f"{d_model * d_model / result['parameters']['total'] * 100:.1f}%",
                f"{d_model * d_model / result['parameters']['total'] * 100:.1f}%",
                f"{(3 * d_model if has_qkv_bias else 0) / result['parameters']['total'] * 100:.1f}%",
                f"{d_model * d_model / result['parameters']['total'] * 100:.1f}%",
                f"{(d_model if has_qkv_bias else 0) / result['parameters']['total'] * 100:.1f}%"
            ]
        })
        
        st.dataframe(params_breakdown, use_container_width=True)
        
        # FLOPsåˆ†è§£
        st.markdown("#### FLOPsåˆ†è§£")
        
        flops_breakdown = pd.DataFrame({
            'æ“ä½œ': ['QKVæŠ•å½±', 'æ³¨æ„åŠ›åˆ†æ•°(Q@K^T)', 'Softmax', 'æ³¨æ„åŠ›åŠ æƒ(Attn@V)', 'è¾“å‡ºæŠ•å½±'],
            'FLOPs': [
                result['flops']['qkv_proj'],
                result['flops']['attn_score'],
                result['flops']['softmax'],
                result['flops']['attn_value'],
                result['flops']['out_proj']
            ],
            'å æ¯”': [
                f"{result['flops']['qkv_proj'] / result['flops']['total'] * 100:.1f}%",
                f"{result['flops']['attn_score'] / result['flops']['total'] * 100:.1f}%",
                f"{result['flops']['softmax'] / result['flops']['total'] * 100:.1f}%",
                f"{result['flops']['attn_value'] / result['flops']['total'] * 100:.1f}%",
                f"{result['flops']['out_proj'] / result['flops']['total'] * 100:.1f}%"
            ]
        })
        
        st.dataframe(flops_breakdown, use_container_width=True)
        
        # å¯è§†åŒ–
        fig = go.Figure(data=[go.Pie(
            labels=flops_breakdown['æ“ä½œ'],
            values=flops_breakdown['FLOPs'],
            hole=.3
        )])
        fig.update_layout(title="FLOPsåˆ†å¸ƒ", height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        # å¤æ‚åº¦åˆ†æ
        st.markdown("#### ğŸ”¬ å¤æ‚åº¦åˆ†æ")
        
        st.markdown(f"""
        **æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦**:
        - è®¡ç®—æ³¨æ„åŠ›çŸ©é˜µ: O(seq_lenÂ²) = O({seq_len}Â²) = {seq_len**2:,} ä¸ªä½ç½®
        - å½“åºåˆ—é•¿åº¦å¢åŠ æ—¶ï¼Œè®¡ç®—é‡å’Œå†…å­˜å ç”¨å‘ˆ**å¹³æ–¹å¢é•¿**
        - å†…å­˜å ç”¨ï¼ˆæ³¨æ„åŠ›çŸ©é˜µï¼‰: {result['memory_mb']['attention_matrix']:.4f} MB
        
        **ä¼˜åŒ–å»ºè®®**:
        - ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ› (Sparse Attention)
        - ä½¿ç”¨çº¿æ€§æ³¨æ„åŠ› (Linear Attention)
        - ä½¿ç”¨å±€éƒ¨çª—å£æ³¨æ„åŠ› (å¦‚ Swin Transformer)
        """)
    
    elif "LSTM" in layer_type:
        st.markdown("### ğŸ”„ LSTM é•¿çŸ­æœŸè®°å¿†ç½‘ç»œåˆ†æ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ç½‘ç»œé…ç½®**")
            input_size = st.number_input("è¾“å…¥ç»´åº¦", min_value=1, value=128, step=1, key="lstm_in")
            hidden_size = st.number_input("éšè—å±‚ç»´åº¦", min_value=1, value=256, step=1, key="lstm_hidden")
        
        with col2:
            st.markdown("**å±‚é…ç½®**")
            num_layers = st.number_input("å±‚æ•°", min_value=1, max_value=10, value=2, step=1, key="lstm_layers")
            bidirectional = st.checkbox("åŒå‘LSTM", value=False, key="lstm_bi")
            use_bias = st.checkbox("ä½¿ç”¨åç½®", value=True, key="lstm_bias")
        
        result = analyzer.lstm_analysis(input_size, hidden_size, num_layers, use_bias, bidirectional)
        
        st.markdown("---")
        st.markdown("### ğŸ“Š åˆ†æç»“æœ")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ€»å‚æ•°é‡", f"{result['parameters']['total']:,}")
            st.metric("æ¯å±‚å‚æ•°é‡", f"{result['parameters']['per_layer']:,}")
        
        with col2:
            st.metric("FLOPs/æ—¶é—´æ­¥", result['flops']['flops_readable'])
            seq_length = st.number_input("åºåˆ—é•¿åº¦", min_value=1, value=50, step=1, key="lstm_seqlen")
            total_flops = result['flops']['per_timestep'] * seq_length
            st.metric("æ€»FLOPs", f"{total_flops/1e9:.2f}G" if total_flops > 1e9 else f"{total_flops/1e6:.2f}M")
        
        with col3:
            st.metric("å‚æ•°å†…å­˜", f"{result['memory_mb']['parameters']:.2f} MB")
            direction_text = "åŒå‘" if bidirectional else "å•å‘"
            st.metric("æ–¹å‘", direction_text)
        
        # LSTMç»“æ„è¯´æ˜
        st.markdown("#### ğŸ§  LSTMå†…éƒ¨ç»“æ„")
        
        st.markdown("""
        LSTMæœ‰**4ä¸ªé—¨**ï¼Œæ¯ä¸ªé—¨éƒ½éœ€è¦æƒé‡çŸ©é˜µï¼š
        1. **è¾“å…¥é—¨ (Input Gate)**: å†³å®šæ–°ä¿¡æ¯çš„é‡è¦æ€§
        2. **é—å¿˜é—¨ (Forget Gate)**: å†³å®šä¸¢å¼ƒå“ªäº›ä¿¡æ¯
        3. **ç»†èƒé—¨ (Cell Gate)**: åˆ›å»ºæ–°çš„å€™é€‰å€¼
        4. **è¾“å‡ºé—¨ (Output Gate)**: å†³å®šè¾“å‡ºä»€ä¹ˆ
        """)
        
        # å‚æ•°é‡å…¬å¼
        st.markdown("#### ğŸ“ å‚æ•°é‡è®¡ç®—å…¬å¼")
        
        st.latex(r"Params_{layer1} = 4 \times (input\_size \times hidden\_size + hidden\_size^2)")
        
        if num_layers > 1:
            input_size_other = hidden_size * (2 if bidirectional else 1)
            st.latex(r"Params_{other} = 4 \times (hidden\_size \times num\_directions \times hidden\_size + hidden\_size^2)")
        
        with st.expander("ğŸ“– æŸ¥çœ‹è¯¦ç»†è®¡ç®—"):
            st.code(f"""
ç¬¬ä¸€å±‚å‚æ•°é‡:
    input-to-hidden: 4 Ã— {input_size} Ã— {hidden_size} = {4 * input_size * hidden_size:,}
    hidden-to-hidden: 4 Ã— {hidden_size} Ã— {hidden_size} = {4 * hidden_size * hidden_size:,}
    åç½®: 4 Ã— {hidden_size} Ã— 2 = {4 * hidden_size * 2:,}
    å°è®¡: {result['parameters']['per_layer']:,}

{'å…¶ä»–' + str(num_layers-1) + 'å±‚å‚æ•°é‡:' if num_layers > 1 else ''}
{'    ' + str((num_layers-1) * result['parameters']['per_layer']) + ' (æ¯å±‚ç›¸åŒ)' if num_layers > 1 else ''}

æ€»å‚æ•°é‡: {result['parameters']['total']:,}
            """)
        
        # ä¸GRUå¯¹æ¯”
        if st.checkbox("ä¸GRUå¯¹æ¯”", key="lstm_compare_gru"):
            gru_params = num_layers * 3 * (input_size * hidden_size + hidden_size * hidden_size)
            if bidirectional:
                gru_params *= 2
            
            st.markdown("#### ğŸ†š LSTM vs GRU")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("LSTMå‚æ•°é‡", f"{result['parameters']['total']:,}")
            with col2:
                st.metric("GRUå‚æ•°é‡ (ä¼°ç®—)", f"{gru_params:,}")
            
            st.info("""
            ğŸ’¡ **LSTM vs GRU**
            
            - **LSTM**: 4ä¸ªé—¨ï¼Œæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†å‚æ•°æ›´å¤š
            - **GRU**: 3ä¸ªé—¨ï¼Œå‚æ•°çº¦ä¸ºLSTMçš„75%ï¼Œè®­ç»ƒæ›´å¿«
            - åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šæ€§èƒ½ç›¸è¿‘ï¼ŒGRUæ›´è½»é‡
            """)
    
    elif "Embedding" in layer_type:
        st.markdown("### ğŸ“š Embedding åµŒå…¥å±‚åˆ†æ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            num_embeddings = st.number_input(
                "è¯è¡¨å¤§å° (num_embeddings)",
                min_value=100,
                max_value=1000000,
                value=30000,
                step=1000,
                key="emb_vocab"
            )
        
        with col2:
            embedding_dim = st.number_input(
                "åµŒå…¥ç»´åº¦ (embedding_dim)",
                min_value=16,
                max_value=2048,
                value=512,
                step=64,
                key="emb_dim"
            )
        
        result = analyzer.embedding_analysis(num_embeddings, embedding_dim)
        
        st.markdown("---")
        st.markdown("### ğŸ“Š åˆ†æç»“æœ")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ€»å‚æ•°é‡", f"{result['parameters']['total']:,}")
            readable = f"{result['parameters']['total']/1e6:.2f}M" if result['parameters']['total'] > 1e6 else f"{result['parameters']['total']/1e3:.2f}K"
            st.metric("å¯è¯»æ ¼å¼", readable)
        
        with col2:
            st.metric("å‚æ•°å†…å­˜", f"{result['memory_mb']['parameters']:.2f} MB")
        
        with col3:
            st.metric("FLOPs", result['flops']['flops_readable'])
        
        # å‚æ•°é‡å…¬å¼
        st.markdown("#### ğŸ“ å‚æ•°é‡è®¡ç®—")
        st.latex(r"Params = num\_embeddings \times embedding\_dim")
        st.code(f"""
è®¡ç®—è¿‡ç¨‹:
{num_embeddings:,} Ã— {embedding_dim} = {result['parameters']['total']:,}
        """)
        
        # å¸¸è§è¯è¡¨å¤§å°å‚è€ƒ
        st.markdown("#### ğŸ“š å¸¸è§è¯è¡¨å¤§å°å‚è€ƒ")
        
        vocab_sizes = pd.DataFrame({
            'æ¨¡å‹/åœºæ™¯': ['BERT-base', 'GPT-2', 'T5', 'LLaMA', 'ä¸­æ–‡æ¨¡å‹', 'å¤šè¯­è¨€æ¨¡å‹'],
            'è¯è¡¨å¤§å°': ['30,522', '50,257', '32,128', '32,000', '21,128', '250,000+'],
            'åµŒå…¥ç»´åº¦': [768, 768, 512, 4096, 768, 1024]
        })
        
        st.dataframe(vocab_sizes, use_container_width=True)
        
        # è­¦å‘Š
        if result['parameters']['total'] > 10e6:
            st.warning(f"""
            âš ï¸ **å‚æ•°é‡è­¦å‘Š**
            
            åµŒå…¥å±‚æœ‰ **{result['parameters']['total']/1e6:.1f}M** å‚æ•°ï¼
            
            **ä¼˜åŒ–å»ºè®®**:
            - ä½¿ç”¨å­è¯åˆ†è¯ï¼ˆBPE, WordPieceï¼‰å‡å°è¯è¡¨
            - ä½¿ç”¨å“ˆå¸ŒæŠ€å·§ï¼ˆHash Trickï¼‰
            - æƒé‡å…±äº«ï¼ˆå¦‚è¾“å…¥è¾“å‡ºåµŒå…¥å…±äº«ï¼‰
            - ä½¿ç”¨æ›´å°çš„åµŒå…¥ç»´åº¦
            """)
    
    elif "LayerNorm" in layer_type:
        st.markdown("### ğŸ“ LayerNorm å±‚å½’ä¸€åŒ–åˆ†æ")
        
        st.info("""
        ğŸ’¡ **Transformerä¸­çš„æ ‡å‡†å½’ä¸€åŒ–æ–¹å¼**
        
        LayerNormå¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸BatchNormä¸åŒï¼Œä¸ä¾èµ–batchç»Ÿè®¡ã€‚
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            normalized_shape = st.number_input(
                "å½’ä¸€åŒ–ç»´åº¦ (normalized_shape)",
                min_value=1,
                value=512,
                step=64,
                key="ln_shape"
            )
        
        with col2:
            batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=1, value=32, step=1, key="ln_batch")
            seq_len = st.number_input("åºåˆ—é•¿åº¦", min_value=1, value=128, step=1, key="ln_seq")
        
        input_shape = (batch_size, seq_len, normalized_shape)
        
        result = analyzer.layernorm_analysis(normalized_shape, input_shape)
        
        st.markdown("---")
        st.markdown("### ğŸ“Š åˆ†æç»“æœ")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ€»å‚æ•°é‡", f"{result['parameters']['total']:,}")
        
        with col2:
            st.metric("FLOPs", result['flops']['flops_readable'])
        
        with col3:
            st.metric("å‚æ•°å†…å­˜", f"{result['memory_mb']['parameters']:.4f} MB")
        
        # å‚æ•°è¯´æ˜
        st.markdown("#### å‚æ•°æ„æˆ")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Gamma (ç¼©æ”¾)", result['parameters']['gamma'])
        with col2:
            st.metric("Beta (å¹³ç§»)", result['parameters']['beta'])
        
        # LayerNorm vs BatchNorm
        st.markdown("#### ğŸ†š LayerNorm vs BatchNorm")
        
        comparison = pd.DataFrame({
            'ç‰¹æ€§': ['å½’ä¸€åŒ–ç»´åº¦', 'Batchä¾èµ–', 'é€‚ç”¨åœºæ™¯', 'å‚æ•°é‡', 'è®­ç»ƒ/æ¨ç†å·®å¼‚'],
            'LayerNorm': [
                'ç‰¹å¾ç»´åº¦ (Feature)',
                'å¦',
                'Transformer, RNN',
                f'{normalized_shape * 2}',
                'æ— å·®å¼‚'
            ],
            'BatchNorm': [
                'æ‰¹æ¬¡ç»´åº¦ (Batch)',
                'æ˜¯',
                'CNN',
                f'{normalized_shape * 2}',
                'éœ€è¦running_mean/var'
            ]
        })
        
        st.dataframe(comparison, use_container_width=True)
        
        st.info("""
        ğŸ’¡ **ä¸ºä»€ä¹ˆTransformerç”¨LayerNormï¼Ÿ**
        
        - ä¸ä¾èµ–æ‰¹æ¬¡å¤§å°ï¼Œé€‚åˆå°batchè®­ç»ƒ
        - è®­ç»ƒå’Œæ¨ç†è¡Œä¸ºä¸€è‡´
        - å¯¹åºåˆ—é•¿åº¦å˜åŒ–ä¸æ•æ„Ÿ
        - æ›´é€‚åˆNLPä»»åŠ¡çš„ç‰¹å¾åˆ†å¸ƒ
        """)
    
    # æ·»åŠ å®Œæ•´ç½‘ç»œåˆ†æå…¥å£
    st.markdown("---")
    st.markdown("## ğŸ—ï¸ å®Œæ•´ç½‘ç»œåˆ†æ")
    
    if st.button("åˆ‡æ¢åˆ°å®Œæ•´ç½‘ç»œåˆ†ææ¨¡å¼", use_container_width=True):
        st.session_state.calc_mode = "network"
        st.rerun()
    
    # æ˜¾ç¤ºå®Œæ•´ç½‘ç»œåˆ†æ
    if st.session_state.get('calc_mode') == 'network':
        _full_network_analysis()


def _full_network_analysis():
    """å®Œæ•´ç½‘ç»œåˆ†ææ¨¡å¼"""
    st.markdown("---")
    st.markdown("## ğŸ—ï¸ å®Œæ•´ç½‘ç»œå‚æ•°åˆ†æ")
    
    st.markdown("""
    é€‰æ‹©é¢„å®šä¹‰ç½‘ç»œæˆ–è‡ªå®šä¹‰ç½‘ç»œæ¶æ„ï¼Œç”Ÿæˆè¯¦ç»†çš„å‚æ•°/FLOPsæŠ¥å‘Šã€‚
    """)
    
    # ç½‘ç»œé€‰æ‹©
    network_mode = st.radio(
        "é€‰æ‹©æ¨¡å¼",
        ["é¢„å®šä¹‰ç½‘ç»œ", "è‡ªå®šä¹‰ç½‘ç»œ"],
        horizontal=True,
        key="network_mode"
    )
    
    if network_mode == "é¢„å®šä¹‰ç½‘ç»œ":
        _predefined_network_analysis()
    else:
        _custom_network_analysis()
    
    # è¿”å›å•å±‚åˆ†æ
    if st.button("è¿”å›å•å±‚åˆ†æ", use_container_width=True):
        st.session_state.calc_mode = "single"
        st.rerun()


def _predefined_network_analysis():
    """é¢„å®šä¹‰ç½‘ç»œåˆ†æ"""
    st.markdown("### ğŸ“¦ é¢„å®šä¹‰ç½‘ç»œæ¶æ„")
    
    network_name = st.selectbox(
        "é€‰æ‹©ç½‘ç»œ",
        [
            "ResNet-18 (CNN)",
            "ResNet-50 (CNN)",
            "VGG-16 (CNN)",
            "MobileNetV2 (è½»é‡çº§CNN)",
            "BERT-base (Transformer)",
            "GPT-2 small (Transformer)",
            "ViT-Base (Vision Transformer)"
        ],
        key="predefined_network"
    )
    
    # è¾“å…¥å°ºå¯¸
    col1, col2 = st.columns(2)
    with col1:
        batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=1, value=1, step=1, key="batch_size")
    with col2:
        input_size = st.selectbox("è¾“å…¥å°ºå¯¸", [224, 256, 384, 512], index=0, key="input_size")
    
    # è·å–ç½‘ç»œæ¶æ„
    network_config = _get_network_config(network_name, input_size)
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_params = 0
    total_flops = 0
    total_memory = 0
    
    layers_data = []
    
    for layer_info in network_config:
        total_params += layer_info['params']
        total_flops += layer_info['flops']
        total_memory += layer_info.get('memory', 0)
        layers_data.append(layer_info)
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    st.markdown("---")
    st.markdown("### ğŸ“Š ç½‘ç»œæ€»ä½“ç»Ÿè®¡")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "æ€»å‚æ•°é‡",
            f"{total_params/1e6:.2f}M",
            help="ç½‘ç»œä¸­æ‰€æœ‰å¯å­¦ä¹ å‚æ•°çš„æ€»æ•°"
        )
    
    with col2:
        st.metric(
            "æ€»FLOPs",
            f"{total_flops/1e9:.2f}G",
            help="å•æ¬¡å‰å‘ä¼ æ’­çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°"
        )
    
    with col3:
        st.metric(
            "å‚æ•°å†…å­˜",
            f"{total_params*4/1024/1024:.2f}MB",
            help="å­˜å‚¨æ‰€æœ‰å‚æ•°éœ€è¦çš„å†…å­˜ï¼ˆFP32ï¼‰"
        )
    
    with col4:
        st.metric(
            "æ¿€æ´»å†…å­˜",
            f"{total_memory:.2f}MB",
            help="å‰å‘ä¼ æ’­æ¿€æ´»å€¼å ç”¨çš„å†…å­˜"
        )
    
    # é€å±‚è¯¦ç»†ä¿¡æ¯
    st.markdown("---")
    st.markdown("### ğŸ“‹ é€å±‚è¯¦ç»†åˆ†æ")
    
    # åˆ›å»ºæ•°æ®è¡¨æ ¼
    df = pd.DataFrame(layers_data)
    
    # æ ¼å¼åŒ–æ˜¾ç¤º
    df['params_readable'] = df['params'].apply(lambda x: f"{x/1e6:.2f}M" if x > 1e6 else f"{x/1e3:.2f}K")
    df['flops_readable'] = df['flops'].apply(lambda x: f"{x/1e9:.2f}G" if x > 1e9 else f"{x/1e6:.2f}M")
    df['output_shape_str'] = df['output_shape'].apply(lambda x: f"{x}")
    
    display_df = df[['layer_name', 'layer_type', 'output_shape_str', 'params_readable', 'flops_readable']]
    display_df.columns = ['å±‚åç§°', 'å±‚ç±»å‹', 'è¾“å‡ºå½¢çŠ¶', 'å‚æ•°é‡', 'FLOPs']
    
    st.dataframe(display_df, use_container_width=True, height=400)
    
    # å¯è§†åŒ–
    st.markdown("---")
    st.markdown("### ğŸ“ˆ å¯è§†åŒ–åˆ†æ")
    
    # å‚æ•°é‡åˆ†å¸ƒ
    col1, col2 = st.columns(2)
    
    with col1:
        fig1 = go.Figure(data=[go.Bar(
            x=df['layer_name'],
            y=df['params'],
            marker_color='lightblue'
        )])
        fig1.update_layout(
            title="å„å±‚å‚æ•°é‡åˆ†å¸ƒ",
            xaxis_title="å±‚åç§°",
            yaxis_title="å‚æ•°é‡",
            height=400
        )
        st.plotly_chart(fig1, use_container_width=True)
    
    with col2:
        fig2 = go.Figure(data=[go.Bar(
            x=df['layer_name'],
            y=df['flops'],
            marker_color='lightcoral'
        )])
        fig2.update_layout(
            title="å„å±‚FLOPsåˆ†å¸ƒ",
            xaxis_title="å±‚åç§°",
            yaxis_title="FLOPs",
            height=400
        )
        st.plotly_chart(fig2, use_container_width=True)
    
    # é¥¼å›¾ï¼šå‚æ•°é‡å æ¯”
    st.markdown("#### å‚æ•°é‡å æ¯”")
    
    # æŒ‰å±‚ç±»å‹èšåˆ
    layer_type_params = df.groupby('layer_type')['params'].sum()
    
    fig3 = go.Figure(data=[go.Pie(
        labels=layer_type_params.index,
        values=layer_type_params.values,
        hole=.3
    )])
    fig3.update_layout(title="æŒ‰å±‚ç±»å‹çš„å‚æ•°é‡åˆ†å¸ƒ", height=400)
    st.plotly_chart(fig3, use_container_width=True)
    
    # ç”ŸæˆæŠ¥å‘Š
    st.markdown("---")
    st.markdown("### ğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
    
    if st.button("ç”ŸæˆMarkdownæŠ¥å‘Š", use_container_width=True):
        report = _generate_network_report(network_name, input_size, batch_size, layers_data, total_params, total_flops)
        st.code(report, language="markdown")
        st.download_button(
            "ä¸‹è½½æŠ¥å‘Š",
            report,
            file_name=f"{network_name}_analysis.md",
            mime="text/markdown"
        )


def _custom_network_analysis():
    """è‡ªå®šä¹‰ç½‘ç»œåˆ†æ"""
    st.markdown("### ğŸ› ï¸ è‡ªå®šä¹‰ç½‘ç»œæ¶æ„")
    
    st.markdown("""
    **å¿«é€Ÿæ„å»ºè‡ªå®šä¹‰ç½‘ç»œå¹¶åˆ†æå‚æ•°é‡ã€‚**
    
    åœ¨ä¸‹æ–¹æ·»åŠ å±‚ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨è®¡ç®—å‚æ•°é‡å’ŒFLOPsã€‚
    """)
    
    # åˆå§‹åŒ–session state
    if 'custom_layers' not in st.session_state:
        st.session_state.custom_layers = []
    
    # è¾“å…¥é…ç½®
    col1, col2, col3 = st.columns(3)
    with col1:
        input_channels = st.number_input("è¾“å…¥é€šé“æ•°", min_value=1, value=3, step=1, key="custom_input_c")
    with col2:
        input_height = st.number_input("è¾“å…¥é«˜åº¦", min_value=1, value=224, step=1, key="custom_input_h")
    with col3:
        input_width = st.number_input("è¾“å…¥å®½åº¦", min_value=1, value=224, step=1, key="custom_input_w")
    
    current_shape = (input_channels, input_height, input_width)
    
    # æ·»åŠ å±‚
    st.markdown("---")
    st.markdown("#### æ·»åŠ ç½‘ç»œå±‚")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        layer_to_add = st.selectbox(
            "é€‰æ‹©å±‚ç±»å‹",
            ["Conv2d", "Linear", "MaxPool2d", "BatchNorm2d", "ReLU"],
            key="layer_to_add"
        )
    
    with col2:
        if st.button("æ·»åŠ å±‚", use_container_width=True):
            st.session_state.custom_layers.append({'type': layer_to_add, 'params': {}})
            st.rerun()
    
    # é…ç½®æ¯ä¸€å±‚
    if st.session_state.custom_layers:
        st.markdown("#### é…ç½®ç½‘ç»œå±‚")
        
        analyzer = LayerAnalyzer()
        total_params = 0
        total_flops = 0
        
        for idx, layer in enumerate(st.session_state.custom_layers):
            with st.expander(f"ç¬¬ {idx+1} å±‚: {layer['type']}", expanded=True):
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    if layer['type'] == 'Conv2d':
                        col_a, col_b, col_c, col_d = st.columns(4)
                        with col_a:
                            out_channels = st.number_input("è¾“å‡ºé€šé“", min_value=1, value=64, key=f"conv_out_{idx}")
                        with col_b:
                            kernel = st.number_input("å·ç§¯æ ¸", min_value=1, value=3, key=f"conv_k_{idx}")
                        with col_c:
                            stride = st.number_input("æ­¥é•¿", min_value=1, value=1, key=f"conv_s_{idx}")
                        with col_d:
                            padding = st.number_input("å¡«å……", min_value=0, value=1, key=f"conv_p_{idx}")
                        
                        result = analyzer.conv2d_analysis(
                            current_shape[0], out_channels, kernel, stride, padding, current_shape
                        )
                        current_shape = result['output_shape']
                        total_params += result['parameters']['total']
                        total_flops += result['flops']['total']
                        
                        st.write(f"è¾“å‡ºå½¢çŠ¶: {current_shape}")
                        st.write(f"å‚æ•°é‡: {result['parameters']['total']:,}")
                        st.write(f"FLOPs: {result['flops']['flops_readable']}")
                    
                    elif layer['type'] == 'Linear':
                        out_features = st.number_input("è¾“å‡ºç‰¹å¾æ•°", min_value=1, value=1000, key=f"linear_out_{idx}")
                        
                        # å¦‚æœå‰é¢æ˜¯Convï¼Œéœ€è¦flatten
                        if len(current_shape) == 3:
                            in_features = current_shape[0] * current_shape[1] * current_shape[2]
                            st.info(f"è‡ªåŠ¨å±•å¹³: {current_shape} â†’ {in_features}")
                        else:
                            in_features = current_shape[0]
                        
                        result = analyzer.linear_analysis(in_features, out_features)
                        current_shape = (out_features,)
                        total_params += result['parameters']['total']
                        total_flops += result['flops']['total']
                        
                        st.write(f"è¾“å‡ºå½¢çŠ¶: {current_shape}")
                        st.write(f"å‚æ•°é‡: {result['parameters']['total']:,}")
                        st.write(f"FLOPs: {result['flops']['flops_readable']}")
                    
                    elif layer['type'] == 'MaxPool2d':
                        col_a, col_b = st.columns(2)
                        with col_a:
                            pool_kernel = st.number_input("æ± åŒ–æ ¸", min_value=1, value=2, key=f"pool_k_{idx}")
                        with col_b:
                            pool_stride = st.number_input("æ± åŒ–æ­¥é•¿", min_value=1, value=2, key=f"pool_s_{idx}")
                        
                        if len(current_shape) == 3:
                            new_h = (current_shape[1] - pool_kernel) // pool_stride + 1
                            new_w = (current_shape[2] - pool_kernel) // pool_stride + 1
                            current_shape = (current_shape[0], new_h, new_w)
                        
                        st.write(f"è¾“å‡ºå½¢çŠ¶: {current_shape}")
                        st.write(f"å‚æ•°é‡: 0 (æ— å¯å­¦ä¹ å‚æ•°)")
                    
                    elif layer['type'] == 'BatchNorm2d':
                        if len(current_shape) == 3:
                            result = analyzer.batchnorm2d_analysis(current_shape[0], current_shape)
                            total_params += result['parameters']['total']
                            total_flops += result['flops']['total']
                            
                            st.write(f"è¾“å‡ºå½¢çŠ¶: {current_shape}")
                            st.write(f"å‚æ•°é‡: {result['parameters']['total']:,}")
                    
                    elif layer['type'] == 'ReLU':
                        st.write(f"è¾“å‡ºå½¢çŠ¶: {current_shape}")
                        st.write(f"å‚æ•°é‡: 0 (æ¿€æ´»å‡½æ•°æ— å‚æ•°)")
                
                with col2:
                    if st.button("åˆ é™¤", key=f"del_{idx}", use_container_width=True):
                        st.session_state.custom_layers.pop(idx)
                        st.rerun()
        
        # æ€»ä½“ç»Ÿè®¡
        st.markdown("---")
        st.markdown("### ğŸ“Š ç½‘ç»œæ€»ä½“ç»Ÿè®¡")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ€»å±‚æ•°", len(st.session_state.custom_layers))
        with col2:
            st.metric("æ€»å‚æ•°é‡", f"{total_params/1e6:.2f}M" if total_params > 1e6 else f"{total_params:,}")
        with col3:
            st.metric("æ€»FLOPs", f"{total_flops/1e9:.2f}G" if total_flops > 1e9 else f"{total_flops/1e6:.2f}M")
        
        # æ¸…ç©ºæŒ‰é’®
        if st.button("æ¸…ç©ºæ‰€æœ‰å±‚", use_container_width=True):
            st.session_state.custom_layers = []
            st.rerun()


def _get_network_config(network_name: str, input_size: int) -> List[Dict]:
    """è·å–é¢„å®šä¹‰ç½‘ç»œçš„é…ç½®"""
    
    if "ResNet-18" in network_name:
        return [
            {'layer_name': 'conv1', 'layer_type': 'Conv2d', 'output_shape': (64, input_size//2, input_size//2), 
             'params': 9408, 'flops': 118013952},
            {'layer_name': 'layer1', 'layer_type': 'ResBlock', 'output_shape': (64, input_size//2, input_size//2), 
             'params': 147968, 'flops': 924844032},
            {'layer_name': 'layer2', 'layer_type': 'ResBlock', 'output_shape': (128, input_size//4, input_size//4), 
             'params': 525568, 'flops': 924844032},
            {'layer_name': 'layer3', 'layer_type': 'ResBlock', 'output_shape': (256, input_size//8, input_size//8), 
             'params': 2099712, 'flops': 924844032},
            {'layer_name': 'layer4', 'layer_type': 'ResBlock', 'output_shape': (512, input_size//16, input_size//16), 
             'params': 8394752, 'flops': 924844032},
            {'layer_name': 'fc', 'layer_type': 'Linear', 'output_shape': (1000,), 
             'params': 513000, 'flops': 1024000},
        ]
    
    elif "VGG-16" in network_name:
        return [
            {'layer_name': 'conv1_1', 'layer_type': 'Conv2d', 'output_shape': (64, input_size, input_size), 
             'params': 1792, 'flops': 86704128},
            {'layer_name': 'conv1_2', 'layer_type': 'Conv2d', 'output_shape': (64, input_size, input_size), 
             'params': 36928, 'flops': 1849688064},
            {'layer_name': 'pool1', 'layer_type': 'MaxPool2d', 'output_shape': (64, input_size//2, input_size//2), 
             'params': 0, 'flops': 0},
            {'layer_name': 'conv2_1', 'layer_type': 'Conv2d', 'output_shape': (128, input_size//2, input_size//2), 
             'params': 73856, 'flops': 924844032},
            {'layer_name': 'conv2_2', 'layer_type': 'Conv2d', 'output_shape': (128, input_size//2, input_size//2), 
             'params': 147584, 'flops': 1849688064},
            {'layer_name': 'fc', 'layer_type': 'Linear', 'output_shape': (1000,), 
             'params': 4096000, 'flops': 8192000},
        ]
    
    elif "MobileNetV2" in network_name:
        return [
            {'layer_name': 'conv1', 'layer_type': 'Conv2d', 'output_shape': (32, input_size//2, input_size//2), 
             'params': 864, 'flops': 10838016},
            {'layer_name': 'bottleneck1', 'layer_type': 'InvertedResidual', 'output_shape': (16, input_size//2, input_size//2), 
             'params': 896, 'flops': 11239424},
            {'layer_name': 'bottleneck2', 'layer_type': 'InvertedResidual', 'output_shape': (24, input_size//4, input_size//4), 
             'params': 5136, 'flops': 40140800},
            {'layer_name': 'bottleneck3', 'layer_type': 'InvertedResidual', 'output_shape': (32, input_size//8, input_size//8), 
             'params': 8832, 'flops': 34406400},
            {'layer_name': 'bottleneck4', 'layer_type': 'InvertedResidual', 'output_shape': (64, input_size//16, input_size//16), 
             'params': 25728, 'flops': 50135040},
            {'layer_name': 'bottleneck5', 'layer_type': 'InvertedResidual', 'output_shape': (96, input_size//16, input_size//16), 
             'params': 66624, 'flops': 129957888},
            {'layer_name': 'bottleneck6', 'layer_type': 'InvertedResidual', 'output_shape': (160, input_size//32, input_size//32), 
             'params': 118272, 'flops': 91570176},
            {'layer_name': 'bottleneck7', 'layer_type': 'InvertedResidual', 'output_shape': (320, input_size//32, input_size//32), 
             'params': 155264, 'flops': 120197120},
            {'layer_name': 'conv_last', 'layer_type': 'Conv2d', 'output_shape': (1280, input_size//32, input_size//32), 
             'params': 409600, 'flops': 200704000},
            {'layer_name': 'classifier', 'layer_type': 'Linear', 'output_shape': (1000,), 
             'params': 1281000, 'flops': 2560000},
        ]
    
    elif "BERT-base" in network_name:
        # BERT-base: 12å±‚Transformer, d_model=768, num_heads=12
        d_model = 768
        num_heads = 12
        seq_len = 512
        vocab_size = 30522
        
        layers = []
        
        # Embeddingå±‚
        layers.append({
            'layer_name': 'token_embeddings',
            'layer_type': 'Embedding',
            'output_shape': (seq_len, d_model),
            'params': vocab_size * d_model,  # 23,440,896
            'flops': 0,
            'memory': 0
        })
        
        layers.append({
            'layer_name': 'position_embeddings',
            'layer_type': 'Embedding',
            'output_shape': (seq_len, d_model),
            'params': 512 * d_model,  # 393,216
            'flops': 0,
            'memory': 0
        })
        
        # 12ä¸ªTransformerå±‚
        for i in range(12):
            # Multi-Head Attention
            attn_params = 4 * d_model * d_model + 4 * d_model  # Q,K,V,O + bias
            attn_flops = 6 * seq_len * d_model * d_model + 2 * num_heads * seq_len * seq_len * (d_model // num_heads) * 2
            
            layers.append({
                'layer_name': f'layer{i}_attention',
                'layer_type': 'MultiHeadAttention',
                'output_shape': (seq_len, d_model),
                'params': attn_params,  # 2,362,368
                'flops': attn_flops,
                'memory': 0
            })
            
            # LayerNorm
            layers.append({
                'layer_name': f'layer{i}_ln1',
                'layer_type': 'LayerNorm',
                'output_shape': (seq_len, d_model),
                'params': 2 * d_model,  # 1,536
                'flops': 5 * seq_len * d_model,
                'memory': 0
            })
            
            # Feed Forward (ä¸¤å±‚Linear)
            ffn_hidden = d_model * 4  # 3072
            ffn_params = d_model * ffn_hidden + ffn_hidden + ffn_hidden * d_model + d_model
            ffn_flops = 2 * seq_len * (d_model * ffn_hidden + ffn_hidden * d_model)
            
            layers.append({
                'layer_name': f'layer{i}_ffn',
                'layer_type': 'FeedForward',
                'output_shape': (seq_len, d_model),
                'params': ffn_params,  # 4,722,432
                'flops': ffn_flops,
                'memory': 0
            })
            
            # LayerNorm
            layers.append({
                'layer_name': f'layer{i}_ln2',
                'layer_type': 'LayerNorm',
                'output_shape': (seq_len, d_model),
                'params': 2 * d_model,  # 1,536
                'flops': 5 * seq_len * d_model,
                'memory': 0
            })
        
        # Pooler
        layers.append({
            'layer_name': 'pooler',
            'layer_type': 'Linear',
            'output_shape': (d_model,),
            'params': d_model * d_model + d_model,  # 590,592
            'flops': 2 * d_model * d_model,
            'memory': 0
        })
        
        return layers
    
    elif "GPT-2" in network_name:
        # GPT-2 small: 12å±‚Transformer, d_model=768, num_heads=12
        d_model = 768
        num_heads = 12
        seq_len = 1024
        vocab_size = 50257
        
        layers = []
        
        # Token Embedding
        layers.append({
            'layer_name': 'token_embeddings',
            'layer_type': 'Embedding',
            'output_shape': (seq_len, d_model),
            'params': vocab_size * d_model,  # 38,597,376
            'flops': 0,
            'memory': 0
        })
        
        # Position Embedding
        layers.append({
            'layer_name': 'position_embeddings',
            'layer_type': 'Embedding',
            'output_shape': (seq_len, d_model),
            'params': seq_len * d_model,  # 786,432
            'flops': 0,
            'memory': 0
        })
        
        # 12ä¸ªTransformerå—
        for i in range(12):
            # LayerNorm 1
            layers.append({
                'layer_name': f'layer{i}_ln1',
                'layer_type': 'LayerNorm',
                'output_shape': (seq_len, d_model),
                'params': 2 * d_model,
                'flops': 5 * seq_len * d_model,
                'memory': 0
            })
            
            # Causal Self-Attention
            attn_params = 4 * d_model * d_model + 4 * d_model
            attn_flops = 6 * seq_len * d_model * d_model + 2 * num_heads * seq_len * seq_len * (d_model // num_heads) * 2
            
            layers.append({
                'layer_name': f'layer{i}_attn',
                'layer_type': 'CausalAttention',
                'output_shape': (seq_len, d_model),
                'params': attn_params,
                'flops': attn_flops,
                'memory': 0
            })
            
            # LayerNorm 2
            layers.append({
                'layer_name': f'layer{i}_ln2',
                'layer_type': 'LayerNorm',
                'output_shape': (seq_len, d_model),
                'params': 2 * d_model,
                'flops': 5 * seq_len * d_model,
                'memory': 0
            })
            
            # MLP
            ffn_hidden = d_model * 4
            ffn_params = d_model * ffn_hidden + ffn_hidden + ffn_hidden * d_model + d_model
            ffn_flops = 2 * seq_len * (d_model * ffn_hidden + ffn_hidden * d_model)
            
            layers.append({
                'layer_name': f'layer{i}_mlp',
                'layer_type': 'MLP',
                'output_shape': (seq_len, d_model),
                'params': ffn_params,
                'flops': ffn_flops,
                'memory': 0
            })
        
        # Final LayerNorm
        layers.append({
            'layer_name': 'ln_f',
            'layer_type': 'LayerNorm',
            'output_shape': (seq_len, d_model),
            'params': 2 * d_model,
            'flops': 5 * seq_len * d_model,
            'memory': 0
        })
        
        # Language Model Head (å…±äº«embeddingæƒé‡ï¼Œæ‰€ä»¥å‚æ•°ä¸º0)
        layers.append({
            'layer_name': 'lm_head',
            'layer_type': 'Linear',
            'output_shape': (vocab_size,),
            'params': 0,  # æƒé‡å…±äº«
            'flops': 2 * seq_len * vocab_size * d_model,
            'memory': 0
        })
        
        return layers
    
    elif "ViT-Base" in network_name:
        # Vision Transformer Base: patch_size=16, d_model=768, num_heads=12, 12å±‚
        patch_size = 16
        d_model = 768
        num_heads = 12
        num_patches = (input_size // patch_size) ** 2  # 196 for 224x224
        seq_len = num_patches + 1  # +1 for class token
        
        layers = []
        
        # Patch Embedding
        layers.append({
            'layer_name': 'patch_embed',
            'layer_type': 'Conv2d',
            'output_shape': (d_model, input_size//patch_size, input_size//patch_size),
            'params': 3 * patch_size * patch_size * d_model + d_model,  # 590,592
            'flops': 3 * patch_size * patch_size * d_model * num_patches * 2,
            'memory': 0
        })
        
        # Position Embedding
        layers.append({
            'layer_name': 'pos_embed',
            'layer_type': 'Embedding',
            'output_shape': (seq_len, d_model),
            'params': seq_len * d_model,  # 151,296 for 224x224
            'flops': 0,
            'memory': 0
        })
        
        # 12ä¸ªTransformerç¼–ç å™¨å±‚
        for i in range(12):
            # LayerNorm + Attention
            layers.append({
                'layer_name': f'block{i}_ln1',
                'layer_type': 'LayerNorm',
                'output_shape': (seq_len, d_model),
                'params': 2 * d_model,
                'flops': 5 * seq_len * d_model,
                'memory': 0
            })
            
            attn_params = 4 * d_model * d_model + 4 * d_model
            attn_flops = 6 * seq_len * d_model * d_model + 2 * num_heads * seq_len * seq_len * (d_model // num_heads) * 2
            
            layers.append({
                'layer_name': f'block{i}_attn',
                'layer_type': 'MultiHeadAttention',
                'output_shape': (seq_len, d_model),
                'params': attn_params,
                'flops': attn_flops,
                'memory': 0
            })
            
            # LayerNorm + MLP
            layers.append({
                'layer_name': f'block{i}_ln2',
                'layer_type': 'LayerNorm',
                'output_shape': (seq_len, d_model),
                'params': 2 * d_model,
                'flops': 5 * seq_len * d_model,
                'memory': 0
            })
            
            ffn_hidden = d_model * 4
            ffn_params = d_model * ffn_hidden + ffn_hidden + ffn_hidden * d_model + d_model
            ffn_flops = 2 * seq_len * (d_model * ffn_hidden + ffn_hidden * d_model)
            
            layers.append({
                'layer_name': f'block{i}_mlp',
                'layer_type': 'MLP',
                'output_shape': (seq_len, d_model),
                'params': ffn_params,
                'flops': ffn_flops,
                'memory': 0
            })
        
        # Classification Head
        layers.append({
            'layer_name': 'head',
            'layer_type': 'Linear',
            'output_shape': (1000,),
            'params': d_model * 1000 + 1000,  # 769,000
            'flops': 2 * d_model * 1000,
            'memory': 0
        })
        
        return layers
    
    # é»˜è®¤è¿”å›ç©º
    return []


def _generate_network_report(network_name: str, input_size: int, batch_size: int, 
                             layers_data: List[Dict], total_params: int, total_flops: int) -> str:
    """ç”Ÿæˆç½‘ç»œåˆ†ææŠ¥å‘Š"""
    
    report = f"""# {network_name} ç½‘ç»œåˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **ç½‘ç»œåç§°**: {network_name}
- **è¾“å…¥å°ºå¯¸**: [{batch_size}, 3, {input_size}, {input_size}]
- **æ€»å‚æ•°é‡**: {total_params:,} ({total_params/1e6:.2f}M)
- **æ€»FLOPs**: {total_flops:,} ({total_flops/1e9:.2f}G)
- **å‚æ•°å†…å­˜**: {total_params*4/1024/1024:.2f} MB (FP32)

## é€å±‚è¯¦ç»†ä¿¡æ¯

| å±‚åç§° | å±‚ç±»å‹ | è¾“å‡ºå½¢çŠ¶ | å‚æ•°é‡ | FLOPs |
|--------|--------|----------|--------|-------|
"""
    
    for layer in layers_data:
        params_str = f"{layer['params']/1e6:.2f}M" if layer['params'] > 1e6 else f"{layer['params']:,}"
        flops_str = f"{layer['flops']/1e9:.2f}G" if layer['flops'] > 1e9 else f"{layer['flops']/1e6:.2f}M"
        report += f"| {layer['layer_name']} | {layer['layer_type']} | {layer['output_shape']} | {params_str} | {flops_str} |\n"
    
    report += f"""
## æ€§èƒ½è¯„ä¼°

### å‚æ•°é‡åˆ†æ
- æ€»å‚æ•°é‡è¾ƒ{'å¤§' if total_params > 50e6 else 'å°'}ï¼Œ{'å¯èƒ½' if total_params > 50e6 else 'ä¸'}éœ€è¦æ¨¡å‹å‹ç¼©
- å¹³å‡æ¯å±‚å‚æ•°é‡: {total_params/len(layers_data):,.0f}

### è®¡ç®—å¤æ‚åº¦
- FLOPs: {total_flops/1e9:.2f}G
- ä¼°è®¡æ¨ç†æ—¶é—´ (1080Ti): ~{total_flops/1e12*10:.2f}ms

### å†…å­˜å ç”¨
- æ¨¡å‹å‚æ•°: {total_params*4/1024/1024:.2f} MB
- ä¼°è®¡å³°å€¼å†…å­˜: ~{total_params*4/1024/1024*3:.2f} MB (åŒ…å«æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€)

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}*
"""
    
    return report
