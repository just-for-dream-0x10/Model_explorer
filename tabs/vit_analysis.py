"""
Vision Transformer (ViT) åˆ†æ
Vision Transformer Analysis

å±•ç¤ºViTçš„æ ¸å¿ƒæœºåˆ¶:Patch Embedding|Self-Attention|Position Encoding
æ ¸å¿ƒç†å¿µ:ç”¨å¯è§†åŒ–å±•ç¤º"å¦‚ä½•æŠŠå›¾åƒå˜æˆåºåˆ—"
"""

import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
from PIL import Image

from utils.vit_models import (
    PatchEmbedding,
    MultiHeadSelfAttention,
    VisionTransformer,
    create_vit_tiny,
    create_vit_small,
    create_vit_base,
    get_vit_info,
)
from utils.input_config import get_preset_shape
from utils.example_generator import get_dynamic_example


def visualize_patch_embedding(img_size=224, patch_size=16):
    """
    å¯è§†åŒ–Patch Embeddingè¿‡ç¨‹

    Args:
        img_size: å›¾åƒå°ºå¯¸
        patch_size: Patchå¤§å°

    Returns:
        fig: Plotlyå›¾è¡¨
    """
    num_patches_per_side = img_size // patch_size
    num_patches = num_patches_per_side**2

    # åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒ(ç½‘æ ¼)
    img = np.zeros((img_size, img_size, 3))

    # ç»˜åˆ¶ç½‘æ ¼çº¿
    for i in range(0, img_size, patch_size):
        img[i : i + 2, :] = [1, 0, 0]  # çº¢è‰²æ°´å¹³çº¿
        img[:, i : i + 2] = [0, 0, 1]  # è“è‰²å‚ç›´çº¿

    # ç»™æ¯ä¸ªpatchæ ‡å·
    patch_labels = []
    for i in range(num_patches_per_side):
        for j in range(num_patches_per_side):
            patch_idx = i * num_patches_per_side + j
            patch_labels.append(
                {
                    "x": j * patch_size + patch_size // 2,
                    "y": i * patch_size + patch_size // 2,
                    "text": str(patch_idx),
                }
            )

    # åˆ›å»ºå›¾è¡¨
    fig = go.Figure()

    # æ˜¾ç¤ºå›¾åƒ
    fig.add_trace(go.Image(z=img))

    # æ·»åŠ patchç¼–å·
    for label in patch_labels:
        fig.add_annotation(
            x=label["x"],
            y=label["y"],
            text=label["text"],
            showarrow=False,
            font=dict(size=10, color="white"),
            bgcolor="rgba(0,0,0,0.5)",
            borderpad=2,
        )

    fig.update_layout(
        title=f"Patch Embedding: {img_size}x{img_size} å›¾åƒåˆ‡åˆ†æˆ {num_patches} ä¸ª {patch_size}x{patch_size} patches",
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        height=500,
        width=500,
    )

    return fig


def visualize_attention_weights(attn_weights, img_size=224, patch_size=16):
    """
    å¯è§†åŒ–Attentionæƒé‡

    Args:
        attn_weights: æ³¨æ„åŠ›æƒé‡ [num_heads, num_patches+1, num_patches+1]
        img_size: å›¾åƒå°ºå¯¸
        patch_size: Patchå¤§å°

    Returns:
        fig: Plotlyå›¾è¡¨
    """
    num_heads = attn_weights.shape[0]

    # åªæ˜¾ç¤ºå‰4ä¸ªå¤´
    heads_to_show = min(4, num_heads)

    fig = make_subplots(
        rows=2, cols=2, subplot_titles=[f"Head {i}" for i in range(heads_to_show)]
    )

    for idx in range(heads_to_show):
        row = idx // 2 + 1
        col = idx % 2 + 1

        # å–ç¬¬ä¸€ä¸ªtoken(CLS)å¯¹æ‰€æœ‰tokençš„æ³¨æ„åŠ›
        attn = attn_weights[idx, 0, 1:].detach().cpu().numpy()  # å»æ‰CLS token

        # reshapeæˆ2D
        num_patches_per_side = img_size // patch_size
        attn_2d = attn.reshape(num_patches_per_side, num_patches_per_side)

        # æ·»åŠ çƒ­åŠ›å›¾
        fig.add_trace(
            go.Heatmap(z=attn_2d, colorscale="Viridis", showscale=(idx == 0)),
            row=row,
            col=col,
        )

    fig.update_layout(
        title="Self-Attentionæƒé‡å¯è§†åŒ–(CLS Tokenå¯¹å„Patchçš„æ³¨æ„åŠ›)", height=600
    )

    return fig


def compare_vit_cnn_params():
    """
    å¯¹æ¯”ViTå’ŒCNNçš„å‚æ•°é‡

    Returns:
        fig: Plotlyå›¾è¡¨
    """
    models_info = {
        "ResNet-50": 25.6,
        "ResNet-101": 44.5,
        "ViT-Tiny": 5.7,
        "ViT-Small": 22.0,
        "ViT-Base": 86.0,
        "ViT-Large": 307.0,
    }

    fig = go.Figure()

    colors = ["red" if "ResNet" in name else "blue" for name in models_info.keys()]

    fig.add_trace(
        go.Bar(
            x=list(models_info.keys()),
            y=list(models_info.values()),
            text=[f"{v:.1f}M" for v in models_info.values()],
            textposition="auto",
            marker_color=colors,
        )
    )

    fig.update_layout(
        title="ViT vs CNN å‚æ•°é‡å¯¹æ¯”",
        xaxis_title="æ¨¡å‹",
        yaxis_title="å‚æ•°é‡ (Million)",
        height=400,
    )

    return fig


def explain_vit_architecture():
    """å±•ç¤ºViTçš„æ¶æ„åŸç†"""
    st.markdown(
        """
    ### ğŸ—ï¸ Vision Transformer (ViT) æ¶æ„åŸç†
    
    #### æ ¸å¿ƒæ€æƒ³:æŠŠå›¾åƒå½“ä½œåºåˆ—å¤„ç†
    """
    )

    st.markdown(
        """
**ä¼ ç»ŸCNN**:
```
å›¾åƒ -> å·ç§¯å±‚ -> æ± åŒ–å±‚ -> ... -> å…¨è¿æ¥å±‚ -> åˆ†ç±»
```

**ViT**:
```
å›¾åƒ -> Patch Embedding -> Transformer Encoder -> åˆ†ç±»
```

#### ViTçš„å››å¤§æ ¸å¿ƒç»„ä»¶

**1. Patch Embedding(å›¾åƒåˆ‡ç‰‡)**
    """
    )

    try:
        example = get_dynamic_example("vit")
        calc = example["calculation"]

        st.markdown(
            f"""
        è¾“å…¥å›¾åƒ: [B, 3, {example['img_size']}, {example['img_size']}]  
        {calc['patches']}  
        {calc['embedding']}  
        è¾“å‡º: [B, {example['num_patches']}, {example['d_model']}]
        """
        )

        st.markdown(
            f"""
        **å®ç°æ–¹å¼**: ä½¿ç”¨Conv2d(3, {example['d_model']}, kernel_size={example['patch_size']}, stride={example['patch_size']})
        - ç­‰ä»·äºå°†æ¯ä¸ª{example['patch_size']}x{example['patch_size']}çš„patchçº¿æ€§æŠ•å½±åˆ°{example['d_model']}ç»´
        """
        )
    except Exception as e:
        # å¦‚æœåŠ¨æ€ç”Ÿæˆå¤±è´¥,ä½¿ç”¨é»˜è®¤ç¤ºä¾‹
        st.markdown(
            """
        è¾“å…¥å›¾åƒ: [B, 3, 224, 224]
        åˆ‡åˆ†patches: 224/16 = 14, å…±14x14=196ä¸ªpatches
        æ¯ä¸ªpatch: [3, 16, 16] = 768ç»´å‘é‡
        è¾“å‡º: [B, 196, 768]
        """
        )

        st.markdown(
            """
        **å®ç°æ–¹å¼**: ä½¿ç”¨Conv2d(3, 768, kernel_size=16, stride=16)
        - ç­‰ä»·äºå°†æ¯ä¸ª16x16çš„patchçº¿æ€§æŠ•å½±åˆ°768ç»´
        """
        )

    st.markdown(
        """
    **2. Position Embedding(ä½ç½®ç¼–ç )**
    ```python
    ä¸ºä»€ä¹ˆéœ€è¦?Transformeræ²¡æœ‰ä½ç½®ä¿¡æ¯!
    
    å¯å­¦ä¹ ä½ç½®ç¼–ç : [1, 197, 768]  # 196ä¸ªpatches + 1ä¸ªCLS token
    æ·»åŠ æ–¹å¼: x = x + pos_embed
    ```
    
    **3. [CLS] Token(åˆ†ç±»æ ‡è®°)**
    ```python
    ä½œç”¨: ç”¨äºåˆ†ç±»çš„ç‰¹æ®Štoken
    åˆå§‹åŒ–: [1, 1, 768] å¯å­¦ä¹ å‚æ•°
    ä½ç½®: æ’å…¥åˆ°åºåˆ—å¼€å¤´
    
    [CLS, patch_1, patch_2, ..., patch_196]
    ```
    
    **4. Self-Attention(è‡ªæ³¨æ„åŠ›)**
    ```python
    Q = X @ W_q
    K = X @ W_k  
    V = X @ W_v
    Attention = softmax(Q @ K^T / sqrt(d_k)) @ V
    
    å¤æ‚åº¦: O(N^2),N=196(patchesæ•°é‡)
    ```
    
    #### ViT vs CNN çš„å…³é”®å·®å¼‚
    
    | ç‰¹æ€§ | CNN | ViT |
    |------|-----|-----|
    | **å½’çº³åç½®** | å¼º(å¹³ç§»ä¸å˜æ€§|å±€éƒ¨æ€§) | å¼±(éœ€è¦ä»æ•°æ®å­¦ä¹ ) |
    | **æ„Ÿå—é‡** | é€å±‚å¢é•¿ | å…¨å±€(ç¬¬ä¸€å±‚å°±èƒ½çœ‹åˆ°æ•´ä¸ªå›¾åƒ) |
    | **æ•°æ®éœ€æ±‚** | å°æ•°æ®é›†ä¹Ÿèƒ½work | éœ€è¦å¤§æ•°æ®é›†é¢„è®­ç»ƒ |
    | **è®¡ç®—å¤æ‚åº¦** | O(N) | O(N^2) |
    | **å‚æ•°é‡** | ç›¸å¯¹è¾ƒå°‘ | ç›¸å¯¹è¾ƒå¤š |
    
    #### ä¸ºä»€ä¹ˆViTéœ€è¦æ›´å¤šæ•°æ®?
    
    **CNNçš„ä¼˜åŠ¿**:
    - å·ç§¯æ“ä½œå†…ç½®äº†å¹³ç§»ä¸å˜æ€§(translation invariance)
    - å±€éƒ¨è¿æ¥å¤©ç„¶é€‚åˆå›¾åƒçš„ç©ºé—´ç»“æ„
    - å¯ä»¥ç”¨å°‘é‡æ•°æ®å­¦åˆ°å¥½çš„ç‰¹å¾
    
    **ViTçš„åŠ£åŠ¿**:
    - æ²¡æœ‰å†…ç½®å½’çº³åç½®,éœ€è¦ä»æ•°æ®ä¸­å­¦ä¹ 
    - åœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆ
    - éœ€è¦å¤§è§„æ¨¡é¢„è®­ç»ƒ (ImageNet 21K, JFT 300M)
    
    **å®éªŒæ•°æ®**:
    - å°æ•°æ®é›†(ImageNet 1K): CNN > ViT
    - å¤§æ•°æ®é›†(ImageNet 21K): ViT â‰ˆ CNN
    - è¶…å¤§æ•°æ®é›†(JFT 300M): ViT > CNN
    """
    )


def vit_analysis_tab(chinese_supported=True):
    """Vision Transformeråˆ†æä¸»å‡½æ•°"""

    st.header("ğŸ” Vision Transformer (ViT) åˆ†æ")
    st.markdown(
        """
    > **æ ¸å¿ƒé—®é¢˜**:Transformerå¦‚ä½•åº”ç”¨åˆ°å›¾åƒé¢†åŸŸ?ViTå’ŒCNNæœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«?
    
    **éªŒè¯æ–¹æ³•**:å¯è§†åŒ–Patch Embedding|Self-Attention,å¯¹æ¯”ViTå’ŒCNN
    """
    )

    st.markdown("---")

    # æ¶æ„åŸç†
    with st.expander("ğŸ—ï¸ ViTæ¶æ„åŸç†(ç‚¹å‡»å±•å¼€)", expanded=False):
        explain_vit_architecture()

    st.markdown("---")

    # Patch Embeddingå¯è§†åŒ–
    st.subheader("ğŸ“ 1. Patch Embeddingå¯è§†åŒ–")

    col1, col2 = st.columns(2)

    with col1:
        img_size = st.selectbox("å›¾åƒå°ºå¯¸", [224, 384], index=0)

    with col2:
        patch_size = st.selectbox("Patchå¤§å°", [16, 32], index=0)

    num_patches = (img_size // patch_size) ** 2

    st.info(
        f"""
    **è®¡ç®—è¿‡ç¨‹**:
    - å›¾åƒå°ºå¯¸: {img_size}x{img_size}
    - Patchå¤§å°: {patch_size}x{patch_size}
    - Patchesæ•°é‡: ({img_size}/{patch_size})^2 = **{num_patches}ä¸ª**
    - æ¯ä¸ªpatchç»´åº¦: {patch_size}x{patch_size}x3 = {patch_size*patch_size*3}
    """
    )

    # æ˜¾ç¤ºåˆ‡åˆ†å¯è§†åŒ–
    fig1 = visualize_patch_embedding(img_size, patch_size)
    st.plotly_chart(fig1, use_container_width=True)

    st.markdown(
        """
    **å…³é”®ç†è§£**:
    - å›¾åƒè¢«åˆ‡åˆ†æˆä¸é‡å çš„patches
    - æ¯ä¸ªpatché€šè¿‡çº¿æ€§æŠ•å½±å˜æˆembedding
    - å®ç°æ–¹å¼:Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)
    """
    )

    # ViTæ¨¡å‹å¯¹æ¯”
    st.markdown("---")
    st.subheader("âš–ï¸ 2. ViTæ¨¡å‹è§„æ¨¡å¯¹æ¯”")

    model_choice = st.selectbox(
        "é€‰æ‹©ViTæ¨¡å‹",
        ["vit_tiny", "vit_small", "vit_base"],
        format_func=lambda x: {
            "vit_tiny": "ViT-Tiny (5.7Må‚æ•°)",
            "vit_small": "ViT-Small (22Må‚æ•°)",
            "vit_base": "ViT-Base (86Må‚æ•°)",
        }[x],
    )

    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    vit_info = get_vit_info(model_choice)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**æ¨¡å‹é…ç½®**")
        config = vit_info["config"]
        st.code(
            f"""
Embeddingç»´åº¦: {config['embed_dim']}
Transformerå±‚æ•°: {config['depth']}
æ³¨æ„åŠ›å¤´æ•°: {config['num_heads']}
Patchå¤§å°: {config['patch_size']}x{config['patch_size']}
é¢„ä¼°å‚æ•°é‡: {config['params_estimate']}
        """
        )

    with col2:
        st.markdown("**æ¶æ„ç»†èŠ‚**")
        arch = vit_info["architecture"]
        st.code(
            f"""
Patch Embedding:
  {arch['patch_embedding']}

Position Embedding:
  {arch['position_embedding']}

Transformer Blocks:
  {arch['transformer_blocks']}
  {arch['attention']}
        """
        )

    # å‚æ•°é‡å¯¹æ¯”å›¾
    st.markdown("#### ğŸ“Š ViT vs CNN å‚æ•°é‡å¯¹æ¯”")
    fig2 = compare_vit_cnn_params()
    st.plotly_chart(fig2, use_container_width=True)

    st.info(
        """
    **è§‚å¯Ÿ**:
    - ViT-Base (86M) å‚æ•°é‡çº¦ä¸º ResNet-50 (25M) çš„3.4å€
    - ViT-Large (307M) å‚æ•°é‡éå¸¸å¤§,éœ€è¦å¤§è§„æ¨¡æ•°æ®é¢„è®­ç»ƒ
    - ViT-Tiny (5.7M) é€‚åˆèµ„æºå—é™åœºæ™¯
    """
    )

    # Self-Attentionå¯è§†åŒ–
    st.markdown("---")
    st.subheader("ğŸ‘ï¸ 3. Self-Attentionå¯è§†åŒ–")

    st.markdown(
        """
    **Self-Attentionçš„ä½œç”¨**:
    - æ¯ä¸ªpatchå¯ä»¥"çœ‹åˆ°"æ‰€æœ‰å…¶ä»–patches
    - ç¬¬ä¸€å±‚å°±æœ‰å…¨å±€æ„Ÿå—é‡(ä¸CNNä¸åŒ)
    - æ³¨æ„åŠ›æƒé‡åæ˜ äº†patchesä¹‹é—´çš„å…³ç³»
    """
    )

    if st.button("ğŸš€ ç”Ÿæˆéšæœºæ•°æ®å¹¶å¯è§†åŒ–Attention", type="primary"):
        with st.spinner("è®¡ç®—ä¸­..."):
            # åˆ›å»ºæ¨¡å‹
            if model_choice == "vit_tiny":
                model = create_vit_tiny(img_size=224, num_classes=10)
            elif model_choice == "vit_small":
                model = create_vit_small(img_size=224, num_classes=10)
            else:
                model = create_vit_base(img_size=224, num_classes=10)

            model.eval()

            # ç”Ÿæˆéšæœºè¾“å…¥
            x = torch.randn(1, 3, 224, 224)

            # å‰å‘ä¼ æ’­å¹¶è·å–attention weights
            with torch.no_grad():
                _, attn_weights_list = model(x, return_attention=True)

            # æ˜¾ç¤ºç¬¬ä¸€å±‚çš„attention
            first_layer_attn = attn_weights_list[0][0]  # [num_heads, 197, 197]

            st.success(f"âœ… æˆåŠŸè·å–Attentionæƒé‡,å½¢çŠ¶: {first_layer_attn.shape}")

            # å¯è§†åŒ–
            fig3 = visualize_attention_weights(
                first_layer_attn, img_size=224, patch_size=16
            )
            st.plotly_chart(fig3, use_container_width=True)

            st.markdown(
                """
            **è§£è¯»**:
            - çƒ­åŠ›å›¾æ˜¾ç¤ºäº†[CLS] tokenå¯¹å„ä¸ªpatchçš„æ³¨æ„åŠ›åˆ†å¸ƒ
            - ä¸åŒçš„attention headå…³æ³¨ä¸åŒçš„åŒºåŸŸ
            - äº®è‰²åŒºåŸŸè¡¨ç¤ºé«˜æ³¨æ„åŠ›,æš—è‰²åŒºåŸŸè¡¨ç¤ºä½æ³¨æ„åŠ›
            """
            )

    # è®¡ç®—å¤æ‚åº¦åˆ†æ
    st.markdown("---")
    st.subheader("âš¡ 4. è®¡ç®—å¤æ‚åº¦åˆ†æ")

    st.markdown(
        """
    ### Self-Attentionçš„è®¡ç®—å¤æ‚åº¦
    
    å¯¹äºè¾“å…¥åºåˆ—é•¿åº¦N(patchesæ•°é‡):
    
    **æ—¶é—´å¤æ‚åº¦**:
    ```
    Q @ K^T: O(N^2 Â· d)    # Nxd çŸ©é˜µä¹˜ä»¥ dxN çŸ©é˜µ
    Softmax: O(N^2)        # å¯¹NxNçŸ©é˜µåšsoftmax
    Attn @ V: O(N^2 Â· d)   # NxN çŸ©é˜µä¹˜ä»¥ Nxd çŸ©é˜µ
    
    æ€»å¤æ‚åº¦: O(N^2 Â· d)
    ```
    
    **ç©ºé—´å¤æ‚åº¦**:
    ```
    å­˜å‚¨attentionçŸ©é˜µ: O(N^2)
    ```
    
    ### ä¸CNNå¯¹æ¯”
    
    | æ“ä½œ | å¤æ‚åº¦ | è¯´æ˜ |
    |------|--------|------|
    | **Self-Attention** | O(N^2Â·d) | N=patchesæ•°é‡,éšå›¾åƒå°ºå¯¸å¹³æ–¹å¢é•¿ |
    | **å·ç§¯** | O(k^2Â·d^2Â·N) | k=kernelå¤§å°,d=é€šé“æ•°,N=ç‰¹å¾å›¾å¤§å° |
    
    ### å®é™…æ•°å€¼
    
    å‡è®¾224x224å›¾åƒ,patch_size=16:
    - N = (224/16)^2 = 196ä¸ªpatches
    - AttentionçŸ©é˜µ: 196x196 = 38,416ä¸ªå…ƒç´ 
    - 12ä¸ªå¤´: 12x38,416 = 460,992ä¸ªå…ƒç´ 
    
    **ç»“è®º**:
    - ViTçš„è®¡ç®—é‡éšå›¾åƒå°ºå¯¸å¹³æ–¹å¢é•¿
    - é«˜åˆ†è¾¨ç‡å›¾åƒ(å¦‚1024x1024)è®¡ç®—é‡å·¨å¤§
    - éœ€è¦ç”¨åˆ°å„ç§ä¼˜åŒ–æŠ€å·§(å¦‚Linformer|Performerç­‰)
    """
    )

    # é€‚ç”¨åœºæ™¯
    st.markdown("---")
    st.subheader("ğŸ¯ 5. ViT vs CNN:ä½•æ—¶ä½¿ç”¨?")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### âœ… ä½¿ç”¨ViTçš„åœºæ™¯")
        st.markdown(
            """
        1. **æœ‰å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹**
           - ä½¿ç”¨ImageNet-21ké¢„è®­ç»ƒæƒé‡
           - æˆ–è€…JFT-300Mç­‰è¶…å¤§æ•°æ®é›†
        
        2. **ç›®æ ‡ä»»åŠ¡æ•°æ®é‡å……è¶³**
           - è‡³å°‘æœ‰å‡ ä¸‡å¼ æ ‡æ³¨å›¾åƒ
           - æˆ–è€…å¯ä»¥åšæ•°æ®å¢å¼º
        
        3. **éœ€è¦å…¨å±€å»ºæ¨¡èƒ½åŠ›**
           - ç›®æ ‡æ£€æµ‹|å®ä¾‹åˆ†å‰²
           - éœ€è¦é•¿è·ç¦»ä¾èµ–å…³ç³»
        
        4. **è®¡ç®—èµ„æºå……è¶³**
           - æœ‰GPU/TPUæ”¯æŒ
           - å¯ä»¥æ¥å—è¾ƒé•¿çš„è®­ç»ƒæ—¶é—´
        """
        )

    with col2:
        st.markdown("### âœ… ä½¿ç”¨CNNçš„åœºæ™¯")
        st.markdown(
            """
        1. **æ•°æ®é‡è¾ƒå°**
           - åªæœ‰å‡ åƒå¼ å›¾åƒ
           - éš¾ä»¥è·å–å¤§è§„æ¨¡æ•°æ®
        
        2. **éœ€è¦å¿«é€Ÿè®­ç»ƒ**
           - èµ„æºå—é™
           - éœ€è¦è¾¹ç¼˜éƒ¨ç½²
        
        3. **ä»»åŠ¡ä¾èµ–å±€éƒ¨ç‰¹å¾**
           - çº¹ç†åˆ†ç±»
           - è¾¹ç¼˜æ£€æµ‹
        
        4. **éœ€è¦å¹³ç§»ä¸å˜æ€§**
           - ç›®æ ‡ä½ç½®ä¸å›ºå®š
           - éœ€è¦æ³›åŒ–åˆ°ä¸åŒä½ç½®
        """
        )

    # æ€»ç»“
    st.markdown("---")
    st.subheader("ğŸ’¡ æ ¸å¿ƒè¦ç‚¹")

    st.markdown(
        """
    ### ViTçš„é©å‘½æ€§è´¡çŒ®
    
    1. **è¯æ˜äº†Transformerå¯ä»¥åº”ç”¨åˆ°è§†è§‰é¢†åŸŸ**
       - æ‰“ç ´äº†CNNåœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„å„æ–­
       - å¼€å¯äº†è§†è§‰Transformerçš„ç ”ç©¶çƒ­æ½®
    
    2. **å±•ç¤ºäº†scaling lawçš„å¨åŠ›**
       - æ¨¡å‹è¶Šå¤§+æ•°æ®è¶Šå¤š = æ€§èƒ½è¶Šå¥½
       - ViT-Hugeåœ¨JFT-300Mä¸Šè¾¾åˆ°äº†SOTA
    
    3. **ç®€åŒ–äº†æ¨¡å‹è®¾è®¡**
       - ä¸éœ€è¦ç²¾å¿ƒè®¾è®¡çš„å·ç§¯ç»“æ„
       - ç»Ÿä¸€çš„Transformeræ¶æ„
    
    ### å®é™…å·¥ç¨‹å»ºè®®
    
    **å¦‚æœä½ æ˜¯...**
    
    - **å­¦ç”Ÿ/ç ”ç©¶è€…**: ä½¿ç”¨é¢„è®­ç»ƒçš„ViT(timmåº“),åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒ
    - **å·¥ä¸šç•Œ**: å°æ•°æ®é›†ç”¨CNN,å¤§æ•°æ®é›†ç”¨ViT
    - **è¾¹ç¼˜è®¾å¤‡**: ä¼˜å…ˆè€ƒè™‘MobileNet|EfficientNetç­‰è½»é‡CNN
    - **äº‘ç«¯éƒ¨ç½²**: å¯ä»¥ä½¿ç”¨ViT-Baseæˆ–ViT-Large
    
    ### è®°ä½ä¸‰ä¸ªå…³é”®æ•°å­—
    
    - **196**: 224x224å›¾åƒä½¿ç”¨16x16 patchå¾—åˆ°çš„åºåˆ—é•¿åº¦
    - **768**: ViT-Baseçš„embeddingç»´åº¦
    - **12**: ViT-Baseçš„Transformerå±‚æ•°å’Œæ³¨æ„åŠ›å¤´æ•°
    """
    )


if __name__ == "__main__":
    # æµ‹è¯•è¿è¡Œ
    vit_analysis_tab()
