"""
å•ç¥ç»å…ƒå¯è§†åŒ–æ¨¡å— - Single Neuron Visualization
==================================================

é€šè¿‡å•ä¸ªç¥ç»å…ƒçš„è§†è§’ï¼Œæ·±å…¥ç†è§£ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚

æ ¸å¿ƒæ¦‚å¿µï¼š
1. ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬è®¡ç®—å•å…ƒ
2. å‰å‘ä¼ æ’­ï¼šåŠ æƒå’Œ + æ¿€æ´»å‡½æ•°
3. åå‘ä¼ æ’­ï¼šé“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦
4. å‚æ•°æ›´æ–°ï¼šæ¢¯åº¦ä¸‹é™ä¼˜åŒ–

Author: Neural Network Math Explorer
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from simple_latex import display_latex


class SingleNeuron:
    """
    å•ç¥ç»å…ƒæ¨¡å‹ - ç¥ç»ç½‘ç»œçš„æœ€å°è®¡ç®—å•å…ƒ
    
    æ•°å­¦æ¨¡å‹ï¼š
        å‰å‘ä¼ æ’­: y = activation(w^T Â· x + b)
        å…¶ä¸­ï¼š
        - x: è¾“å…¥å‘é‡
        - w: æƒé‡å‘é‡
        - b: åç½®
        - activation: æ¿€æ´»å‡½æ•°
    """
    
    def __init__(self, input_size=3, activation='relu', seed=42):
        """
        åˆå§‹åŒ–å•ç¥ç»å…ƒ
        
        Args:
            input_size: è¾“å…¥ç»´åº¦
            activation: æ¿€æ´»å‡½æ•°ç±»å‹ ('relu', 'sigmoid', 'tanh')
            seed: éšæœºç§å­
        """
        np.random.seed(seed)
        self.input_size = input_size
        self.activation_name = activation
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®ï¼ˆå°éšæœºå€¼ï¼‰
        self.weights = np.random.randn(input_size) * 0.5
        self.bias = np.random.randn() * 0.1
        
        # å­˜å‚¨è®¡ç®—å†å²ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.forward_history = {}
        self.backward_history = {}
    
    def activation(self, z):
        """æ¿€æ´»å‡½æ•°"""
        if self.activation_name == 'relu':
            return np.maximum(0, z)
        elif self.activation_name == 'sigmoid':
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation_name == 'tanh':
            return np.tanh(z)
        else:
            return z
    
    def activation_derivative(self, z):
        """æ¿€æ´»å‡½æ•°çš„å¯¼æ•°"""
        if self.activation_name == 'relu':
            return (z > 0).astype(float)
        elif self.activation_name == 'sigmoid':
            s = self.activation(z)
            return s * (1 - s)
        elif self.activation_name == 'tanh':
            return 1 - np.tanh(z) ** 2
        else:
            return np.ones_like(z)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        è®¡ç®—æ­¥éª¤ï¼š
        1. åŠ æƒå’Œ: z = w^T Â· x + b = sum(w_i * x_i) + b
        2. æ¿€æ´»: y = activation(z)
        
        Args:
            x: è¾“å…¥å‘é‡ (input_size,)
            
        Returns:
            output: ç¥ç»å…ƒè¾“å‡º
        """
        x = np.array(x, dtype=np.float64)
        
        # æ­¥éª¤1: è®¡ç®—åŠ æƒå’Œ
        weighted_sum = np.dot(self.weights, x) + self.bias
        
        # æ­¥éª¤2: åº”ç”¨æ¿€æ´»å‡½æ•°
        output = self.activation(weighted_sum)
        
        # ä¿å­˜å†å²ç”¨äºå¯è§†åŒ–å’Œåå‘ä¼ æ’­
        self.forward_history = {
            'input': x.copy(),
            'weights': self.weights.copy(),
            'bias': self.bias,
            'weighted_sum': weighted_sum,
            'activation_derivative': self.activation_derivative(weighted_sum),
            'output': output
        }
        
        return output
    
    def backward(self, upstream_gradient):
        """
        åå‘ä¼ æ’­ - ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦
        
        é“¾å¼æ³•åˆ™ï¼š
        âˆ‚L/âˆ‚w_i = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z Â· âˆ‚z/âˆ‚w_i
                = upstream_grad Â· activation'(z) Â· x_i
        
        âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z Â· âˆ‚z/âˆ‚b
              = upstream_grad Â· activation'(z) Â· 1
        
        Args:
            upstream_gradient: æ¥è‡ªæŸå¤±å‡½æ•°çš„æ¢¯åº¦ âˆ‚L/âˆ‚y
            
        Returns:
            gradients: åŒ…å«æ‰€æœ‰å‚æ•°æ¢¯åº¦çš„å­—å…¸
        """
        # å±€éƒ¨æ¢¯åº¦: âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z
        local_gradient = upstream_gradient * self.forward_history['activation_derivative']
        
        # æƒé‡æ¢¯åº¦: âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚w = âˆ‚L/âˆ‚z Â· x
        grad_weights = local_gradient * self.forward_history['input']
        
        # åç½®æ¢¯åº¦: âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚b = âˆ‚L/âˆ‚z Â· 1
        grad_bias = local_gradient
        
        # è¾“å…¥æ¢¯åº¦ï¼ˆç”¨äºå¤šå±‚ç½‘ç»œï¼‰: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚x = âˆ‚L/âˆ‚z Â· w
        grad_input = local_gradient * self.weights
        
        # ä¿å­˜æ¢¯åº¦å†å²
        self.backward_history = {
            'upstream_gradient': upstream_gradient,
            'local_gradient': local_gradient,
            'grad_weights': grad_weights,
            'grad_bias': grad_bias,
            'grad_input': grad_input
        }
        
        return {
            'weights': grad_weights,
            'bias': grad_bias,
            'input': grad_input
        }
    
    def update_parameters(self, learning_rate=0.01):
        """
        ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°
        
        æ›´æ–°è§„åˆ™ï¼š
        w_new = w_old - learning_rate Â· âˆ‚L/âˆ‚w
        b_new = b_old - learning_rate Â· âˆ‚L/âˆ‚b
        
        Args:
            learning_rate: å­¦ä¹ ç‡
        """
        self.weights -= learning_rate * self.backward_history['grad_weights']
        self.bias -= learning_rate * self.backward_history['grad_bias']


def create_computation_table(neuron, precision=4):
    """
    åˆ›å»ºå‰å‘ä¼ æ’­è®¡ç®—æ­¥éª¤è¡¨æ ¼
    
    Args:
        neuron: SingleNeuronå®ä¾‹
        precision: æ•°å€¼ç²¾åº¦
        
    Returns:
        pd.DataFrame: è®¡ç®—æ­¥éª¤è¡¨æ ¼
    """
    history = neuron.forward_history
    steps = []
    
    # æ­¥éª¤1: æ˜¾ç¤ºè¾“å…¥
    for i, val in enumerate(history['input']):
        steps.append({
            'æ­¥éª¤': f'è¾“å…¥ {i+1}',
            'ç¬¦å·': f'$x_{{{i}}}$',
            'æ•°å€¼': round(val, precision),
            'è¯´æ˜': f'ç¬¬{i+1}ä¸ªè¾“å…¥ç‰¹å¾'
        })
    
    # æ­¥éª¤2: æ˜¾ç¤ºæƒé‡
    for i, w in enumerate(history['weights']):
        steps.append({
            'æ­¥éª¤': f'æƒé‡ {i+1}',
            'ç¬¦å·': f'$w_{{{i}}}$',
            'æ•°å€¼': round(w, precision),
            'è¯´æ˜': f'ç¬¬{i+1}ä¸ªæƒé‡å‚æ•°'
        })
    
    # æ­¥éª¤3: æ˜¾ç¤ºåç½®
    steps.append({
        'æ­¥éª¤': 'åç½®',
        'ç¬¦å·': r'$b$',
        'æ•°å€¼': round(history['bias'], precision),
        'è¯´æ˜': 'åç½®é¡¹'
    })
    
    # æ­¥éª¤4: è®¡ç®—åŠ æƒå’Œ
    weighted_parts = [f"({round(w, precision)} Ã— {round(x, precision)})" 
                     for x, w in zip(history['input'], history['weights'])]
    steps.append({
        'æ­¥éª¤': 'åŠ æƒå’Œ',
        'ç¬¦å·': r'$z = \sum_{i} w_i \cdot x_i + b$',
        'æ•°å€¼': round(history['weighted_sum'], precision),
        'è¯´æ˜': f'{" + ".join(weighted_parts)} + {round(history["bias"], precision)}'
    })
    
    # æ­¥éª¤5: æ¿€æ´»å‡½æ•°
    steps.append({
        'æ­¥éª¤': 'æ¿€æ´»å‡½æ•°',
        'ç¬¦å·': f'$y = {neuron.activation_name}(z)$',
        'æ•°å€¼': round(history['output'], precision),
        'è¯´æ˜': f'{neuron.activation_name}({round(history["weighted_sum"], precision)}) = {round(history["output"], precision)}'
    })
    
    return pd.DataFrame(steps)


def create_gradient_table(neuron, precision=6):
    """
    åˆ›å»ºåå‘ä¼ æ’­æ¢¯åº¦è¡¨æ ¼
    
    Args:
        neuron: SingleNeuronå®ä¾‹
        precision: æ•°å€¼ç²¾åº¦
        
    Returns:
        pd.DataFrame: æ¢¯åº¦è¡¨æ ¼
    """
    backward_hist = neuron.backward_history
    forward_hist = neuron.forward_history
    gradients = []
    
    # ä¸Šæ¸¸æ¢¯åº¦
    gradients.append({
        'æ¢¯åº¦ç±»å‹': 'ä¸Šæ¸¸æ¢¯åº¦',
        'ç¬¦å·': r'$\frac{\partial L}{\partial y}$',
        'æ•°å€¼': round(backward_hist['upstream_gradient'], precision),
        'è¯´æ˜': 'æ¥è‡ªæŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼ˆå‡è®¾å€¼ï¼‰'
    })
    
    # æ¿€æ´»å‡½æ•°å¯¼æ•°
    gradients.append({
        'æ¢¯åº¦ç±»å‹': 'æ¿€æ´»å‡½æ•°å¯¼æ•°',
        'ç¬¦å·': r'$\frac{\partial y}{\partial z}$',
        'æ•°å€¼': round(forward_hist['activation_derivative'], precision),
        'è¯´æ˜': f"{neuron.activation_name}'({round(forward_hist['weighted_sum'], precision)})"
    })
    
    # å±€éƒ¨æ¢¯åº¦
    gradients.append({
        'æ¢¯åº¦ç±»å‹': 'å±€éƒ¨æ¢¯åº¦',
        'ç¬¦å·': r'$\frac{\partial L}{\partial z}$',
        'æ•°å€¼': round(backward_hist['local_gradient'], precision),
        'è¯´æ˜': f"= {round(backward_hist['upstream_gradient'], precision)} Ã— {round(forward_hist['activation_derivative'], precision)}"
    })
    
    # æƒé‡æ¢¯åº¦
    for i, grad_w in enumerate(backward_hist['grad_weights']):
        gradients.append({
            'æ¢¯åº¦ç±»å‹': f'æƒé‡æ¢¯åº¦ {i+1}',
            'ç¬¦å·': rf'$\frac{{\partial L}}{{\partial w_{{{i}}}}}$',
            'æ•°å€¼': round(grad_w, precision),
            'è¯´æ˜': f"= {round(backward_hist['local_gradient'], precision)} Ã— {round(forward_hist['input'][i], precision)}"
        })
    
    # åç½®æ¢¯åº¦
    gradients.append({
        'æ¢¯åº¦ç±»å‹': 'åç½®æ¢¯åº¦',
        'ç¬¦å·': r'$\frac{\partial L}{\partial b}$',
        'æ•°å€¼': round(backward_hist['grad_bias'], precision),
        'è¯´æ˜': f"= {round(backward_hist['local_gradient'], precision)} Ã— 1"
    })
    
    return pd.DataFrame(gradients)


def visualize_forward_pass(neuron):
    """
    å¯è§†åŒ–å‰å‘ä¼ æ’­è¿‡ç¨‹
    
    Args:
        neuron: SingleNeuronå®ä¾‹
        
    Returns:
        plotly Figureå¯¹è±¡
    """
    history = neuron.forward_history
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('è¾“å…¥ä¸æƒé‡', 'åŠ æƒå’Œè®¡ç®—', 'æ¿€æ´»å‡½æ•°æ›²çº¿', 'è®¡ç®—æµç¨‹'),
        specs=[[{'type': 'bar'}, {'type': 'indicator'}],
               [{'type': 'scatter'}, {'type': 'bar'}]]
    )
    
    # å­å›¾1: è¾“å…¥ä¸æƒé‡å¯¹æ¯”
    x_labels = [f'x[{i}]' for i in range(len(history['input']))]
    fig.add_trace(
        go.Bar(name='è¾“å…¥å€¼', x=x_labels, y=history['input'], 
               marker_color='lightblue'),
        row=1, col=1
    )
    fig.add_trace(
        go.Bar(name='æƒé‡å€¼', x=x_labels, y=history['weights'], 
               marker_color='lightcoral'),
        row=1, col=1
    )
    
    # å­å›¾2: åŠ æƒå’ŒæŒ‡ç¤ºå™¨
    fig.add_trace(
        go.Indicator(
            mode="number+delta",
            value=history['weighted_sum'],
            title={'text': "åŠ æƒå’Œ z"},
            delta={'reference': 0},
        ),
        row=1, col=2
    )
    
    # å­å›¾3: æ¿€æ´»å‡½æ•°æ›²çº¿
    z_range = np.linspace(-3, 3, 100)
    if neuron.activation_name == 'relu':
        y_range = np.maximum(0, z_range)
    elif neuron.activation_name == 'sigmoid':
        y_range = 1 / (1 + np.exp(-z_range))
    elif neuron.activation_name == 'tanh':
        y_range = np.tanh(z_range)
    else:
        y_range = z_range
    
    fig.add_trace(
        go.Scatter(x=z_range, y=y_range, mode='lines', 
                   name=neuron.activation_name, line=dict(color='blue')),
        row=2, col=1
    )
    fig.add_trace(
        go.Scatter(x=[history['weighted_sum']], y=[history['output']], 
                   mode='markers', name='å½“å‰ç‚¹', 
                   marker=dict(size=12, color='red')),
        row=2, col=1
    )
    
    # å­å›¾4: åŠ æƒä¹˜ç§¯åˆ†è§£
    products = history['input'] * history['weights']
    x_labels_prod = [f'w[{i}]Ã—x[{i}]' for i in range(len(products))]
    fig.add_trace(
        go.Bar(x=x_labels_prod, y=products, 
               marker_color='lightgreen', name='wÃ—x'),
        row=2, col=2
    )
    
    fig.update_layout(height=700, showlegend=True, 
                      title_text="å•ç¥ç»å…ƒå‰å‘ä¼ æ’­å¯è§†åŒ–")
    
    return fig


def visualize_backward_pass(neuron):
    """
    å¯è§†åŒ–åå‘ä¼ æ’­è¿‡ç¨‹
    
    Args:
        neuron: SingleNeuronå®ä¾‹
        
    Returns:
        plotly Figureå¯¹è±¡
    """
    backward_hist = neuron.backward_history
    forward_hist = neuron.forward_history
    
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('æ¢¯åº¦æµåŠ¨', 'å‚æ•°æ¢¯åº¦åˆ†å¸ƒ'),
        specs=[[{'type': 'bar'}, {'type': 'bar'}]]
    )
    
    # å­å›¾1: æ¢¯åº¦æµåŠ¨ï¼ˆä»è¾“å‡ºåˆ°è¾“å…¥ï¼‰
    gradient_flow = [
        backward_hist['upstream_gradient'],
        backward_hist['local_gradient'],
        np.mean(np.abs(backward_hist['grad_weights']))
    ]
    gradient_labels = ['ä¸Šæ¸¸æ¢¯åº¦\nâˆ‚L/âˆ‚y', 'å±€éƒ¨æ¢¯åº¦\nâˆ‚L/âˆ‚z', 'æƒé‡æ¢¯åº¦\nâˆ‚L/âˆ‚w']
    
    fig.add_trace(
        go.Bar(x=gradient_labels, y=gradient_flow, 
               marker_color=['red', 'orange', 'yellow']),
        row=1, col=1
    )
    
    # å­å›¾2: å„å‚æ•°æ¢¯åº¦å¤§å°
    param_labels = [f'w[{i}]' for i in range(len(backward_hist['grad_weights']))] + ['b']
    param_grads = list(backward_hist['grad_weights']) + [backward_hist['grad_bias']]
    
    fig.add_trace(
        go.Bar(x=param_labels, y=param_grads, 
               marker_color='lightblue'),
        row=1, col=2
    )
    
    fig.update_layout(height=400, showlegend=False,
                      title_text="å•ç¥ç»å…ƒåå‘ä¼ æ’­å¯è§†åŒ–")
    
    return fig


def single_neuron_tab(CHINESE_SUPPORTED=True):
    """
    å•ç¥ç»å…ƒå¯è§†åŒ–æ ‡ç­¾é¡µ - ä¸»UIç•Œé¢
    """
    
    if CHINESE_SUPPORTED:
        st.header("ğŸ§¬ å•ç¥ç»å…ƒï¼šç†è§£ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒ")
        
        st.markdown("""
        ### ğŸ’¡ æ ¸å¿ƒæ€æƒ³
        
        ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„**æœ€å°è®¡ç®—å•å…ƒ**ã€‚ç†è§£å•ä¸ªç¥ç»å…ƒå¦‚ä½•å·¥ä½œï¼Œå°±èƒ½ç†è§£æ•´ä¸ªç¥ç»ç½‘ç»œçš„è¿ä½œåŸç†ã€‚
        
        **ä¸€ä¸ªç¥ç»å…ƒåšä»€ä¹ˆï¼Ÿ**
        1. æ¥æ”¶å¤šä¸ªè¾“å…¥ä¿¡å·
        2. å¯¹æ¯ä¸ªè¾“å…¥åŠ æƒæ±‚å’Œï¼ˆåŠ ä¸Šåç½®ï¼‰
        3. é€šè¿‡æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§
        4. è¾“å‡ºå¤„ç†åçš„ä¿¡å·
        """)
        
        # æ˜¾ç¤ºæ•°å­¦å…¬å¼
        st.markdown("### ğŸ“ æ•°å­¦è¡¨è¾¾")
        st.markdown(r"""

        $\text{å‰å‘ä¼ æ’­ï¼š}$
        $$
        \begin{aligned}
        z &= \sum_{i=0}^{n} w_i \cdot x_i + b = w^T x + b \\
        y &= \text{activation}(z)
        \end{aligned}
        $$
        
        $\text{åå‘ä¼ æ’­ï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š}$

        $$
        \begin{aligned}
        \frac{\partial L}{\partial w_i} &= \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w_i} 
        = \frac{\partial L}{\partial y} \cdot \text{activation}'(z) \cdot x_i \\
        \frac{\partial L}{\partial b} &= \frac{\partial L}{\partial y} \cdot \text{activation}'(z)
        \end{aligned}
        $$
        """)
        
        st.markdown("---")
        
        # é…ç½®åŒºåŸŸ
        st.subheader("âš™ï¸ é…ç½®ç¥ç»å…ƒ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            input_size = st.slider("è¾“å…¥ç»´åº¦", 1, 5, 3, 
                                   help="ç¥ç»å…ƒæ¥æ”¶å¤šå°‘ä¸ªè¾“å…¥")
            activation = st.selectbox(
                "æ¿€æ´»å‡½æ•°",
                ['relu', 'sigmoid', 'tanh'],
                help="é€‰æ‹©æ¿€æ´»å‡½æ•°ç±»å‹"
            )
        
        with col2:
            seed = st.number_input("éšæœºç§å­", 0, 100, 42, 
                                   help="ç”¨äºåˆå§‹åŒ–æƒé‡")
            learning_rate = st.slider("å­¦ä¹ ç‡", 0.001, 0.5, 0.01, 0.001,
                                      help="æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿")
        
        # åˆ›å»ºç¥ç»å…ƒ
        neuron = SingleNeuron(input_size=input_size, activation=activation, seed=seed)
        
        st.markdown("---")
        
        # è¾“å…¥æ•°æ®
        st.subheader("ğŸ“¥ è¾“å…¥æ•°æ®")
        
        st.write("è®¾ç½®è¾“å…¥å€¼ï¼š")
        input_cols = st.columns(input_size)
        input_data = []
        for i, col in enumerate(input_cols):
            with col:
                val = st.number_input(
                    f"$x_{{{i}}}$",
                    value=float(np.random.randn() * 0.5),
                    format="%.4f",
                    key=f"input_{i}"
                )
                input_data.append(val)
        
        input_data = np.array(input_data)
        
        # æ˜¾ç¤ºå½“å‰å‚æ•°
        st.subheader("ğŸ¯ å½“å‰å‚æ•°")
        param_col1, param_col2 = st.columns(2)
        
        with param_col1:
            st.write("**æƒé‡å‘é‡ w:**")
            weights_df = pd.DataFrame({
                'ç´¢å¼•': [f'w[{i}]' for i in range(input_size)],
                'æ•°å€¼': [f'{w:.6f}' for w in neuron.weights]
            })
            st.markdown(weights_df.to_markdown(index=False))
        
        with param_col2:
            st.write("**åç½® b:**")
            st.metric("bias", f"{neuron.bias:.6f}")
        
        st.markdown("---")
        
        # ä¸Šæ¸¸æ¢¯åº¦è®¾ç½®
        st.subheader("ğŸ¯ ä¸Šæ¸¸æ¢¯åº¦è®¾ç½®")
        upstream_grad = st.number_input(
            "ä¸Šæ¸¸æ¢¯åº¦ (âˆ‚L/âˆ‚y)",
            value=1.0,
            format="%.6f",
            help="å‡è®¾è¿™æ˜¯ä»æŸå¤±å‡½æ•°ä¼ å›çš„æ¢¯åº¦ã€‚åœ¨çœŸå®è®­ç»ƒä¸­ï¼Œè¿™æ¥è‡ªæŸå¤±å‡½æ•°å¯¹è¾“å‡ºçš„å¯¼æ•°ã€‚"
        )
        
        st.markdown("---")
        
        # æ‰§è¡Œå®Œæ•´çš„å‰å‘-åå‘-æ›´æ–°æµç¨‹
        if st.button("ğŸš€ æ‰§è¡Œå®Œæ•´è®¡ç®—æµç¨‹ï¼ˆå‰å‘â†’åå‘â†’æ›´æ–°ï¼‰", type="primary"):
            # ==================== å‰å‘ä¼ æ’­ ====================
            st.subheader("â¡ï¸ 1. å‰å‘ä¼ æ’­")
            output = neuron.forward(input_data)
            
            st.success(f"âœ… ç¥ç»å…ƒè¾“å‡º: **{output:.6f}**")
            
            # è®¡ç®—æ­¥éª¤è¡¨æ ¼
            with st.expander("ğŸ“‹ è¯¦ç»†è®¡ç®—æ­¥éª¤", expanded=True):
                comp_table = create_computation_table(neuron)
                st.markdown(comp_table.to_markdown(index=False))
            
            # å¯è§†åŒ–
            with st.expander("ğŸ“Š å‰å‘ä¼ æ’­å¯è§†åŒ–", expanded=True):
                fig_forward = visualize_forward_pass(neuron)
                st.plotly_chart(fig_forward, use_container_width=True)
            
            # ==================== åå‘ä¼ æ’­ ====================
            st.markdown("---")
            st.subheader("â¬…ï¸ 2. åå‘ä¼ æ’­")
            
            # ä¿å­˜æ—§å‚æ•°ç”¨äºå¯¹æ¯”
            old_weights = neuron.weights.copy()
            old_bias = neuron.bias
            
            gradients = neuron.backward(upstream_grad)
            
            st.success("âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ")
            
            # æ¢¯åº¦è¡¨æ ¼
            with st.expander("ğŸ“‹ æ¢¯åº¦è¯¦ç»†ä¿¡æ¯", expanded=True):
                grad_table = create_gradient_table(neuron)
                st.markdown(grad_table.to_markdown(index=False))
            
            # å¯è§†åŒ–
            with st.expander("ğŸ“Š æ¢¯åº¦æµåŠ¨å¯è§†åŒ–", expanded=True):
                fig_backward = visualize_backward_pass(neuron)
                st.plotly_chart(fig_backward, use_container_width=True)
            
            # ==================== å‚æ•°æ›´æ–° ====================
            st.markdown("---")
            st.subheader("ğŸ“Š 3. å‚æ•°æ›´æ–°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰")
            
            # æ›´æ–°å‚æ•°
            neuron.update_parameters(learning_rate)
            
            st.success("âœ… å‚æ•°å·²æ›´æ–°")
            
            # æ˜¾ç¤ºæ›´æ–°è¯¦æƒ…
            st.write("**å‚æ•°å˜åŒ–å¯¹æ¯”ï¼š**")
            
            update_data = []
            for i in range(input_size):
                delta = neuron.weights[i] - old_weights[i]
                update_data.append({
                    'å‚æ•°': f'$w_{{{i}}}$',
                    'æ›´æ–°å‰': f'{old_weights[i]:.6f}',
                    'æ¢¯åº¦': f'{gradients["weights"][i]:.6f}',
                    'æ›´æ–°é‡': f'{-learning_rate * gradients["weights"][i]:.6f}',
                    'æ›´æ–°å': f'{neuron.weights[i]:.6f}',
                    'å˜åŒ–': f'{delta:.6f}'
                })
            
            delta_bias = neuron.bias - old_bias
            update_data.append({
                'å‚æ•°': '$b$',
                'æ›´æ–°å‰': f'{old_bias:.6f}',
                'æ¢¯åº¦': f'{gradients["bias"]:.6f}',
                'æ›´æ–°é‡': f'{-learning_rate * gradients["bias"]:.6f}',
                'æ›´æ–°å': f'{neuron.bias:.6f}',
                'å˜åŒ–': f'{delta_bias:.6f}'
            })
            
            update_df = pd.DataFrame(update_data)
            st.markdown(update_df.to_markdown(index=False))
            
            st.info(f"ğŸ’¡ **æ›´æ–°è§„åˆ™**: $\\theta_{{\text{{new}}}} = \\theta_{{\text{{old}}}} - \\alpha \\cdot \\frac{{\\partial L}}{{\\partial \\theta}}$ï¼Œå…¶ä¸­å­¦ä¹ ç‡ $\\alpha = {learning_rate}$")
            
            # ==================== æ€»ç»“ ====================
            st.markdown("---")
            st.subheader("ğŸ“ˆ å®Œæ•´æµç¨‹æ€»ç»“")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    "å‰å‘ä¼ æ’­è¾“å‡º",
                    f"{output:.6f}",
                    help="ç¥ç»å…ƒçš„æœ€ç»ˆè¾“å‡º"
                )
            
            with col2:
                avg_grad = np.mean(np.abs(gradients['weights']))
                st.metric(
                    "å¹³å‡æƒé‡æ¢¯åº¦",
                    f"{avg_grad:.6f}",
                    help="æƒé‡æ¢¯åº¦çš„å¹³å‡ç»å¯¹å€¼"
                )
            
            with col3:
                total_change = np.linalg.norm(neuron.weights - old_weights)
                st.metric(
                    "å‚æ•°å˜åŒ–é‡",
                    f"{total_change:.6f}",
                    help="æ‰€æœ‰æƒé‡å˜åŒ–çš„L2èŒƒæ•°"
                )
            
            st.success("""
            âœ… **å®Œæ•´è®­ç»ƒæ­¥éª¤å·²å®Œæˆï¼**
            
            è¿™å°±æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒçš„ä¸€ä¸ªå®Œæ•´è¿­ä»£ï¼š
            1. **å‰å‘ä¼ æ’­**ï¼šè¾“å…¥ â†’ åŠ æƒå’Œ â†’ æ¿€æ´» â†’ è¾“å‡º
            2. **åå‘ä¼ æ’­**ï¼šè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
            3. **å‚æ•°æ›´æ–°**ï¼šæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°
            
            åœ¨å®é™…è®­ç»ƒä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤æˆåƒä¸Šä¸‡æ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ ·æœ¬ã€‚
            """)
        
        # æ•™å­¦è¯´æ˜
        st.markdown("---")
        with st.expander("ğŸ“š è¯¦ç»†è¯´æ˜ï¼šç¥ç»å…ƒå¦‚ä½•å·¥ä½œ", expanded=False):
            st.markdown("""
            ### ğŸ” æ·±å…¥ç†è§£
            
            #### 1. å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰
            
            ç¥ç»å…ƒæ¥æ”¶è¾“å…¥åï¼Œæ‰§è¡Œä¸¤æ­¥è®¡ç®—ï¼š
            
            **æ­¥éª¤1: çº¿æ€§ç»„åˆï¼ˆåŠ æƒå’Œï¼‰**
            - æ¯ä¸ªè¾“å…¥ $x_i$ ä¹˜ä»¥å¯¹åº”çš„æƒé‡ $w_i$
            - å°†æ‰€æœ‰ä¹˜ç§¯ç›¸åŠ ï¼Œå†åŠ ä¸Šåç½® $b$
            - ç»“æœï¼š$z = w_0 x_0 + w_1 x_1 + ... + w_n x_n + b$
            
            **æ­¥éª¤2: éçº¿æ€§æ¿€æ´»**
            - å°† $z$ é€šè¿‡æ¿€æ´»å‡½æ•°è½¬æ¢
            - æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œè®©ç½‘ç»œèƒ½å­¦ä¹ å¤æ‚æ¨¡å¼
            - å¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼š
                - **ReLU**: $f(z) = \\max(0, z)$ - ç®€å•æœ‰æ•ˆï¼Œæœ€å¸¸ç”¨
                - **Sigmoid**: $f(z) = \\frac{1}{1+e^{-z}}$ - è¾“å‡º0åˆ°1ï¼Œç”¨äºæ¦‚ç‡
                - **Tanh**: $f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ - è¾“å‡º-1åˆ°1ï¼Œé›¶ä¸­å¿ƒåŒ–
            
            #### 2. åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰
            
            ä½¿ç”¨**é“¾å¼æ³•åˆ™**è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼š
            
            **æ ¸å¿ƒæ€æƒ³**: ä»è¾“å‡ºå¾€å›ä¼ æ’­è¯¯å·®
            
            - **ä¸Šæ¸¸æ¢¯åº¦** $\\frac{\\partial L}{\\partial y}$: æ¥è‡ªæŸå¤±å‡½æ•°æˆ–ä¸‹ä¸€å±‚
            - **æ¿€æ´»å‡½æ•°å¯¼æ•°** $\\frac{\\partial y}{\\partial z}$: æ¿€æ´»å‡½æ•°åœ¨å½“å‰ç‚¹çš„æ–œç‡
            - **å±€éƒ¨æ¢¯åº¦** $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z}$
            - **å‚æ•°æ¢¯åº¦**: 
                - $\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial z} \\cdot x_i$
                - $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z}$
            
            #### 3. å‚æ•°æ›´æ–°ï¼ˆParameter Updateï¼‰
            
            ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å‚æ•°ï¼š
            
            $$
            w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial w}
            $$
            
            - $\\alpha$ æ˜¯å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ›´æ–°æ­¥é•¿
            - æ¢¯åº¦æŒ‡å‘è¯¯å·®å¢åŠ çš„æ–¹å‘ï¼Œæ‰€ä»¥è¦å‡å»æ¢¯åº¦
            - è¿­ä»£å¤šæ¬¡åï¼Œå‚æ•°ä¼šé€æ¸ä¼˜åŒ–
            
            #### 4. å…³é”®æ´å¯Ÿ
            
            - **æƒé‡**å†³å®šæ¯ä¸ªè¾“å…¥çš„é‡è¦æ€§
            - **åç½®**è°ƒæ•´ç¥ç»å…ƒçš„æ¿€æ´»é˜ˆå€¼
            - **æ¿€æ´»å‡½æ•°**å¼•å…¥éçº¿æ€§ï¼Œæ˜¯æ·±åº¦å­¦ä¹ çš„å…³é”®
            - **æ¢¯åº¦**æŒ‡ç¤ºå‚æ•°åº”è¯¥å¦‚ä½•è°ƒæ•´
            - **å­¦ä¹ ç‡**æ§åˆ¶å­¦ä¹ é€Ÿåº¦ï¼ˆè¿‡å¤§éœ‡è¡ï¼Œè¿‡å°ç¼“æ…¢ï¼‰
            
            #### 5. ä»å•ç¥ç»å…ƒåˆ°ç¥ç»ç½‘ç»œ
            
            - å¤šä¸ªç¥ç»å…ƒå¹¶è¡Œ â†’ **å±‚ï¼ˆLayerï¼‰**
            - å¤šä¸ªå±‚å †å  â†’ **æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰**
            - æ¯ä¸€å±‚åšç±»ä¼¼çš„è®¡ç®—ï¼š$\\text{output} = \\text{activation}(W \\cdot \\text{input} + b)$
            - é€šè¿‡åå‘ä¼ æ’­ï¼Œæ‰€æœ‰å±‚çš„å‚æ•°åŒæ—¶æ›´æ–°
            
            **ç†è§£å•ä¸ªç¥ç»å…ƒ = ç†è§£æ•´ä¸ªç¥ç»ç½‘ç»œï¼**
            """)
    
    else:
        st.header("ğŸ§¬ Single Neuron: Understanding the Basic Unit")
        st.info("English version - Coming soon!")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    neuron = SingleNeuron(input_size=3, activation='relu')
    x = np.array([0.5, -0.3, 0.2])
    
    # å‰å‘ä¼ æ’­
    output = neuron.forward(x)
    print(f"Forward pass output: {output:.6f}")
    
    # åå‘ä¼ æ’­
    gradients = neuron.backward(upstream_gradient=1.0)
    print(f"Gradients: {gradients}")
    
    # å‚æ•°æ›´æ–°
    neuron.update_parameters(learning_rate=0.01)
    print("Parameters updated!")

