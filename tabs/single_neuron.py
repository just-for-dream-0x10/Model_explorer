"""
å•ç¥ç»å…ƒå¯è§†åŒ–æ¨¡å— - Single Neuron Visualization
==================================================

é€šè¿‡å•ä¸ªç¥ç»å…ƒçš„è§†è§’ï¼Œæ·±å…¥ç†è§£ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚

æ ¸å¿ƒæ¦‚å¿µï¼š
1. ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬è®¡ç®—å•å…ƒ
2. å‰å‘ä¼ æ’­ï¼šåŠ æƒå’Œ + æ¿€æ´»å‡½æ•°
3. åå‘ä¼ æ’­ï¼šé“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦
4. å‚æ•°æ›´æ–°ï¼šæ¢¯åº¦ä¸‹é™ä¼˜åŒ–

æ”¯æŒå¤šç§ç¥ç»å…ƒç±»å‹ï¼š
- å…¨è¿æ¥ç¥ç»å…ƒï¼ˆDense/FCï¼‰
- å·ç§¯ç¥ç»å…ƒï¼ˆConvï¼‰
- å¾ªç¯ç¥ç»å…ƒï¼ˆRNNï¼‰
- GRUç¥ç»å…ƒ
- LSTMç¥ç»å…ƒ
- æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰

v2.2.0 æ–°å¢ï¼š
- æ•°å€¼ç¨³å®šæ€§è‡ªåŠ¨æ£€æµ‹
- æ¢¯åº¦éªŒè¯
- é—®é¢˜è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ

Author: Neural Network Math Explorer
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from simple_latex import display_latex
from utils.numerical_stability_checker import (
    StabilityChecker,
    compute_numerical_gradient,
)


class SingleNeuron:
    """
    å•ç¥ç»å…ƒæ¨¡å‹ - ç¥ç»ç½‘ç»œçš„æœ€å°è®¡ç®—å•å…ƒ

    æ•°å­¦æ¨¡å‹ï¼š
        å‰å‘ä¼ æ’­: y = activation(w^T Â· x + b)
        å…¶ä¸­ï¼š
        - x: è¾“å…¥å‘é‡
        - w: æƒé‡å‘é‡
        - b: åç½®
        - activation: æ¿€æ´»å‡½æ•°
    """

    def __init__(self, input_size=3, activation="relu", seed=42):
        """
        åˆå§‹åŒ–å•ç¥ç»å…ƒ

        Args:
            input_size: è¾“å…¥ç»´åº¦
            activation: æ¿€æ´»å‡½æ•°ç±»å‹ ('relu', 'sigmoid', 'tanh')
            seed: éšæœºç§å­
        """
        np.random.seed(seed)
        self.input_size = input_size
        self.activation_name = activation

        # åˆå§‹åŒ–æƒé‡å’Œåç½®ï¼ˆå°éšæœºå€¼ï¼‰
        self.weights = np.random.randn(input_size) * 0.5
        self.bias = np.random.randn() * 0.1

        # å­˜å‚¨è®¡ç®—å†å²ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.forward_history = {}
        self.backward_history = {}

    def activation(self, z):
        """æ¿€æ´»å‡½æ•°"""
        if self.activation_name == "relu":
            return np.maximum(0, z)
        elif self.activation_name == "sigmoid":
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation_name == "tanh":
            return np.tanh(z)
        else:
            return z

    def activation_derivative(self, z):
        """æ¿€æ´»å‡½æ•°çš„å¯¼æ•°"""
        if self.activation_name == "relu":
            return (z > 0).astype(float)
        elif self.activation_name == "sigmoid":
            s = self.activation(z)
            return s * (1 - s)
        elif self.activation_name == "tanh":
            return 1 - np.tanh(z) ** 2
        else:
            return np.ones_like(z)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        è®¡ç®—æ­¥éª¤ï¼š
        1. åŠ æƒå’Œ: z = w^T Â· x + b = sum(w_i * x_i) + b
        2. æ¿€æ´»: y = activation(z)

        Args:
            x: è¾“å…¥å‘é‡ (input_size,)

        Returns:
            output: ç¥ç»å…ƒè¾“å‡º
        """
        x = np.array(x, dtype=np.float64)

        # æ­¥éª¤1: è®¡ç®—åŠ æƒå’Œ
        weighted_sum = np.dot(self.weights, x) + self.bias

        # æ­¥éª¤2: åº”ç”¨æ¿€æ´»å‡½æ•°
        output = self.activation(weighted_sum)

        # ä¿å­˜å†å²ç”¨äºå¯è§†åŒ–å’Œåå‘ä¼ æ’­
        self.forward_history = {
            "input": x.copy(),
            "weights": self.weights.copy(),
            "bias": self.bias,
            "weighted_sum": weighted_sum,
            "activation_derivative": self.activation_derivative(weighted_sum),
            "output": output,
        }

        return output

    def backward(self, upstream_gradient):
        """
        åå‘ä¼ æ’­ - ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦

        é“¾å¼æ³•åˆ™ï¼š
        âˆ‚L/âˆ‚w_i = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z Â· âˆ‚z/âˆ‚w_i
                = upstream_grad Â· activation'(z) Â· x_i

        âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z Â· âˆ‚z/âˆ‚b
              = upstream_grad Â· activation'(z) Â· 1

        Args:
            upstream_gradient: æ¥è‡ªæŸå¤±å‡½æ•°çš„æ¢¯åº¦ âˆ‚L/âˆ‚y

        Returns:
            gradients: åŒ…å«æ‰€æœ‰å‚æ•°æ¢¯åº¦çš„å­—å…¸
        """
        # å±€éƒ¨æ¢¯åº¦: âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z
        local_gradient = (
            upstream_gradient * self.forward_history["activation_derivative"]
        )

        # æƒé‡æ¢¯åº¦: âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚w = âˆ‚L/âˆ‚z Â· x
        grad_weights = local_gradient * self.forward_history["input"]

        # åç½®æ¢¯åº¦: âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚b = âˆ‚L/âˆ‚z Â· 1
        grad_bias = local_gradient

        # è¾“å…¥æ¢¯åº¦ï¼ˆç”¨äºå¤šå±‚ç½‘ç»œï¼‰: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚x = âˆ‚L/âˆ‚z Â· w
        grad_input = local_gradient * self.weights

        # ä¿å­˜æ¢¯åº¦å†å²
        self.backward_history = {
            "upstream_gradient": upstream_gradient,
            "local_gradient": local_gradient,
            "grad_weights": grad_weights,
            "grad_bias": grad_bias,
            "grad_input": grad_input,
        }

        return {"weights": grad_weights, "bias": grad_bias, "input": grad_input}

    def update_parameters(self, learning_rate=0.01):
        """
        ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°

        æ›´æ–°è§„åˆ™ï¼š
        w_new = w_old - learning_rate Â· âˆ‚L/âˆ‚w
        b_new = b_old - learning_rate Â· âˆ‚L/âˆ‚b

        Args:
            learning_rate: å­¦ä¹ ç‡
        """
        self.weights -= learning_rate * self.backward_history["grad_weights"]
        self.bias -= learning_rate * self.backward_history["grad_bias"]


class ConvNeuron:
    """
    å·ç§¯ç¥ç»å…ƒ - ç”¨äºå›¾åƒå¤„ç†

    æ•°å­¦æ¨¡å‹ï¼š
        å‰å‘ä¼ æ’­: y = activation(sum(kernel * patch) + b)
        å…¶ä¸­ï¼š
        - patch: è¾“å…¥å›¾åƒçš„å±€éƒ¨åŒºåŸŸ (kernel_size Ã— kernel_size)
        - kernel: å·ç§¯æ ¸ (æƒé‡)
        - b: åç½®
    """

    def __init__(self, kernel_size=3, activation="relu", seed=42):
        np.random.seed(seed)
        self.kernel_size = kernel_size
        self.activation_name = activation

        # åˆå§‹åŒ–å·ç§¯æ ¸å’Œåç½®
        self.kernel = np.random.randn(kernel_size, kernel_size) * 0.5
        self.bias = np.random.randn() * 0.1

        self.forward_history = {}
        self.backward_history = {}

    def activation(self, z):
        if self.activation_name == "relu":
            return np.maximum(0, z)
        elif self.activation_name == "sigmoid":
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation_name == "tanh":
            return np.tanh(z)
        return z

    def activation_derivative(self, z):
        if self.activation_name == "relu":
            return (z > 0).astype(float)
        elif self.activation_name == "sigmoid":
            s = self.activation(z)
            return s * (1 - s)
        elif self.activation_name == "tanh":
            return 1 - np.tanh(z) ** 2
        return np.ones_like(z)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ - å·ç§¯æ“ä½œ

        Args:
            x: è¾“å…¥å›¾åƒå— (kernel_size, kernel_size)
        """
        x = np.array(x, dtype=np.float64)

        # å·ç§¯æ“ä½œï¼šé€å…ƒç´ ç›¸ä¹˜å†æ±‚å’Œ
        weighted_sum = np.sum(self.kernel * x) + self.bias
        output = self.activation(weighted_sum)

        self.forward_history = {
            "input": x.copy(),
            "kernel": self.kernel.copy(),
            "bias": self.bias,
            "weighted_sum": weighted_sum,
            "activation_derivative": self.activation_derivative(weighted_sum),
            "output": output,
        }

        return output

    def backward(self, upstream_gradient):
        """åå‘ä¼ æ’­"""
        local_gradient = (
            upstream_gradient * self.forward_history["activation_derivative"]
        )

        # å·ç§¯æ ¸æ¢¯åº¦
        grad_kernel = local_gradient * self.forward_history["input"]
        grad_bias = local_gradient
        grad_input = local_gradient * self.kernel

        self.backward_history = {
            "upstream_gradient": upstream_gradient,
            "local_gradient": local_gradient,
            "grad_kernel": grad_kernel,
            "grad_bias": grad_bias,
            "grad_input": grad_input,
        }

        return {"kernel": grad_kernel, "bias": grad_bias, "input": grad_input}

    def update_parameters(self, learning_rate=0.01):
        """æ›´æ–°å‚æ•°"""
        self.kernel -= learning_rate * self.backward_history["grad_kernel"]
        self.bias -= learning_rate * self.backward_history["grad_bias"]


class RNNNeuron:
    """
    å¾ªç¯ç¥ç»å…ƒ - ç”¨äºåºåˆ—å¤„ç†

    æ•°å­¦æ¨¡å‹ï¼š
        h_t = activation(W_hh * h_{t-1} + W_xh * x_t + b)
        å…¶ä¸­ï¼š
        - h_t: å½“å‰éšè—çŠ¶æ€
        - h_{t-1}: å‰ä¸€æ—¶åˆ»éšè—çŠ¶æ€
        - x_t: å½“å‰è¾“å…¥
    """

    def __init__(self, input_size=3, hidden_size=3, activation="tanh", seed=42):
        np.random.seed(seed)
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.activation_name = activation

        # åˆå§‹åŒ–æƒé‡
        self.W_xh = np.random.randn(hidden_size, input_size) * 0.5  # è¾“å…¥åˆ°éšè—
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.5  # éšè—åˆ°éšè—
        self.b_h = np.random.randn(hidden_size) * 0.1

        # åˆå§‹åŒ–éšè—çŠ¶æ€
        self.h = np.zeros(hidden_size)

        self.forward_history = {}
        self.backward_history = {}

    def activation(self, z):
        if self.activation_name == "relu":
            return np.maximum(0, z)
        elif self.activation_name == "sigmoid":
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation_name == "tanh":
            return np.tanh(z)
        return z

    def activation_derivative(self, z):
        if self.activation_name == "relu":
            return (z > 0).astype(float)
        elif self.activation_name == "sigmoid":
            s = self.activation(z)
            return s * (1 - s)
        elif self.activation_name == "tanh":
            return 1 - np.tanh(z) ** 2
        return np.ones_like(z)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: å½“å‰æ—¶åˆ»è¾“å…¥ (input_size,)
        """
        x = np.array(x, dtype=np.float64)
        h_prev = self.h.copy()

        # RNN è®¡ç®—
        weighted_sum = np.dot(self.W_hh, h_prev) + np.dot(self.W_xh, x) + self.b_h
        self.h = self.activation(weighted_sum)

        self.forward_history = {
            "input": x.copy(),
            "h_prev": h_prev,
            "W_xh": self.W_xh.copy(),
            "W_hh": self.W_hh.copy(),
            "b_h": self.b_h.copy(),
            "weighted_sum": weighted_sum,
            "activation_derivative": self.activation_derivative(weighted_sum),
            "output": self.h.copy(),
        }

        return self.h

    def backward(self, upstream_gradient):
        """åå‘ä¼ æ’­"""
        local_gradient = (
            upstream_gradient * self.forward_history["activation_derivative"]
        )

        # è®¡ç®—å„å‚æ•°æ¢¯åº¦
        grad_W_xh = np.outer(local_gradient, self.forward_history["input"])
        grad_W_hh = np.outer(local_gradient, self.forward_history["h_prev"])
        grad_b_h = local_gradient

        # ä¼ é€’ç»™å‰ä¸€æ—¶åˆ»çš„æ¢¯åº¦
        grad_h_prev = np.dot(self.W_hh.T, local_gradient)
        grad_input = np.dot(self.W_xh.T, local_gradient)

        self.backward_history = {
            "upstream_gradient": upstream_gradient,
            "local_gradient": local_gradient,
            "grad_W_xh": grad_W_xh,
            "grad_W_hh": grad_W_hh,
            "grad_b_h": grad_b_h,
            "grad_h_prev": grad_h_prev,
            "grad_input": grad_input,
        }

        return {
            "W_xh": grad_W_xh,
            "W_hh": grad_W_hh,
            "b_h": grad_b_h,
            "h_prev": grad_h_prev,
            "input": grad_input,
        }

    def update_parameters(self, learning_rate=0.01):
        """æ›´æ–°å‚æ•°"""
        self.W_xh -= learning_rate * self.backward_history["grad_W_xh"]
        self.W_hh -= learning_rate * self.backward_history["grad_W_hh"]
        self.b_h -= learning_rate * self.backward_history["grad_b_h"]

    def reset_hidden(self):
        """é‡ç½®éšè—çŠ¶æ€"""
        self.h = np.zeros(self.hidden_size)


class GRUNeuron:
    """
    GRUç¥ç»å…ƒ - é—¨æ§å¾ªç¯å•å…ƒï¼ˆLSTMçš„ç®€åŒ–ç‰ˆï¼‰

    æ•°å­¦æ¨¡å‹ï¼š
        é‡ç½®é—¨: r_t = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)
        æ›´æ–°é—¨: z_t = Ïƒ(W_z Â· [h_{t-1}, x_t] + b_z)
        å€™é€‰éšè—çŠ¶æ€: hÌƒ_t = tanh(W_h Â· [r_t âŠ™ h_{t-1}, x_t] + b_h)
        éšè—çŠ¶æ€: h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t

    GRU vs LSTM:
    - GRUåªæœ‰2ä¸ªé—¨ï¼ˆé‡ç½®ã€æ›´æ–°ï¼‰vs LSTMçš„3ä¸ªé—¨ï¼ˆé—å¿˜ã€è¾“å…¥ã€è¾“å‡ºï¼‰
    - GRUæ²¡æœ‰å•ç‹¬çš„ç»†èƒçŠ¶æ€ï¼Œç›´æ¥æ›´æ–°éšè—çŠ¶æ€
    - GRUå‚æ•°é‡æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«
    - æ€§èƒ½é€šå¸¸ä¸LSTMç›¸å½“
    """

    def __init__(self, input_size=3, hidden_size=3, seed=42):
        np.random.seed(seed)
        self.input_size = input_size
        self.hidden_size = hidden_size

        combined_size = hidden_size + input_size

        # åˆå§‹åŒ–3ç»„æƒé‡ï¼ˆé‡ç½®é—¨ã€æ›´æ–°é—¨ã€å€™é€‰éšè—çŠ¶æ€ï¼‰
        self.W_r = np.random.randn(hidden_size, combined_size) * 0.5
        self.b_r = np.random.randn(hidden_size) * 0.1

        self.W_z = np.random.randn(hidden_size, combined_size) * 0.5
        self.b_z = np.random.randn(hidden_size) * 0.1

        self.W_h = np.random.randn(hidden_size, combined_size) * 0.5
        self.b_h = np.random.randn(hidden_size) * 0.1

        # åˆå§‹åŒ–éšè—çŠ¶æ€
        self.h = np.zeros(hidden_size)

        self.forward_history = {}
        self.backward_history = {}

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

    def sigmoid_derivative(self, z):
        s = self.sigmoid(z)
        return s * (1 - s)

    def tanh_derivative(self, z):
        return 1 - np.tanh(z) ** 2

    def forward(self, x):
        """
        GRUå‰å‘ä¼ æ’­

        Args:
            x: å½“å‰æ—¶åˆ»è¾“å…¥ (input_size,)
        """
        x = np.array(x, dtype=np.float64)
        h_prev = self.h.copy()

        # æ‹¼æ¥è¾“å…¥
        combined = np.concatenate([h_prev, x])

        # é‡ç½®é—¨
        r_t = self.sigmoid(np.dot(self.W_r, combined) + self.b_r)

        # æ›´æ–°é—¨
        z_t = self.sigmoid(np.dot(self.W_z, combined) + self.b_z)

        # å€™é€‰éšè—çŠ¶æ€ï¼ˆä½¿ç”¨é‡ç½®é—¨ï¼‰
        combined_reset = np.concatenate([r_t * h_prev, x])
        h_tilde = np.tanh(np.dot(self.W_h, combined_reset) + self.b_h)

        # æ›´æ–°éšè—çŠ¶æ€ï¼ˆä½¿ç”¨æ›´æ–°é—¨ï¼‰
        self.h = (1 - z_t) * h_prev + z_t * h_tilde

        # ä¿å­˜å‰å‘ä¼ æ’­å†å²
        self.forward_history = {
            "input": x.copy(),
            "h_prev": h_prev,
            "combined": combined,
            "r_t": r_t,
            "z_t": z_t,
            "combined_reset": combined_reset,
            "h_tilde": h_tilde,
            "output": self.h.copy(),
        }

        return self.h

    def backward(self, upstream_gradient):
        """GRUåå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„åå‘ä¼ æ’­
        dh = upstream_gradient
        h_prev = self.forward_history["h_prev"]
        h_tilde = self.forward_history["h_tilde"]
        z_t = self.forward_history["z_t"]
        r_t = self.forward_history["r_t"]

        # æ›´æ–°é—¨æ¢¯åº¦
        dz_t = dh * (h_tilde - h_prev)

        # å€™é€‰éšè—çŠ¶æ€æ¢¯åº¦
        dh_tilde = dh * z_t

        # é‡ç½®é—¨æ¢¯åº¦ï¼ˆç®€åŒ–ï¼‰
        dr_t = (
            np.dot(
                self.W_h[:, : self.hidden_size].T,
                dh_tilde
                * self.tanh_derivative(
                    np.dot(self.W_h, self.forward_history["combined_reset"]) + self.b_h
                ),
            )
            * h_prev
        )

        # æƒé‡æ¢¯åº¦
        grad_W_r = np.outer(
            dr_t
            * self.sigmoid_derivative(
                np.dot(self.W_r, self.forward_history["combined"]) + self.b_r
            ),
            self.forward_history["combined"],
        )
        grad_b_r = dr_t * self.sigmoid_derivative(
            np.dot(self.W_r, self.forward_history["combined"]) + self.b_r
        )

        grad_W_z = np.outer(
            dz_t
            * self.sigmoid_derivative(
                np.dot(self.W_z, self.forward_history["combined"]) + self.b_z
            ),
            self.forward_history["combined"],
        )
        grad_b_z = dz_t * self.sigmoid_derivative(
            np.dot(self.W_z, self.forward_history["combined"]) + self.b_z
        )

        grad_W_h = np.outer(
            dh_tilde
            * self.tanh_derivative(
                np.dot(self.W_h, self.forward_history["combined_reset"]) + self.b_h
            ),
            self.forward_history["combined_reset"],
        )
        grad_b_h = dh_tilde * self.tanh_derivative(
            np.dot(self.W_h, self.forward_history["combined_reset"]) + self.b_h
        )

        self.backward_history = {
            "grad_W_r": grad_W_r,
            "grad_b_r": grad_b_r,
            "grad_W_z": grad_W_z,
            "grad_b_z": grad_b_z,
            "grad_W_h": grad_W_h,
            "grad_b_h": grad_b_h,
        }

        return self.backward_history

    def update_parameters(self, learning_rate=0.01):
        """æ›´æ–°æ‰€æœ‰å‚æ•°"""
        self.W_r -= learning_rate * self.backward_history["grad_W_r"]
        self.b_r -= learning_rate * self.backward_history["grad_b_r"]
        self.W_z -= learning_rate * self.backward_history["grad_W_z"]
        self.b_z -= learning_rate * self.backward_history["grad_b_z"]
        self.W_h -= learning_rate * self.backward_history["grad_W_h"]
        self.b_h -= learning_rate * self.backward_history["grad_b_h"]

    def reset_state(self):
        """é‡ç½®éšè—çŠ¶æ€"""
        self.h = np.zeros(self.hidden_size)


class LSTMNeuron:
    """
    LSTMç¥ç»å…ƒ - é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ

    æ•°å­¦æ¨¡å‹ï¼š
        é—å¿˜é—¨: f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
        è¾“å…¥é—¨: i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
        å€™é€‰å€¼: CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)
        ç»†èƒçŠ¶æ€: C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t
        è¾“å‡ºé—¨: o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
        éšè—çŠ¶æ€: h_t = o_t âŠ™ tanh(C_t)
    """

    def __init__(self, input_size=3, hidden_size=3, seed=42):
        np.random.seed(seed)
        self.input_size = input_size
        self.hidden_size = hidden_size

        combined_size = hidden_size + input_size

        # åˆå§‹åŒ–4ç»„æƒé‡ï¼ˆé—å¿˜é—¨ã€è¾“å…¥é—¨ã€å€™é€‰å€¼ã€è¾“å‡ºé—¨ï¼‰
        self.W_f = np.random.randn(hidden_size, combined_size) * 0.5
        self.b_f = np.random.randn(hidden_size) * 0.1

        self.W_i = np.random.randn(hidden_size, combined_size) * 0.5
        self.b_i = np.random.randn(hidden_size) * 0.1

        self.W_C = np.random.randn(hidden_size, combined_size) * 0.5
        self.b_C = np.random.randn(hidden_size) * 0.1

        self.W_o = np.random.randn(hidden_size, combined_size) * 0.5
        self.b_o = np.random.randn(hidden_size) * 0.1

        # åˆå§‹åŒ–çŠ¶æ€
        self.h = np.zeros(hidden_size)
        self.C = np.zeros(hidden_size)

        self.forward_history = {}
        self.backward_history = {}

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

    def sigmoid_derivative(self, z):
        s = self.sigmoid(z)
        return s * (1 - s)

    def tanh_derivative(self, z):
        return 1 - np.tanh(z) ** 2

    def forward(self, x):
        """
        LSTMå‰å‘ä¼ æ’­

        Args:
            x: å½“å‰æ—¶åˆ»è¾“å…¥ (input_size,)
        """
        x = np.array(x, dtype=np.float64)
        h_prev = self.h.copy()
        C_prev = self.C.copy()

        # æ‹¼æ¥è¾“å…¥
        combined = np.concatenate([h_prev, x])

        # é—å¿˜é—¨
        f_t = self.sigmoid(np.dot(self.W_f, combined) + self.b_f)

        # è¾“å…¥é—¨
        i_t = self.sigmoid(np.dot(self.W_i, combined) + self.b_i)

        # å€™é€‰å€¼
        C_tilde = np.tanh(np.dot(self.W_C, combined) + self.b_C)

        # æ›´æ–°ç»†èƒçŠ¶æ€
        self.C = f_t * C_prev + i_t * C_tilde

        # è¾“å‡ºé—¨
        o_t = self.sigmoid(np.dot(self.W_o, combined) + self.b_o)

        # æ›´æ–°éšè—çŠ¶æ€
        self.h = o_t * np.tanh(self.C)

        # ä¿å­˜å‰å‘ä¼ æ’­å†å²
        self.forward_history = {
            "input": x.copy(),
            "h_prev": h_prev,
            "C_prev": C_prev,
            "combined": combined,
            "f_t": f_t,
            "i_t": i_t,
            "C_tilde": C_tilde,
            "C_t": self.C.copy(),
            "o_t": o_t,
            "output": self.h.copy(),
        }

        return self.h

    def backward(self, upstream_gradient):
        """LSTMåå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„åå‘ä¼ æ’­ï¼Œå®é™…LSTMçš„å®Œæ•´BPTTæ›´å¤æ‚
        dh = upstream_gradient

        # è¾“å‡ºé—¨æ¢¯åº¦
        dC = (
            dh
            * self.forward_history["o_t"]
            * self.tanh_derivative(self.forward_history["C_t"])
        )
        do_t = dh * np.tanh(self.forward_history["C_t"])

        # å€™é€‰å€¼æ¢¯åº¦
        dC_tilde = dC * self.forward_history["i_t"]
        di_t = dC * self.forward_history["C_tilde"]
        df_t = dC * self.forward_history["C_prev"]

        # æƒé‡æ¢¯åº¦
        combined = self.forward_history["combined"]

        grad_W_f = np.outer(
            df_t * self.sigmoid_derivative(np.dot(self.W_f, combined) + self.b_f),
            combined,
        )
        grad_b_f = df_t * self.sigmoid_derivative(np.dot(self.W_f, combined) + self.b_f)

        grad_W_i = np.outer(
            di_t * self.sigmoid_derivative(np.dot(self.W_i, combined) + self.b_i),
            combined,
        )
        grad_b_i = di_t * self.sigmoid_derivative(np.dot(self.W_i, combined) + self.b_i)

        grad_W_C = np.outer(
            dC_tilde * self.tanh_derivative(np.dot(self.W_C, combined) + self.b_C),
            combined,
        )
        grad_b_C = dC_tilde * self.tanh_derivative(
            np.dot(self.W_C, combined) + self.b_C
        )

        grad_W_o = np.outer(
            do_t * self.sigmoid_derivative(np.dot(self.W_o, combined) + self.b_o),
            combined,
        )
        grad_b_o = do_t * self.sigmoid_derivative(np.dot(self.W_o, combined) + self.b_o)

        self.backward_history = {
            "grad_W_f": grad_W_f,
            "grad_b_f": grad_b_f,
            "grad_W_i": grad_W_i,
            "grad_b_i": grad_b_i,
            "grad_W_C": grad_W_C,
            "grad_b_C": grad_b_C,
            "grad_W_o": grad_W_o,
            "grad_b_o": grad_b_o,
        }

        return self.backward_history

    def update_parameters(self, learning_rate=0.01):
        """æ›´æ–°æ‰€æœ‰å‚æ•°"""
        self.W_f -= learning_rate * self.backward_history["grad_W_f"]
        self.b_f -= learning_rate * self.backward_history["grad_b_f"]
        self.W_i -= learning_rate * self.backward_history["grad_W_i"]
        self.b_i -= learning_rate * self.backward_history["grad_b_i"]
        self.W_C -= learning_rate * self.backward_history["grad_W_C"]
        self.b_C -= learning_rate * self.backward_history["grad_b_C"]
        self.W_o -= learning_rate * self.backward_history["grad_W_o"]
        self.b_o -= learning_rate * self.backward_history["grad_b_o"]

    def reset_state(self):
        """é‡ç½®éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€"""
        self.h = np.zeros(self.hidden_size)
        self.C = np.zeros(self.hidden_size)


class AttentionNeuron:
    """
    æ³¨æ„åŠ›æœºåˆ¶ç¥ç»å…ƒ - ç®€åŒ–çš„å•å¤´æ³¨æ„åŠ›

    æ•°å­¦æ¨¡å‹ï¼š
        Query: Q = x Â· W_Q
        Key: K = x Â· W_K
        Value: V = x Â· W_V
        Attention Score: score = Q Â· K^T / sqrt(d_k)
        Attention Weight: Î± = softmax(score)
        Output: y = Î± Â· V
    """

    def __init__(self, input_size=3, d_model=3, seed=42):
        np.random.seed(seed)
        self.input_size = input_size
        self.d_model = d_model

        # Q, K, V æŠ•å½±çŸ©é˜µ
        self.W_Q = np.random.randn(d_model, input_size) * 0.5
        self.W_K = np.random.randn(d_model, input_size) * 0.5
        self.W_V = np.random.randn(d_model, input_size) * 0.5

        self.forward_history = {}
        self.backward_history = {}

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

    def forward(self, query, keys, values):
        """
        æ³¨æ„åŠ›å‰å‘ä¼ æ’­

        Args:
            query: æŸ¥è¯¢å‘é‡ (input_size,)
            keys: é”®å‘é‡åˆ—è¡¨ï¼Œæ¯ä¸ª (input_size,)
            values: å€¼å‘é‡åˆ—è¡¨ï¼Œæ¯ä¸ª (input_size,)
        """
        query = np.array(query, dtype=np.float64)
        keys = np.array(keys, dtype=np.float64)
        values = np.array(values, dtype=np.float64)

        # æŠ•å½±åˆ° Q, K, V
        Q = np.dot(self.W_Q, query)
        K = np.array([np.dot(self.W_K, k) for k in keys])
        V = np.array([np.dot(self.W_V, v) for v in values])

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = np.dot(K, Q) / np.sqrt(self.d_model)

        # Softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attention_weights = self.softmax(scores)

        # åŠ æƒæ±‚å’Œ
        output = np.dot(attention_weights, V)

        self.forward_history = {
            "query": query.copy(),
            "keys": keys.copy(),
            "values": values.copy(),
            "Q": Q,
            "K": K,
            "V": V,
            "scores": scores,
            "attention_weights": attention_weights,
            "output": output,
        }

        return output, attention_weights

    def backward(self, upstream_gradient):
        """æ³¨æ„åŠ›åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€åŒ–çš„æ¢¯åº¦è®¡ç®—
        grad_V = np.outer(self.forward_history["attention_weights"], upstream_gradient)
        grad_W_V = np.zeros_like(self.W_V)

        for i, v in enumerate(self.forward_history["values"]):
            grad_W_V += np.outer(grad_V[i], v)

        # å…¶ä»–æ¢¯åº¦è®¡ç®—ï¼ˆç®€åŒ–ï¼‰
        grad_W_Q = np.outer(upstream_gradient, self.forward_history["query"])
        grad_W_K = np.outer(upstream_gradient, self.forward_history["query"])

        self.backward_history = {
            "grad_W_Q": grad_W_Q,
            "grad_W_K": grad_W_K,
            "grad_W_V": grad_W_V,
        }

        return self.backward_history

    def update_parameters(self, learning_rate=0.01):
        """æ›´æ–°å‚æ•°"""
        self.W_Q -= learning_rate * self.backward_history["grad_W_Q"]
        self.W_K -= learning_rate * self.backward_history["grad_W_K"]
        self.W_V -= learning_rate * self.backward_history["grad_W_V"]


def create_computation_table(neuron, precision=4):
    """
    åˆ›å»ºå‰å‘ä¼ æ’­è®¡ç®—æ­¥éª¤è¡¨æ ¼

    Args:
        neuron: SingleNeuronå®ä¾‹
        precision: æ•°å€¼ç²¾åº¦

    Returns:
        pd.DataFrame: è®¡ç®—æ­¥éª¤è¡¨æ ¼
    """
    history = neuron.forward_history
    steps = []

    # æ­¥éª¤1: æ˜¾ç¤ºè¾“å…¥
    for i, val in enumerate(history["input"]):
        steps.append(
            {
                "æ­¥éª¤": f"è¾“å…¥ {i+1}",
                "ç¬¦å·": f"$x_{{{i}}}$",
                "æ•°å€¼": round(val, precision),
                "è¯´æ˜": f"ç¬¬{i+1}ä¸ªè¾“å…¥ç‰¹å¾",
            }
        )

    # æ­¥éª¤2: æ˜¾ç¤ºæƒé‡
    for i, w in enumerate(history["weights"]):
        steps.append(
            {
                "æ­¥éª¤": f"æƒé‡ {i+1}",
                "ç¬¦å·": f"$w_{{{i}}}$",
                "æ•°å€¼": round(w, precision),
                "è¯´æ˜": f"ç¬¬{i+1}ä¸ªæƒé‡å‚æ•°",
            }
        )

    # æ­¥éª¤3: æ˜¾ç¤ºåç½®
    steps.append(
        {
            "æ­¥éª¤": "åç½®",
            "ç¬¦å·": r"$b$",
            "æ•°å€¼": round(history["bias"], precision),
            "è¯´æ˜": "åç½®é¡¹",
        }
    )

    # æ­¥éª¤4: è®¡ç®—åŠ æƒå’Œ
    weighted_parts = [
        f"({round(w, precision)} Ã— {round(x, precision)})"
        for x, w in zip(history["input"], history["weights"])
    ]
    steps.append(
        {
            "æ­¥éª¤": "åŠ æƒå’Œ",
            "ç¬¦å·": r"$z = \sum_{i} w_i \cdot x_i + b$",
            "æ•°å€¼": round(history["weighted_sum"], precision),
            "è¯´æ˜": f'{" + ".join(weighted_parts)} + {round(history["bias"], precision)}',
        }
    )

    # æ­¥éª¤5: æ¿€æ´»å‡½æ•°
    steps.append(
        {
            "æ­¥éª¤": "æ¿€æ´»å‡½æ•°",
            "ç¬¦å·": f"$y = {neuron.activation_name}(z)$",
            "æ•°å€¼": round(history["output"], precision),
            "è¯´æ˜": f'{neuron.activation_name}({round(history["weighted_sum"], precision)}) = {round(history["output"], precision)}',
        }
    )

    return pd.DataFrame(steps)


def create_gradient_table(neuron, precision=6):
    """
    åˆ›å»ºåå‘ä¼ æ’­æ¢¯åº¦è¡¨æ ¼

    Args:
        neuron: SingleNeuronå®ä¾‹
        precision: æ•°å€¼ç²¾åº¦

    Returns:
        pd.DataFrame: æ¢¯åº¦è¡¨æ ¼
    """
    backward_hist = neuron.backward_history
    forward_hist = neuron.forward_history
    gradients = []

    # ä¸Šæ¸¸æ¢¯åº¦
    gradients.append(
        {
            "æ¢¯åº¦ç±»å‹": "ä¸Šæ¸¸æ¢¯åº¦",
            "ç¬¦å·": r"$\frac{\partial L}{\partial y}$",
            "æ•°å€¼": round(backward_hist["upstream_gradient"], precision),
            "è¯´æ˜": "æ¥è‡ªæŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼ˆå‡è®¾å€¼ï¼‰",
        }
    )

    # æ¿€æ´»å‡½æ•°å¯¼æ•°
    gradients.append(
        {
            "æ¢¯åº¦ç±»å‹": "æ¿€æ´»å‡½æ•°å¯¼æ•°",
            "ç¬¦å·": r"$\frac{\partial y}{\partial z}$",
            "æ•°å€¼": round(forward_hist["activation_derivative"], precision),
            "è¯´æ˜": f"{neuron.activation_name}'({round(forward_hist['weighted_sum'], precision)})",
        }
    )

    # å±€éƒ¨æ¢¯åº¦
    gradients.append(
        {
            "æ¢¯åº¦ç±»å‹": "å±€éƒ¨æ¢¯åº¦",
            "ç¬¦å·": r"$\frac{\partial L}{\partial z}$",
            "æ•°å€¼": round(backward_hist["local_gradient"], precision),
            "è¯´æ˜": f"= {round(backward_hist['upstream_gradient'], precision)} Ã— {round(forward_hist['activation_derivative'], precision)}",
        }
    )

    # æƒé‡æ¢¯åº¦
    for i, grad_w in enumerate(backward_hist["grad_weights"]):
        gradients.append(
            {
                "æ¢¯åº¦ç±»å‹": f"æƒé‡æ¢¯åº¦ {i+1}",
                "ç¬¦å·": rf"$\frac{{\partial L}}{{\partial w_{{{i}}}}}$",
                "æ•°å€¼": round(grad_w, precision),
                "è¯´æ˜": f"= {round(backward_hist['local_gradient'], precision)} Ã— {round(forward_hist['input'][i], precision)}",
            }
        )

    # åç½®æ¢¯åº¦
    gradients.append(
        {
            "æ¢¯åº¦ç±»å‹": "åç½®æ¢¯åº¦",
            "ç¬¦å·": r"$\frac{\partial L}{\partial b}$",
            "æ•°å€¼": round(backward_hist["grad_bias"], precision),
            "è¯´æ˜": f"= {round(backward_hist['local_gradient'], precision)} Ã— 1",
        }
    )

    return pd.DataFrame(gradients)


def visualize_forward_pass(neuron):
    """
    å¯è§†åŒ–å‰å‘ä¼ æ’­è¿‡ç¨‹

    Args:
        neuron: SingleNeuronå®ä¾‹

    Returns:
        plotly Figureå¯¹è±¡
    """
    history = neuron.forward_history

    fig = make_subplots(
        rows=2,
        cols=2,
        subplot_titles=("è¾“å…¥ä¸æƒé‡", "åŠ æƒå’Œè®¡ç®—", "æ¿€æ´»å‡½æ•°æ›²çº¿", "è®¡ç®—æµç¨‹"),
        specs=[
            [{"type": "bar"}, {"type": "indicator"}],
            [{"type": "scatter"}, {"type": "bar"}],
        ],
    )

    # å­å›¾1: è¾“å…¥ä¸æƒé‡å¯¹æ¯”
    x_labels = [f"x[{i}]" for i in range(len(history["input"]))]
    fig.add_trace(
        go.Bar(name="è¾“å…¥å€¼", x=x_labels, y=history["input"], marker_color="lightblue"),
        row=1,
        col=1,
    )
    fig.add_trace(
        go.Bar(
            name="æƒé‡å€¼", x=x_labels, y=history["weights"], marker_color="lightcoral"
        ),
        row=1,
        col=1,
    )

    # å­å›¾2: åŠ æƒå’ŒæŒ‡ç¤ºå™¨
    fig.add_trace(
        go.Indicator(
            mode="number+delta",
            value=history["weighted_sum"],
            title={"text": "åŠ æƒå’Œ z"},
            delta={"reference": 0},
        ),
        row=1,
        col=2,
    )

    # å­å›¾3: æ¿€æ´»å‡½æ•°æ›²çº¿
    z_range = np.linspace(-3, 3, 100)
    if neuron.activation_name == "relu":
        y_range = np.maximum(0, z_range)
    elif neuron.activation_name == "sigmoid":
        y_range = 1 / (1 + np.exp(-z_range))
    elif neuron.activation_name == "tanh":
        y_range = np.tanh(z_range)
    else:
        y_range = z_range

    fig.add_trace(
        go.Scatter(
            x=z_range,
            y=y_range,
            mode="lines",
            name=neuron.activation_name,
            line=dict(color="blue"),
        ),
        row=2,
        col=1,
    )
    fig.add_trace(
        go.Scatter(
            x=[history["weighted_sum"]],
            y=[history["output"]],
            mode="markers",
            name="å½“å‰ç‚¹",
            marker=dict(size=12, color="red"),
        ),
        row=2,
        col=1,
    )

    # å­å›¾4: åŠ æƒä¹˜ç§¯åˆ†è§£
    products = history["input"] * history["weights"]
    x_labels_prod = [f"w[{i}]Ã—x[{i}]" for i in range(len(products))]
    fig.add_trace(
        go.Bar(x=x_labels_prod, y=products, marker_color="lightgreen", name="wÃ—x"),
        row=2,
        col=2,
    )

    fig.update_layout(height=700, showlegend=True, title_text="å•ç¥ç»å…ƒå‰å‘ä¼ æ’­å¯è§†åŒ–")

    return fig


def visualize_backward_pass(neuron):
    """
    å¯è§†åŒ–åå‘ä¼ æ’­è¿‡ç¨‹

    Args:
        neuron: SingleNeuronå®ä¾‹

    Returns:
        plotly Figureå¯¹è±¡
    """
    backward_hist = neuron.backward_history
    forward_hist = neuron.forward_history

    fig = make_subplots(
        rows=1,
        cols=2,
        subplot_titles=("æ¢¯åº¦æµåŠ¨", "å‚æ•°æ¢¯åº¦åˆ†å¸ƒ"),
        specs=[[{"type": "bar"}, {"type": "bar"}]],
    )

    # å­å›¾1: æ¢¯åº¦æµåŠ¨ï¼ˆä»è¾“å‡ºåˆ°è¾“å…¥ï¼‰
    gradient_flow = [
        backward_hist["upstream_gradient"],
        backward_hist["local_gradient"],
        np.mean(np.abs(backward_hist["grad_weights"])),
    ]
    gradient_labels = ["ä¸Šæ¸¸æ¢¯åº¦\nâˆ‚L/âˆ‚y", "å±€éƒ¨æ¢¯åº¦\nâˆ‚L/âˆ‚z", "æƒé‡æ¢¯åº¦\nâˆ‚L/âˆ‚w"]

    fig.add_trace(
        go.Bar(
            x=gradient_labels, y=gradient_flow, marker_color=["red", "orange", "yellow"]
        ),
        row=1,
        col=1,
    )

    # å­å›¾2: å„å‚æ•°æ¢¯åº¦å¤§å°
    param_labels = [f"w[{i}]" for i in range(len(backward_hist["grad_weights"]))] + [
        "b"
    ]
    param_grads = list(backward_hist["grad_weights"]) + [backward_hist["grad_bias"]]

    fig.add_trace(
        go.Bar(x=param_labels, y=param_grads, marker_color="lightblue"), row=1, col=2
    )

    fig.update_layout(height=400, showlegend=False, title_text="å•ç¥ç»å…ƒåå‘ä¼ æ’­å¯è§†åŒ–")

    return fig


def render_dense_neuron():
    """æ¸²æŸ“å…¨è¿æ¥ç¥ç»å…ƒæ¼”ç¤º"""
    st.subheader("ğŸ“ å…¨è¿æ¥ç¥ç»å…ƒï¼ˆDense/Fully Connectedï¼‰")

    st.markdown(
        """
    **å·¥ä½œåŸç†**ï¼š
    1. æ¥æ”¶å¤šä¸ªè¾“å…¥ä¿¡å·
    2. å¯¹æ¯ä¸ªè¾“å…¥åŠ æƒæ±‚å’Œï¼ˆåŠ ä¸Šåç½®ï¼‰
    3. é€šè¿‡æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§
    4. è¾“å‡ºå¤„ç†åçš„ä¿¡å·
    
    **æ•°å­¦è¡¨è¾¾**ï¼š
    $$
    \\begin{aligned}
    z &= \\sum_{i=0}^{n} w_i \\cdot x_i + b = w^T x + b \\\\
    y &= \\text{activation}(z)
    \\end{aligned}
    $$
    """
    )

    st.markdown("---")
    st.subheader("âš™ï¸ é…ç½®ç¥ç»å…ƒ")

    col1, col2 = st.columns(2)

    with col1:
        input_size = st.slider("è¾“å…¥ç»´åº¦", 1, 5, 3, help="ç¥ç»å…ƒæ¥æ”¶å¤šå°‘ä¸ªè¾“å…¥")
        activation = st.selectbox(
            "æ¿€æ´»å‡½æ•°", ["relu", "sigmoid", "tanh"], help="é€‰æ‹©æ¿€æ´»å‡½æ•°ç±»å‹"
        )

    with col2:
        seed = st.number_input("éšæœºç§å­", 0, 100, 42, help="ç”¨äºåˆå§‹åŒ–æƒé‡")
        learning_rate = st.slider(
            "å­¦ä¹ ç‡", 0.001, 0.5, 0.01, 0.001, help="æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿"
        )

    # åˆ›å»ºç¥ç»å…ƒ
    neuron = SingleNeuron(input_size=input_size, activation=activation, seed=seed)

    st.markdown("---")

    # è¾“å…¥æ•°æ®
    st.subheader("ğŸ“¥ è¾“å…¥æ•°æ®")

    st.write("è®¾ç½®è¾“å…¥å€¼ï¼š")
    input_cols = st.columns(input_size)
    input_data = []
    for i, col in enumerate(input_cols):
        with col:
            val = st.number_input(
                f"$x_{{{i}}}$",
                value=float(np.random.randn() * 0.5),
                format="%.4f",
                key=f"dense_input_{i}",
            )
            input_data.append(val)

    input_data = np.array(input_data)

    # æ˜¾ç¤ºå½“å‰å‚æ•°
    st.subheader("ğŸ¯ å½“å‰å‚æ•°")
    param_col1, param_col2 = st.columns(2)

    with param_col1:
        st.write("**æƒé‡å‘é‡ w:**")
        weights_df = pd.DataFrame(
            {
                "ç´¢å¼•": [f"w[{i}]" for i in range(input_size)],
                "æ•°å€¼": [f"{w:.6f}" for w in neuron.weights],
            }
        )
        st.markdown(weights_df.to_markdown(index=False))

    with param_col2:
        st.write("**åç½® b:**")
        st.metric("bias", f"{neuron.bias:.6f}")

    st.markdown("---")

    # ä¸Šæ¸¸æ¢¯åº¦è®¾ç½®
    st.subheader("ğŸ¯ ä¸Šæ¸¸æ¢¯åº¦è®¾ç½®")
    upstream_grad = st.number_input(
        "ä¸Šæ¸¸æ¢¯åº¦ (âˆ‚L/âˆ‚y)",
        value=1.0,
        format="%.6f",
        key="dense_upstream_grad",
        help="å‡è®¾è¿™æ˜¯ä»æŸå¤±å‡½æ•°ä¼ å›çš„æ¢¯åº¦ã€‚åœ¨çœŸå®è®­ç»ƒä¸­ï¼Œè¿™æ¥è‡ªæŸå¤±å‡½æ•°å¯¹è¾“å‡ºçš„å¯¼æ•°ã€‚",
    )

    st.markdown("---")

    # æ‰§è¡Œå®Œæ•´çš„å‰å‘-åå‘-æ›´æ–°æµç¨‹
    if st.button(
        "ğŸš€ æ‰§è¡Œå®Œæ•´è®¡ç®—æµç¨‹ï¼ˆå‰å‘â†’åå‘â†’æ›´æ–°ï¼‰", type="primary", key="dense_compute"
    ):
        # ==================== å‰å‘ä¼ æ’­ ====================
        st.subheader("â¡ï¸ 1. å‰å‘ä¼ æ’­")
        output = neuron.forward(input_data)

        st.success(f"âœ… ç¥ç»å…ƒè¾“å‡º: **{output:.6f}**")

        # è®¡ç®—æ­¥éª¤è¡¨æ ¼
        with st.expander("ğŸ“‹ è¯¦ç»†è®¡ç®—æ­¥éª¤", expanded=True):
            comp_table = create_computation_table(neuron)
            st.markdown(comp_table.to_markdown(index=False))

        # å¯è§†åŒ–
        with st.expander("ğŸ“Š å‰å‘ä¼ æ’­å¯è§†åŒ–", expanded=True):
            fig_forward = visualize_forward_pass(neuron)
            st.plotly_chart(fig_forward, use_container_width=True)

        # ==================== åå‘ä¼ æ’­ ====================
        st.markdown("---")
        st.subheader("â¬…ï¸ 2. åå‘ä¼ æ’­")

        # ä¿å­˜æ—§å‚æ•°ç”¨äºå¯¹æ¯”
        old_weights = neuron.weights.copy()
        old_bias = neuron.bias

        gradients = neuron.backward(upstream_grad)

        st.success("âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ")

        # æ¢¯åº¦è¡¨æ ¼
        with st.expander("ğŸ“‹ æ¢¯åº¦è¯¦ç»†ä¿¡æ¯", expanded=True):
            grad_table = create_gradient_table(neuron)
            st.markdown(grad_table.to_markdown(index=False))

        # å¯è§†åŒ–
        with st.expander("ğŸ“Š æ¢¯åº¦æµåŠ¨å¯è§†åŒ–", expanded=True):
            fig_backward = visualize_backward_pass(neuron)
            st.plotly_chart(fig_backward, use_container_width=True)

        # ==================== æ•°å€¼ç¨³å®šæ€§æ£€æµ‹ ====================
        st.markdown("---")
        st.subheader("ğŸ”¬ æ•°å€¼ç¨³å®šæ€§è¯Šæ–­")

        st.info('ğŸ’¡ æ ¹æ®é¡¹ç›®å®šä½ï¼Œæˆ‘ä»¬ä¸ä»…å±•ç¤º"ç®—äº†ä»€ä¹ˆ"ï¼Œæ›´è¦æ£€æµ‹"ä»€ä¹ˆæ—¶å€™ä¼šå‡ºé—®é¢˜"')

        # æ”¶é›†æ‰€æœ‰æ£€æµ‹ç»“æœ
        stability_issues = []

        # 1. æ£€æŸ¥æ¢¯åº¦
        grad_check = StabilityChecker.check_gradient(gradients["weights"], "æƒé‡æ¢¯åº¦")
        stability_issues.append(grad_check)

        bias_grad_check = StabilityChecker.check_gradient(
            np.array([gradients["bias"]]), "åç½®æ¢¯åº¦"
        )
        stability_issues.append(bias_grad_check)

        # 2. æ£€æŸ¥æ¿€æ´»å€¼
        act_check = StabilityChecker.check_activation(
            np.array([neuron.forward_history["output"]]), "è¾“å‡ºæ¿€æ´»å€¼"
        )
        stability_issues.append(act_check)

        # 3. éªŒè¯æ¢¯åº¦æ­£ç¡®æ€§ï¼ˆæ•°å€¼æ¢¯åº¦ vs è§£ææ¢¯åº¦ï¼‰
        with st.spinner("æ­£åœ¨è®¡ç®—æ•°å€¼æ¢¯åº¦è¿›è¡ŒéªŒè¯..."):
            try:
                numerical_grad = compute_numerical_gradient(
                    neuron, input_data, upstream_grad, epsilon=1e-5
                )
                grad_verify = StabilityChecker.verify_gradient(
                    numerical_grad, gradients["weights"], "æƒé‡æ¢¯åº¦"
                )
                stability_issues.append(grad_verify)
            except Exception as e:
                st.warning(f"âš ï¸ æ•°å€¼æ¢¯åº¦è®¡ç®—å¤±è´¥: {str(e)}")

        # 4. æ£€æŸ¥å­¦ä¹ ç‡
        param_norm = np.linalg.norm(neuron.weights)
        grad_norm = np.linalg.norm(gradients["weights"])
        lr_check = StabilityChecker.check_learning_rate(
            learning_rate, grad_norm, param_norm
        )
        stability_issues.append(lr_check)

        # æ˜¾ç¤ºè¯Šæ–­ç»“æœ
        StabilityChecker.display_issues(stability_issues)

        # ==================== å‚æ•°æ›´æ–° ====================
        st.markdown("---")
        st.subheader("ğŸ“Š 3. å‚æ•°æ›´æ–°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰")

        # æ›´æ–°å‚æ•°
        neuron.update_parameters(learning_rate)

        st.success("âœ… å‚æ•°å·²æ›´æ–°")

        # æ˜¾ç¤ºæ›´æ–°è¯¦æƒ…
        st.write("**å‚æ•°å˜åŒ–å¯¹æ¯”ï¼š**")

        update_data = []
        for i in range(input_size):
            delta = neuron.weights[i] - old_weights[i]
            update_data.append(
                {
                    "å‚æ•°": f"$w_{{{i}}}$",
                    "æ›´æ–°å‰": f"{old_weights[i]:.6f}",
                    "æ¢¯åº¦": f'{gradients["weights"][i]:.6f}',
                    "æ›´æ–°é‡": f'{-learning_rate * gradients["weights"][i]:.6f}',
                    "æ›´æ–°å": f"{neuron.weights[i]:.6f}",
                    "å˜åŒ–": f"{delta:.6f}",
                }
            )

        delta_bias = neuron.bias - old_bias
        update_data.append(
            {
                "å‚æ•°": "$b$",
                "æ›´æ–°å‰": f"{old_bias:.6f}",
                "æ¢¯åº¦": f'{gradients["bias"]:.6f}',
                "æ›´æ–°é‡": f'{-learning_rate * gradients["bias"]:.6f}',
                "æ›´æ–°å": f"{neuron.bias:.6f}",
                "å˜åŒ–": f"{delta_bias:.6f}",
            }
        )

        update_df = pd.DataFrame(update_data)
        st.markdown(update_df.to_markdown(index=False))

        st.info(
            f"ğŸ’¡ **æ›´æ–°è§„åˆ™**: $\\theta_{{\text{{new}}}} = \\theta_{{\text{{old}}}} - \\alpha \\cdot \\frac{{\\partial L}}{{\\partial \\theta}}$ï¼Œå…¶ä¸­å­¦ä¹ ç‡ $\\alpha = {learning_rate}$"
        )

        # ==================== æ€»ç»“ ====================
        st.markdown("---")
        st.subheader("ğŸ“ˆ å®Œæ•´æµç¨‹æ€»ç»“")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("å‰å‘ä¼ æ’­è¾“å‡º", f"{output:.6f}", help="ç¥ç»å…ƒçš„æœ€ç»ˆè¾“å‡º")

        with col2:
            avg_grad = np.mean(np.abs(gradients["weights"]))
            st.metric("å¹³å‡æƒé‡æ¢¯åº¦", f"{avg_grad:.6f}", help="æƒé‡æ¢¯åº¦çš„å¹³å‡ç»å¯¹å€¼")

        with col3:
            total_change = np.linalg.norm(neuron.weights - old_weights)
            st.metric("å‚æ•°å˜åŒ–é‡", f"{total_change:.6f}", help="æ‰€æœ‰æƒé‡å˜åŒ–çš„L2èŒƒæ•°")

        st.success(
            """
        âœ… **å®Œæ•´è®­ç»ƒæ­¥éª¤å·²å®Œæˆï¼**
        
        è¿™å°±æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒçš„ä¸€ä¸ªå®Œæ•´è¿­ä»£ï¼š
        1. **å‰å‘ä¼ æ’­**ï¼šè¾“å…¥ â†’ åŠ æƒå’Œ â†’ æ¿€æ´» â†’ è¾“å‡º
        2. **åå‘ä¼ æ’­**ï¼šè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
        3. **å‚æ•°æ›´æ–°**ï¼šæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°
        
        åœ¨å®é™…è®­ç»ƒä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤æˆåƒä¸Šä¸‡æ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ ·æœ¬ã€‚
        """
        )


def render_conv_neuron():
    """æ¸²æŸ“å·ç§¯ç¥ç»å…ƒæ¼”ç¤º"""
    st.subheader("ğŸ–¼ï¸ å·ç§¯ç¥ç»å…ƒï¼ˆConvolutional Neuronï¼‰")

    st.markdown(
        """
    **å·¥ä½œåŸç†**ï¼š
    1. ä½¿ç”¨å°çš„å·ç§¯æ ¸æ‰«æè¾“å…¥å›¾åƒ
    2. å±€éƒ¨è¿æ¥ï¼šåªå…³æ³¨è¾“å…¥çš„ä¸€å°å—åŒºåŸŸ
    3. æƒå€¼å…±äº«ï¼šåŒä¸€ä¸ªå·ç§¯æ ¸åœ¨æ•´ä¸ªè¾“å…¥ä¸Šå¤ç”¨
    4. æå–å±€éƒ¨ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ç­‰ï¼‰
    
    **æ•°å­¦è¡¨è¾¾**ï¼š
    $$
    y = \\text{activation}\\left(\\sum_{i,j} K[i,j] \\cdot X[i,j] + b\\right)
    $$
    å…¶ä¸­ $K$ æ˜¯å·ç§¯æ ¸ï¼Œ$X$ æ˜¯è¾“å…¥å›¾åƒå—
    """
    )

    st.markdown("---")
    st.subheader("âš™ï¸ é…ç½®å·ç§¯ç¥ç»å…ƒ")

    col1, col2 = st.columns(2)

    with col1:
        kernel_size = st.slider(
            "å·ç§¯æ ¸å¤§å°", 2, 5, 3, help="å·ç§¯æ ¸çš„å°ºå¯¸ (kernel_size Ã— kernel_size)"
        )
        activation = st.selectbox(
            "æ¿€æ´»å‡½æ•°",
            ["relu", "sigmoid", "tanh"],
            help="é€‰æ‹©æ¿€æ´»å‡½æ•°ç±»å‹",
            key="conv_activation",
        )

    with col2:
        seed = st.number_input(
            "éšæœºç§å­", 0, 100, 42, help="ç”¨äºåˆå§‹åŒ–å·ç§¯æ ¸", key="conv_seed"
        )
        learning_rate = st.slider(
            "å­¦ä¹ ç‡", 0.001, 0.5, 0.01, 0.001, help="æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿", key="conv_lr"
        )

    # åˆ›å»ºå·ç§¯ç¥ç»å…ƒ
    neuron = ConvNeuron(kernel_size=kernel_size, activation=activation, seed=seed)

    st.markdown("---")

    # è¾“å…¥æ•°æ®ï¼ˆå›¾åƒå—ï¼‰
    st.subheader("ğŸ“¥ è¾“å…¥å›¾åƒå—")
    st.write(f"è®¾ç½®è¾“å…¥å›¾åƒå—ï¼ˆ{kernel_size}Ã—{kernel_size}ï¼‰ï¼š")

    # åˆ›å»ºè¾“å…¥çŸ©é˜µ
    input_data = np.random.randn(kernel_size, kernel_size) * 0.5

    # ä½¿ç”¨åˆ—æ˜¾ç¤ºè¾“å…¥
    for i in range(kernel_size):
        cols = st.columns(kernel_size)
        for j in range(kernel_size):
            with cols[j]:
                input_data[i, j] = st.number_input(
                    f"$X[{i},{j}]$",
                    value=float(input_data[i, j]),
                    format="%.4f",
                    key=f"conv_input_{i}_{j}",
                )

    # æ˜¾ç¤ºå·ç§¯æ ¸
    st.subheader("ğŸ¯ å·ç§¯æ ¸ï¼ˆæƒé‡ï¼‰")
    st.write("**å·ç§¯æ ¸ K:**")

    kernel_display = []
    for i in range(kernel_size):
        for j in range(kernel_size):
            kernel_display.append(
                {"ä½ç½®": f"K[{i},{j}]", "æ•°å€¼": f"{neuron.kernel[i, j]:.6f}"}
            )

    kernel_df = pd.DataFrame(kernel_display)
    st.markdown(kernel_df.to_markdown(index=False))

    st.write(f"**åç½® b:** {neuron.bias:.6f}")

    st.markdown("---")

    # ä¸Šæ¸¸æ¢¯åº¦
    upstream_grad = st.number_input(
        "ä¸Šæ¸¸æ¢¯åº¦ (âˆ‚L/âˆ‚y)",
        value=1.0,
        format="%.6f",
        key="conv_upstream_grad",
        help="æ¥è‡ªæŸå¤±å‡½æ•°çš„æ¢¯åº¦",
    )

    st.markdown("---")

    # æ‰§è¡Œè®¡ç®—
    if st.button("ğŸš€ æ‰§è¡Œå·ç§¯è®¡ç®—", type="primary", key="conv_compute"):
        # å‰å‘ä¼ æ’­
        st.subheader("â¡ï¸ 1. å‰å‘ä¼ æ’­")
        output = neuron.forward(input_data)

        st.success(f"âœ… å·ç§¯è¾“å‡º: **{output:.6f}**")

        with st.expander("ğŸ“‹ è¯¦ç»†è®¡ç®—æ­¥éª¤", expanded=True):
            # æ˜¾ç¤ºé€å…ƒç´ ä¹˜ç§¯
            st.write("**é€å…ƒç´ ç›¸ä¹˜ï¼š**")
            element_wise = neuron.kernel * input_data

            steps = []
            for i in range(kernel_size):
                for j in range(kernel_size):
                    steps.append(
                        {
                            "ä½ç½®": f"[{i},{j}]",
                            "å·ç§¯æ ¸": f"{neuron.kernel[i, j]:.4f}",
                            "è¾“å…¥": f"{input_data[i, j]:.4f}",
                            "ä¹˜ç§¯": f"{element_wise[i, j]:.4f}",
                        }
                    )

            steps_df = pd.DataFrame(steps)
            st.markdown(steps_df.to_markdown(index=False))

            st.write(f"**æ±‚å’Œ:** {np.sum(element_wise):.6f}")
            st.write(
                f"**åŠ åç½®:** {np.sum(element_wise):.6f} + {neuron.bias:.6f} = {neuron.forward_history['weighted_sum']:.6f}"
            )
            st.write(f"**æ¿€æ´»å‡½æ•° ({activation}):** {output:.6f}")

        # å¯è§†åŒ–
        with st.expander("ğŸ“Š å·ç§¯å¯è§†åŒ–", expanded=True):
            fig = make_subplots(
                rows=1,
                cols=3,
                subplot_titles=("è¾“å…¥å›¾åƒå—", "å·ç§¯æ ¸", "é€å…ƒç´ ä¹˜ç§¯"),
                specs=[[{"type": "heatmap"}, {"type": "heatmap"}, {"type": "heatmap"}]],
            )

            fig.add_trace(
                go.Heatmap(z=input_data, colorscale="Viridis", name="è¾“å…¥"),
                row=1,
                col=1,
            )
            fig.add_trace(
                go.Heatmap(z=neuron.kernel, colorscale="RdBu", name="å·ç§¯æ ¸"),
                row=1,
                col=2,
            )
            fig.add_trace(
                go.Heatmap(
                    z=neuron.kernel * input_data, colorscale="Greens", name="ä¹˜ç§¯"
                ),
                row=1,
                col=3,
            )

            fig.update_layout(height=400, showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

        # åå‘ä¼ æ’­
        st.markdown("---")
        st.subheader("â¬…ï¸ 2. åå‘ä¼ æ’­")

        old_kernel = neuron.kernel.copy()
        old_bias = neuron.bias

        gradients = neuron.backward(upstream_grad)

        st.success("âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ")

        with st.expander("ğŸ“‹ æ¢¯åº¦ä¿¡æ¯", expanded=True):
            grad_steps = []
            for i in range(kernel_size):
                for j in range(kernel_size):
                    grad_steps.append(
                        {
                            "ä½ç½®": f"âˆ‚L/âˆ‚K[{i},{j}]",
                            "æ¢¯åº¦": f'{gradients["kernel"][i, j]:.6f}',
                            "è®¡ç®—": f'{upstream_grad:.6f} Ã— {neuron.forward_history["activation_derivative"]:.6f} Ã— {input_data[i, j]:.6f}',
                        }
                    )

            grad_steps.append(
                {
                    "ä½ç½®": "âˆ‚L/âˆ‚b",
                    "æ¢¯åº¦": f'{gradients["bias"]:.6f}',
                    "è®¡ç®—": f'{upstream_grad:.6f} Ã— {neuron.forward_history["activation_derivative"]:.6f}',
                }
            )

            grad_df = pd.DataFrame(grad_steps)
            st.markdown(grad_df.to_markdown(index=False))

        # å‚æ•°æ›´æ–°
        st.markdown("---")
        st.subheader("ğŸ“Š 3. å‚æ•°æ›´æ–°")

        neuron.update_parameters(learning_rate)

        st.success("âœ… å‚æ•°å·²æ›´æ–°")

        st.write("**å·ç§¯æ ¸å˜åŒ–ï¼š**")
        kernel_change = neuron.kernel - old_kernel
        st.write(f"æœ€å¤§å˜åŒ–: {np.max(np.abs(kernel_change)):.6f}")
        st.write(f"å¹³å‡å˜åŒ–: {np.mean(np.abs(kernel_change)):.6f}")

        # ==================== æ•°å€¼ç¨³å®šæ€§æ£€æµ‹ ====================
        st.markdown("---")
        st.subheader("ğŸ”¬ æ•°å€¼ç¨³å®šæ€§è¯Šæ–­")

        st.info("ğŸ’¡ å·ç§¯ç‰¹æœ‰é—®é¢˜ï¼šå·ç§¯æ ¸æ¢¯åº¦ã€ç‰¹å¾å›¾çˆ†ç‚¸ã€æ„Ÿå—é‡è¿‡å°")

        stability_issues = []

        # 1. æ£€æŸ¥å·ç§¯æ ¸æ¢¯åº¦
        grad_kernel_check = StabilityChecker.check_gradient(
            gradients["kernel"].flatten(), "å·ç§¯æ ¸æ¢¯åº¦"
        )
        stability_issues.append(grad_kernel_check)

        grad_bias_check = StabilityChecker.check_gradient(
            np.array([gradients["bias"]]), "åç½®æ¢¯åº¦"
        )
        stability_issues.append(grad_bias_check)

        # 2. æ£€æŸ¥è¾“å…¥å’Œè¾“å‡ºï¼ˆç‰¹å¾å›¾ï¼‰
        input_check = StabilityChecker.check_activation(x_conv.flatten(), "è¾“å…¥å›¾åƒå—")
        stability_issues.append(input_check)

        output_check = StabilityChecker.check_activation(np.array([output]), "å·ç§¯è¾“å‡º")
        stability_issues.append(output_check)

        # 3. æ£€æŸ¥å·ç§¯æ ¸çš„èŒƒæ•°
        kernel_norm = np.linalg.norm(neuron.kernel)
        if kernel_norm > 10:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "å·ç§¯æ ¸èŒƒæ•°è¿‡å¤§",
                    "value": f"{kernel_norm:.4f}",
                    "threshold": "> 10",
                    "icon": "ğŸŸ¡",
                    "severity": "medium",
                    "details": {
                        "å·ç§¯æ ¸èŒƒæ•°": f"{kernel_norm:.4f}",
                        "å·ç§¯æ ¸å½¢çŠ¶": f"{neuron.kernel.shape}",
                        "æœ€å¤§æƒé‡": f"{np.max(np.abs(neuron.kernel)):.4f}",
                    },
                    "solution": [
                        "ä½¿ç”¨Xavier/Heåˆå§‹åŒ–",
                        "æ·»åŠ æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰",
                        "ä½¿ç”¨BatchNorm/LayerNorm",
                        "é™ä½å­¦ä¹ ç‡",
                    ],
                    "explanation": "å·ç§¯æ ¸æƒé‡è¿‡å¤§ä¼šå¯¼è‡´è¾“å‡ºçˆ†ç‚¸ï¼Œå°¤å…¶åœ¨æ·±å±‚ç½‘ç»œä¸­ç´¯ç§¯",
                }
            )
        else:
            stability_issues.append(
                {
                    "status": "success",
                    "type": "å·ç§¯æ ¸èŒƒæ•°",
                    "value": f"{kernel_norm:.4f}",
                    "icon": "ğŸŸ¢",
                    "severity": "none",
                    "details": {
                        "å·ç§¯æ ¸èŒƒæ•°": f"{kernel_norm:.4f}",
                        "å·ç§¯æ ¸å½¢çŠ¶": f"{neuron.kernel.shape}",
                    },
                }
            )

        # 4. æ£€æŸ¥å·ç§¯æ“ä½œçš„æ•°å€¼èŒƒå›´
        element_wise_product = neuron.kernel * x_conv
        product_max = np.max(np.abs(element_wise_product))

        if product_max > 50:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "é€å…ƒç´ ä¹˜ç§¯è¿‡å¤§",
                    "value": f"{product_max:.2f}",
                    "threshold": "> 50",
                    "icon": "ğŸŸ¡",
                    "severity": "medium",
                    "details": {
                        "æœ€å¤§ä¹˜ç§¯": f"{product_max:.2f}",
                        "æ±‚å’Œç»“æœ": f"{np.sum(element_wise_product):.2f}",
                        "åŠ åç½®å": f"{np.sum(element_wise_product) + neuron.bias:.2f}",
                    },
                    "solution": [
                        "å½’ä¸€åŒ–è¾“å…¥ï¼ˆå¦‚é™¤ä»¥255ï¼‰",
                        "ä½¿ç”¨BatchNorm",
                        "å‡å°å·ç§¯æ ¸æƒé‡ï¼ˆXavieråˆå§‹åŒ–ï¼‰",
                    ],
                    "explanation": "è¾“å…¥ä¸å·ç§¯æ ¸çš„ä¹˜ç§¯è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´åç»­æ¿€æ´»å‡½æ•°é¥±å’Œ",
                }
            )

        # 5. æ£€æŸ¥å­¦ä¹ ç‡
        param_norm = np.sqrt(np.linalg.norm(neuron.kernel) ** 2 + neuron.bias**2)
        grad_norm = np.sqrt(
            np.linalg.norm(gradients["kernel"]) ** 2 + gradients["bias"] ** 2
        )
        lr_check = StabilityChecker.check_learning_rate(
            learning_rate, grad_norm, param_norm
        )
        stability_issues.append(lr_check)

        # 6. æ„Ÿå—é‡åˆ†æ
        receptive_field_size = kernel_size * kernel_size
        if kernel_size < 3:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "æ„Ÿå—é‡è¿‡å°",
                    "value": f"{kernel_size}Ã—{kernel_size}",
                    "threshold": "< 3Ã—3",
                    "icon": "ğŸŸ¡",
                    "severity": "low",
                    "details": {
                        "å·ç§¯æ ¸å¤§å°": f"{kernel_size}Ã—{kernel_size}",
                        "æ„Ÿå—é‡": f"{receptive_field_size}ä¸ªåƒç´ ",
                        "å‚æ•°é‡": f"{kernel_size*kernel_size + 1}",
                    },
                    "solution": [
                        "ä½¿ç”¨3Ã—3æˆ–æ›´å¤§çš„å·ç§¯æ ¸",
                        "å †å å¤šä¸ªå°å·ç§¯æ ¸",
                        "ä½¿ç”¨ç©ºæ´å·ç§¯å¢å¤§æ„Ÿå—é‡",
                    ],
                    "explanation": "1Ã—1å·ç§¯æ— ç©ºé—´ä¿¡æ¯ï¼Œ2Ã—2å·ç§¯æ„Ÿå—é‡è¾ƒå°ï¼Œ3Ã—3æ˜¯å¸¸ç”¨é€‰æ‹©",
                }
            )

        # æ˜¾ç¤ºè¯Šæ–­ç»“æœ
        StabilityChecker.display_issues(
            stability_issues, title="ğŸ”¬ å·ç§¯ç¥ç»å…ƒç¨³å®šæ€§è¯Šæ–­"
        )

        st.markdown("---")
        st.info(
            f"""
        ğŸ’¡ **å·ç§¯ç¥ç»å…ƒç‰¹æ€§ä¸ç¨³å®šæ€§**ï¼š
        
        **å±€éƒ¨è¿æ¥**: åªå¤„ç† {kernel_size}Ã—{kernel_size} çš„è¾“å…¥åŒºåŸŸ
        - æ„Ÿå—é‡: {kernel_size*kernel_size} ä¸ªåƒç´ 
        
        **æƒå€¼å…±äº«**: åŒä¸€ä¸ªå·ç§¯æ ¸å¯ä»¥æ‰«ææ•´ä¸ªå›¾åƒ
        - å·ç§¯æ ¸èŒƒæ•°: {kernel_norm:.4f}
        - å‚æ•°æ•ˆç‡: ç›¸åŒå‚æ•°å¤„ç†æ•´ä¸ªå›¾åƒ
        
        **å¹³ç§»ä¸å˜æ€§**: æ— è®ºç‰¹å¾åœ¨å“ªé‡Œï¼Œéƒ½èƒ½è¢«æ£€æµ‹åˆ°
        - è¾“å‡ºèŒƒå›´: [{np.min(element_wise_product):.2f}, {np.max(element_wise_product):.2f}]
        
        **å‚æ•°é‡**: {kernel_size}Ã—{kernel_size} + 1 = {kernel_size*kernel_size + 1} ä¸ªå‚æ•°
        - vs å…¨è¿æ¥: å¦‚æœè¾“å…¥æ˜¯28Ã—28ï¼Œå…¨è¿æ¥éœ€è¦784ä¸ªå‚æ•°
        - èŠ‚çœ: {(1 - (kernel_size*kernel_size + 1) / 785) * 100:.1f}%
        
        **å…¸å‹åº”ç”¨**:
        - 3Ã—3: ResNetã€VGGçš„æ ‡å‡†é€‰æ‹©
        - 5Ã—5: AlexNetçš„ç¬¬ä¸€å±‚
        - 7Ã—7: ResNetçš„è¾“å…¥å±‚
        - 1Ã—1: é€šé“æ•°è°ƒæ•´ã€é™ç»´
        """
        )


def render_rnn_neuron():
    """æ¸²æŸ“å¾ªç¯ç¥ç»å…ƒæ¼”ç¤º"""
    st.subheader("ğŸ” å¾ªç¯ç¥ç»å…ƒï¼ˆRecurrent Neural Networkï¼‰")

    st.markdown(
        """
    **å·¥ä½œåŸç†**ï¼š
    1. ç»´æŠ¤ä¸€ä¸ªéšè—çŠ¶æ€ $h_t$ï¼Œå­˜å‚¨å†å²ä¿¡æ¯
    2. å½“å‰çŠ¶æ€ç”±å‰ä¸€çŠ¶æ€å’Œå½“å‰è¾“å…¥å…±åŒå†³å®š
    3. å¯ä»¥å¤„ç†å˜é•¿åºåˆ—æ•°æ®
    4. å…·æœ‰"è®°å¿†"èƒ½åŠ›
    
    **æ•°å­¦è¡¨è¾¾**ï¼š
    $$
    h_t = \\text{activation}(W_{hh} h_{t-1} + W_{xh} x_t + b)
    $$
    """
    )

    st.markdown("---")
    st.subheader("âš™ï¸ é…ç½®RNNç¥ç»å…ƒ")

    col1, col2 = st.columns(2)

    with col1:
        input_size = st.slider(
            "è¾“å…¥ç»´åº¦", 1, 5, 3, help="æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥å¤§å°", key="rnn_input_size"
        )
        hidden_size = st.slider(
            "éšè—å±‚ç»´åº¦", 1, 5, 3, help="éšè—çŠ¶æ€çš„å¤§å°", key="rnn_hidden_size"
        )
        activation = st.selectbox(
            "æ¿€æ´»å‡½æ•°",
            ["tanh", "relu", "sigmoid"],
            help="RNNé€šå¸¸ä½¿ç”¨tanh",
            key="rnn_activation",
        )

    with col2:
        sequence_length = st.slider(
            "åºåˆ—é•¿åº¦", 1, 5, 3, help="è¦å¤„ç†çš„æ—¶é—´æ­¥æ•°", key="rnn_seq_len"
        )
        seed = st.number_input(
            "éšæœºç§å­", 0, 100, 42, help="ç”¨äºåˆå§‹åŒ–æƒé‡", key="rnn_seed"
        )
        learning_rate = st.slider(
            "å­¦ä¹ ç‡", 0.001, 0.5, 0.01, 0.001, help="æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿", key="rnn_lr"
        )

    # åˆ›å»ºRNNç¥ç»å…ƒ
    neuron = RNNNeuron(
        input_size=input_size, hidden_size=hidden_size, activation=activation, seed=seed
    )

    st.markdown("---")

    # è¾“å…¥åºåˆ—
    st.subheader("ğŸ“¥ è¾“å…¥åºåˆ—")
    st.write(f"è®¾ç½® {sequence_length} ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ï¼š")

    sequence_data = []
    for t in range(sequence_length):
        st.write(f"**æ—¶é—´æ­¥ t={t}:**")
        time_step_input = []
        cols = st.columns(input_size)
        for i in range(input_size):
            with cols[i]:
                val = st.number_input(
                    f"$x_{t}[{i}]$",
                    value=float(np.random.randn() * 0.5),
                    format="%.4f",
                    key=f"rnn_input_t{t}_i{i}",
                )
                time_step_input.append(val)
        sequence_data.append(np.array(time_step_input))

    # æ˜¾ç¤ºæƒé‡çŸ©é˜µ
    st.subheader("ğŸ¯ æƒé‡çŸ©é˜µ")

    col1, col2 = st.columns(2)

    with col1:
        st.write("**W_xh (è¾“å…¥â†’éšè—):**")
        st.write(f"å½¢çŠ¶: ({hidden_size}, {input_size})")
        st.write(f"å‚æ•°é‡: {hidden_size * input_size}")

    with col2:
        st.write("**W_hh (éšè—â†’éšè—):**")
        st.write(f"å½¢çŠ¶: ({hidden_size}, {hidden_size})")
        st.write(f"å‚æ•°é‡: {hidden_size * hidden_size}")

    st.write(
        f"**æ€»å‚æ•°é‡**: {hidden_size * input_size + hidden_size * hidden_size + hidden_size}"
    )

    st.markdown("---")

    # æ‰§è¡Œåºåˆ—å¤„ç†
    if st.button("ğŸš€ å¤„ç†åºåˆ—", type="primary", key="rnn_compute"):
        st.subheader("â¡ï¸ åºåˆ—å¤„ç†è¿‡ç¨‹")

        # é‡ç½®éšè—çŠ¶æ€
        neuron.reset_hidden()

        outputs = []
        hidden_states = [neuron.h.copy()]

        for t, x_t in enumerate(sequence_data):
            st.write(f"### æ—¶é—´æ­¥ t={t}")

            h_t = neuron.forward(x_t)
            outputs.append(h_t.copy())
            hidden_states.append(h_t.copy())

            col1, col2, col3 = st.columns(3)

            with col1:
                st.write("**è¾“å…¥:**")
                st.write(f"{x_t}")

            with col2:
                st.write("**å‰ä¸€çŠ¶æ€:**")
                st.write(f"{hidden_states[t]}")

            with col3:
                st.write("**å½“å‰çŠ¶æ€:**")
                st.write(f"{h_t}")

            st.write(f"**è¾“å‡ºèŒƒæ•°:** {np.linalg.norm(h_t):.6f}")
            st.markdown("---")

        # å¯è§†åŒ–éšè—çŠ¶æ€æ¼”åŒ–
        with st.expander("ğŸ“Š éšè—çŠ¶æ€æ¼”åŒ–", expanded=True):
            fig = go.Figure()

            for i in range(hidden_size):
                values = [h[i] for h in hidden_states]
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(hidden_states))),
                        y=values,
                        mode="lines+markers",
                        name=f"h[{i}]",
                    )
                )

            fig.update_layout(
                title="éšè—çŠ¶æ€éšæ—¶é—´çš„å˜åŒ–",
                xaxis_title="æ—¶é—´æ­¥",
                yaxis_title="éšè—çŠ¶æ€å€¼",
                height=400,
            )
            st.plotly_chart(fig, use_container_width=True)

        # åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼Œåªå±•ç¤ºæœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
        st.markdown("---")
        st.subheader("â¬…ï¸ åå‘ä¼ æ’­ï¼ˆæœ€åæ—¶é—´æ­¥ï¼‰")

        upstream_grad = np.ones(hidden_size)
        st.write(f"ä¸Šæ¸¸æ¢¯åº¦: {upstream_grad}")

        gradients = neuron.backward(upstream_grad)

        st.success("âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ")

        with st.expander("ğŸ“‹ æ¢¯åº¦ä¿¡æ¯", expanded=True):
            st.write("**W_xh æ¢¯åº¦å½¢çŠ¶:**", gradients["W_xh"].shape)
            st.write("**W_xh æ¢¯åº¦èŒƒæ•°:**", f"{np.linalg.norm(gradients['W_xh']):.6f}")
            st.write("**W_hh æ¢¯åº¦å½¢çŠ¶:**", gradients["W_hh"].shape)
            st.write("**W_hh æ¢¯åº¦èŒƒæ•°:**", f"{np.linalg.norm(gradients['W_hh']):.6f}")
            st.write("**b_h æ¢¯åº¦:**", gradients["b_h"])

        # å‚æ•°æ›´æ–°
        st.markdown("---")
        st.subheader("ğŸ“Š å‚æ•°æ›´æ–°")

        old_W_xh = neuron.W_xh.copy()
        old_W_hh = neuron.W_hh.copy()

        neuron.update_parameters(learning_rate)

        st.success("âœ… å‚æ•°å·²æ›´æ–°")

        change_xh = np.linalg.norm(neuron.W_xh - old_W_xh)
        change_hh = np.linalg.norm(neuron.W_hh - old_W_hh)

        st.write(f"**W_xh å˜åŒ–é‡:** {change_xh:.6f}")
        st.write(f"**W_hh å˜åŒ–é‡:** {change_hh:.6f}")

        # ==================== æ•°å€¼ç¨³å®šæ€§æ£€æµ‹ ====================
        st.markdown("---")
        st.subheader("ğŸ”¬ æ•°å€¼ç¨³å®šæ€§è¯Šæ–­")

        st.info("ğŸ’¡ RNNç‰¹æœ‰é—®é¢˜ï¼šæ¢¯åº¦æ¶ˆå¤±ï¼ˆåºåˆ—å±•å¼€åï¼‰ã€éšè—çŠ¶æ€çˆ†ç‚¸")

        stability_issues = []

        # 1. æ£€æŸ¥æ¢¯åº¦
        grad_xh_check = StabilityChecker.check_gradient(
            gradients["W_xh"].flatten(), "W_xhæƒé‡æ¢¯åº¦"
        )
        stability_issues.append(grad_xh_check)

        grad_hh_check = StabilityChecker.check_gradient(
            gradients["W_hh"].flatten(), "W_hhæƒé‡æ¢¯åº¦"
        )
        stability_issues.append(grad_hh_check)

        # 2. æ£€æŸ¥éšè—çŠ¶æ€ï¼ˆå…³é”®ï¼å®¹æ˜“çˆ†ç‚¸ï¼‰
        for t, h in enumerate(hidden_states[1:]):  # è·³è¿‡åˆå§‹çŠ¶æ€
            h_check = StabilityChecker.check_activation(h, f"t={t}éšè—çŠ¶æ€")
            if h_check["status"] != "success":
                stability_issues.append(h_check)

        # å¦‚æœæ‰€æœ‰éšè—çŠ¶æ€éƒ½æ­£å¸¸ï¼Œæ·»åŠ ä¸€ä¸ªæ€»ç»“
        if all(
            StabilityChecker.check_activation(h, "")["status"] == "success"
            for h in hidden_states[1:]
        ):
            stability_issues.append(
                {
                    "status": "success",
                    "type": "æ‰€æœ‰æ—¶é—´æ­¥éšè—çŠ¶æ€",
                    "value": f"{len(hidden_states)-1}ä¸ªçŠ¶æ€",
                    "icon": "ğŸŸ¢",
                    "severity": "none",
                    "details": {
                        "æ—¶é—´æ­¥æ•°": len(hidden_states) - 1,
                        "æœ€å¤§èŒƒæ•°": f"{max(np.linalg.norm(h) for h in hidden_states[1:]):.4f}",
                        "æœ€å°èŒƒæ•°": f"{min(np.linalg.norm(h) for h in hidden_states[1:]):.4f}",
                    },
                }
            )

        # 3. æ£€æŸ¥æ¢¯åº¦é€šè¿‡æ—¶é—´çš„è¡°å‡ï¼ˆBPTTé—®é¢˜ï¼‰
        if len(hidden_states) > 1:
            # ä¼°è®¡æ¢¯åº¦è¡°å‡ç‡
            first_h_norm = np.linalg.norm(hidden_states[1])
            last_h_norm = np.linalg.norm(hidden_states[-1])

            if last_h_norm > 0:
                decay_factor = first_h_norm / last_h_norm

                if decay_factor > 10:
                    stability_issues.append(
                        {
                            "status": "warning",
                            "type": "RNNæ¢¯åº¦è¡°å‡",
                            "value": f"{decay_factor:.2f}å€",
                            "threshold": "> 10",
                            "icon": "ğŸŸ¡",
                            "severity": "medium",
                            "details": {
                                "é¦–ä¸ªçŠ¶æ€èŒƒæ•°": f"{first_h_norm:.4f}",
                                "æœ€åçŠ¶æ€èŒƒæ•°": f"{last_h_norm:.4f}",
                                "è¡°å‡å€æ•°": f"{decay_factor:.2f}",
                            },
                            "solution": [
                                "ä½¿ç”¨LSTMæˆ–GRUæ›¿ä»£æ™®é€šRNN",
                                "å‡å°‘åºåˆ—é•¿åº¦",
                                "ä½¿ç”¨æ¢¯åº¦è£å‰ª",
                                "ä½¿ç”¨æ›´å¥½çš„åˆå§‹åŒ–ï¼ˆOrthogonalï¼‰",
                                "æ·»åŠ è·³è·ƒè¿æ¥",
                            ],
                            "explanation": "RNNåœ¨é•¿åºåˆ—ä¸Šå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±ï¼ŒLSTM/GRUé€šè¿‡é—¨æ§æœºåˆ¶è§£å†³è¿™ä¸ªé—®é¢˜",
                        }
                    )

        # 4. æ£€æŸ¥W_hhçš„ç‰¹å¾å€¼ï¼ˆç†è®ºä¸Šåº”è¯¥æ¥è¿‘1ï¼‰
        eigenvalues = np.linalg.eigvals(neuron.W_hh)
        max_eigenvalue = np.max(np.abs(eigenvalues))

        if max_eigenvalue > 1.1:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "W_hhç‰¹å¾å€¼è¿‡å¤§",
                    "value": f"{max_eigenvalue:.4f}",
                    "threshold": "> 1.1",
                    "icon": "ğŸŸ¡",
                    "severity": "medium",
                    "details": {
                        "æœ€å¤§ç‰¹å¾å€¼": f"{max_eigenvalue:.4f}",
                        "ç‰¹å¾å€¼èŒƒå›´": f"[{np.min(np.abs(eigenvalues)):.4f}, {max_eigenvalue:.4f}]",
                    },
                    "solution": [
                        "ä½¿ç”¨Orthogonalåˆå§‹åŒ–",
                        "æ·»åŠ æƒé‡è¡°å‡",
                        "ä½¿ç”¨æ¢¯åº¦è£å‰ª",
                        "è€ƒè™‘ä½¿ç”¨LSTM/GRU",
                    ],
                    "explanation": "W_hhçš„æœ€å¤§ç‰¹å¾å€¼>1ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼Œ<1ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±",
                }
            )
        elif max_eigenvalue < 0.9:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "W_hhç‰¹å¾å€¼è¿‡å°",
                    "value": f"{max_eigenvalue:.4f}",
                    "threshold": "< 0.9",
                    "icon": "ğŸŸ¡",
                    "severity": "medium",
                    "details": {
                        "æœ€å¤§ç‰¹å¾å€¼": f"{max_eigenvalue:.4f}",
                        "ç‰¹å¾å€¼èŒƒå›´": f"[{np.min(np.abs(eigenvalues)):.4f}, {max_eigenvalue:.4f}]",
                    },
                    "solution": [
                        "ä½¿ç”¨Orthogonalåˆå§‹åŒ–ï¼ˆç‰¹å¾å€¼æ¥è¿‘1ï¼‰",
                        "å¢åŠ å­¦ä¹ ç‡",
                        "è€ƒè™‘ä½¿ç”¨LSTM/GRU",
                    ],
                    "explanation": "W_hhçš„æœ€å¤§ç‰¹å¾å€¼<1ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œä¿¡å·éšæ—¶é—´è¡°å‡",
                }
            )
        else:
            stability_issues.append(
                {
                    "status": "success",
                    "type": "W_hhç‰¹å¾å€¼",
                    "value": f"{max_eigenvalue:.4f}",
                    "icon": "ğŸŸ¢",
                    "severity": "none",
                    "details": {
                        "æœ€å¤§ç‰¹å¾å€¼": f"{max_eigenvalue:.4f}",
                        "ç†æƒ³èŒƒå›´": "[0.9, 1.1]",
                    },
                }
            )

        # 5. æ£€æŸ¥å­¦ä¹ ç‡
        combined_grad_norm = np.sqrt(
            np.linalg.norm(gradients["W_xh"]) ** 2
            + np.linalg.norm(gradients["W_hh"]) ** 2
        )
        combined_param_norm = np.sqrt(
            np.linalg.norm(neuron.W_xh) ** 2 + np.linalg.norm(neuron.W_hh) ** 2
        )
        lr_check = StabilityChecker.check_learning_rate(
            learning_rate, combined_grad_norm, combined_param_norm
        )
        stability_issues.append(lr_check)

        # æ˜¾ç¤ºè¯Šæ–­ç»“æœ
        StabilityChecker.display_issues(
            stability_issues, title="ğŸ”¬ RNNç¥ç»å…ƒç¨³å®šæ€§è¯Šæ–­"
        )

        # RNNç‰¹æœ‰çš„é¢å¤–è¯´æ˜
        st.markdown("---")
        st.info(
            f"""
        ğŸ’¡ **RNN ç‰¹æ€§ä¸ç¨³å®šæ€§**ï¼š
        
        **è®°å¿†èƒ½åŠ›**: éšè—çŠ¶æ€ $h_t$ å­˜å‚¨å†å²ä¿¡æ¯
        - å½“å‰æœ€å¤§èŒƒæ•°: {max(np.linalg.norm(h) for h in hidden_states[1:]):.4f}
        
        **åºåˆ—å¤„ç†**: å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—
        - å½“å‰åºåˆ—é•¿åº¦: {len(hidden_states)-1} ä¸ªæ—¶é—´æ­¥
        
        **å‚æ•°å…±äº«**: æ‰€æœ‰æ—¶é—´æ­¥ä½¿ç”¨ç›¸åŒçš„æƒé‡
        - W_hhç‰¹å¾å€¼: {max_eigenvalue:.4f} (ç†æƒ³â‰ˆ1.0)
        
        **æ—¶é—´å±•å¼€**: åå‘ä¼ æ’­éœ€è¦æ²¿æ—¶é—´å±•å¼€ï¼ˆBPTTï¼‰
        - æ¢¯åº¦éœ€è¦å›ä¼  {len(hidden_states)-1} ä¸ªæ—¶é—´æ­¥
        
        **æ¢¯åº¦é—®é¢˜**: é•¿åºåˆ—å¯èƒ½å‡ºç°æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸
        - å»ºè®®: åºåˆ—é•¿åº¦<50ï¼Œæˆ–ä½¿ç”¨LSTM/GRU
        
        **æ€»å‚æ•°é‡**: {hidden_size * input_size + hidden_size * hidden_size + hidden_size} ä¸ª
        """
        )


def render_gru_neuron():
    """æ¸²æŸ“GRUç¥ç»å…ƒæ¼”ç¤º"""
    st.subheader("ğŸ”„ GRUç¥ç»å…ƒï¼ˆGated Recurrent Unitï¼‰")

    st.markdown(
        """
    **å·¥ä½œåŸç†**ï¼š
    1. ä½¿ç”¨2ä¸ªé—¨æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼ˆæ¯”LSTMå°‘1ä¸ªé—¨ï¼‰
    2. é‡ç½®é—¨å†³å®šå¦‚ä½•ç»“åˆæ–°è¾“å…¥å’Œå‰ä¸€çŠ¶æ€
    3. æ›´æ–°é—¨å†³å®šä¿ç•™å¤šå°‘å‰ä¸€çŠ¶æ€
    4. æ²¡æœ‰å•ç‹¬çš„ç»†èƒçŠ¶æ€ï¼Œç›´æ¥æ›´æ–°éšè—çŠ¶æ€
    
    **æ•°å­¦è¡¨è¾¾**ï¼š
    $$
    \\begin{aligned}
    r_t &= \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r) && \\text{é‡ç½®é—¨} \\\\
    z_t &= \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z) && \\text{æ›´æ–°é—¨} \\\\
    \\tilde{h}_t &= \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h) && \\text{å€™é€‰çŠ¶æ€} \\\\
    h_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t && \\text{æœ€ç»ˆçŠ¶æ€}
    \\end{aligned}
    $$
    
    **GRU vs LSTM vs RNN**ï¼š
    - **RNN**: æ— é—¨æ§ï¼Œæ¢¯åº¦æ¶ˆå¤±ä¸¥é‡
    - **LSTM**: 3ä¸ªé—¨ + ç»†èƒçŠ¶æ€ï¼Œå‚æ•°æœ€å¤šï¼Œèƒ½åŠ›æœ€å¼º
    - **GRU**: 2ä¸ªé—¨ï¼Œå‚æ•°é€‚ä¸­ï¼Œæ€§èƒ½æ¥è¿‘LSTMä½†æ›´å¿«
    """
    )

    st.markdown("---")
    st.subheader("âš™ï¸ é…ç½®GRUç¥ç»å…ƒ")

    col1, col2 = st.columns(2)

    with col1:
        input_size = st.slider(
            "è¾“å…¥ç»´åº¦", 1, 5, 3, help="æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥å¤§å°", key="gru_input_size"
        )
        hidden_size = st.slider(
            "éšè—å±‚ç»´åº¦", 1, 5, 3, help="éšè—çŠ¶æ€çš„å¤§å°", key="gru_hidden_size"
        )

    with col2:
        sequence_length = st.slider(
            "åºåˆ—é•¿åº¦", 1, 5, 3, help="è¦å¤„ç†çš„æ—¶é—´æ­¥æ•°", key="gru_seq_len"
        )
        seed = st.number_input(
            "éšæœºç§å­", 0, 100, 42, help="ç”¨äºåˆå§‹åŒ–æƒé‡", key="gru_seed"
        )
        learning_rate = st.slider(
            "å­¦ä¹ ç‡", 0.001, 0.5, 0.01, 0.001, help="æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿", key="gru_lr"
        )

    # åˆ›å»ºGRUç¥ç»å…ƒ
    neuron = GRUNeuron(input_size=input_size, hidden_size=hidden_size, seed=seed)

    st.markdown("---")

    # æ˜¾ç¤ºå‚æ•°é‡å¹¶ä¸RNNã€LSTMå¯¹æ¯”
    st.subheader("ğŸ¯ å‚æ•°é‡åˆ†æä¸å¯¹æ¯”")
    combined_size = hidden_size + input_size
    params_per_gate = hidden_size * combined_size + hidden_size
    gru_total_params = 3 * params_per_gate

    # è®¡ç®—å¯¹æ¯”æ•°æ®
    rnn_params = hidden_size * input_size + hidden_size * hidden_size + hidden_size
    lstm_params = 4 * params_per_gate

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("RNNå‚æ•°é‡", f"{rnn_params}", delta=f"åŸºå‡†", delta_color="off")
    with col2:
        st.metric(
            "GRUå‚æ•°é‡",
            f"{gru_total_params}",
            delta=f"+{gru_total_params - rnn_params} vs RNN",
        )
    with col3:
        st.metric(
            "LSTMå‚æ•°é‡",
            f"{lstm_params}",
            delta=f"+{lstm_params - gru_total_params} vs GRU",
        )

    st.info(
        f"""
    ğŸ’¡ **å‚æ•°é‡å¯¹æ¯”**ï¼ˆinput={input_size}, hidden={hidden_size}ï¼‰ï¼š
    - **RNN**: {rnn_params}ä¸ªå‚æ•° - æœ€å°‘ï¼Œä½†æ¢¯åº¦æ¶ˆå¤±ä¸¥é‡
    - **GRU**: {gru_total_params}ä¸ªå‚æ•° - é€‚ä¸­ï¼Œæ€§èƒ½å¥½ä¸”å¿«é€Ÿ
    - **LSTM**: {lstm_params}ä¸ªå‚æ•° - æœ€å¤šï¼Œèƒ½åŠ›æœ€å¼º
    
    **GRUçš„ä¼˜åŠ¿**ï¼š
    - âœ… æ¯”LSTMå°‘25%å‚æ•°ï¼ˆ3ä¸ªé—¨ vs 4ä¸ªé—¨ï¼‰
    - âœ… è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æ›´å¿«
    - âœ… åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šæ€§èƒ½ä¸LSTMç›¸å½“
    - âœ… æ›´ç®€å•ï¼Œæ›´å®¹æ˜“ç†è§£å’Œè°ƒè¯•
    """
    )

    st.markdown("---")

    # è¾“å…¥åºåˆ—
    st.subheader("ğŸ“¥ è¾“å…¥åºåˆ—")
    st.write(f"è®¾ç½® {sequence_length} ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ï¼š")

    sequence_data = []
    for t in range(sequence_length):
        st.write(f"**æ—¶é—´æ­¥ t={t}:**")
        time_step_input = []
        cols = st.columns(input_size)
        for i in range(input_size):
            with cols[i]:
                val = st.number_input(
                    f"$x_{t}[{i}]$",
                    value=float(np.random.randn() * 0.5),
                    format="%.4f",
                    key=f"gru_input_t{t}_i{i}",
                )
                time_step_input.append(val)
        sequence_data.append(np.array(time_step_input))

    st.markdown("---")

    # æ‰§è¡Œåºåˆ—å¤„ç†
    if st.button("ğŸš€ å¤„ç†GRUåºåˆ—", type="primary", key="gru_compute"):
        st.subheader("â¡ï¸ åºåˆ—å¤„ç†è¿‡ç¨‹")

        # é‡ç½®éšè—çŠ¶æ€
        neuron.reset_state()

        outputs = []
        hidden_states = [neuron.h.copy()]
        gate_history = []

        for t, x_t in enumerate(sequence_data):
            st.write(f"### æ—¶é—´æ­¥ t={t}")

            h_t = neuron.forward(x_t)
            outputs.append(h_t.copy())
            hidden_states.append(h_t.copy())

            # ä¿å­˜é—¨çš„å€¼
            gate_history.append(
                {
                    "r_t": neuron.forward_history["r_t"].copy(),
                    "z_t": neuron.forward_history["z_t"].copy(),
                    "h_tilde": neuron.forward_history["h_tilde"].copy(),
                }
            )

            # æ˜¾ç¤ºè¯¦ç»†è®¡ç®—è¿‡ç¨‹ï¼ˆä½“ç°é¡¹ç›®å®šä½ï¼‰
            with st.expander(f"ğŸ”¬ t={t} çš„è¯¦ç»†è®¡ç®—æ­¥éª¤", expanded=True):
                st.write("**æ­¥éª¤1: æ‹¼æ¥è¾“å…¥**")
                st.write(f"$[h_{{t-1}}, x_t] = {neuron.forward_history['combined']}$")

                st.write("**æ­¥éª¤2: è®¡ç®—é‡ç½®é—¨ï¼ˆå†³å®šå¦‚ä½•ä½¿ç”¨å†å²ä¿¡æ¯ï¼‰**")
                st.write(f"$r_t = \\sigma(W_r \\cdot [h_{{t-1}}, x_t] + b_r)$")
                gate_data = []
                for i in range(hidden_size):
                    gate_data.append(
                        {
                            "ç»´åº¦": f"[{i}]",
                            "é‡ç½®é—¨å€¼": f'{neuron.forward_history["r_t"][i]:.4f}',
                            "å«ä¹‰": (
                                "æ¥è¿‘0â†’å¿½ç•¥å†å²"
                                if neuron.forward_history["r_t"][i] < 0.5
                                else "æ¥è¿‘1â†’ä¿ç•™å†å²"
                            ),
                        }
                    )
                gate_df = pd.DataFrame(gate_data)
                st.markdown(gate_df.to_markdown(index=False))

                st.write("**æ­¥éª¤3: åº”ç”¨é‡ç½®é—¨**")
                st.write(
                    f"é‡ç½®åçš„å†å²: $r_t \\odot h_{{t-1}} = {neuron.forward_history['r_t'] * neuron.forward_history['h_prev']}$"
                )

                st.write("**æ­¥éª¤4: è®¡ç®—å€™é€‰éšè—çŠ¶æ€**")
                st.write(
                    f"$\\tilde{{h}}_t = \\tanh(W_h \\cdot [r_t \\odot h_{{t-1}}, x_t] + b_h)$"
                )
                st.write(f"$\\tilde{{h}}_t = {neuron.forward_history['h_tilde']}$")

                st.write("**æ­¥éª¤5: è®¡ç®—æ›´æ–°é—¨ï¼ˆå†³å®šæ–°æ—§ä¿¡æ¯çš„æ¯”ä¾‹ï¼‰**")
                st.write(f"$z_t = \\sigma(W_z \\cdot [h_{{t-1}}, x_t] + b_z)$")
                update_data = []
                for i in range(hidden_size):
                    update_data.append(
                        {
                            "ç»´åº¦": f"[{i}]",
                            "æ›´æ–°é—¨å€¼": f'{neuron.forward_history["z_t"][i]:.4f}',
                            "æ—§ä¿¡æ¯æƒé‡": f'{1 - neuron.forward_history["z_t"][i]:.4f}',
                            "æ–°ä¿¡æ¯æƒé‡": f'{neuron.forward_history["z_t"][i]:.4f}',
                        }
                    )
                update_df = pd.DataFrame(update_data)
                st.markdown(update_df.to_markdown(index=False))

                st.write("**æ­¥éª¤6: è®¡ç®—æœ€ç»ˆéšè—çŠ¶æ€ï¼ˆåŠ æƒç»„åˆï¼‰**")
                st.write(
                    f"$h_t = (1 - z_t) \\odot h_{{t-1}} + z_t \\odot \\tilde{{h}}_t$"
                )

                final_data = []
                for i in range(hidden_size):
                    old_contrib = (
                        1 - neuron.forward_history["z_t"][i]
                    ) * neuron.forward_history["h_prev"][i]
                    new_contrib = (
                        neuron.forward_history["z_t"][i]
                        * neuron.forward_history["h_tilde"][i]
                    )
                    final_data.append(
                        {
                            "ç»´åº¦": f"[{i}]",
                            "æ—§çŠ¶æ€è´¡çŒ®": f"{old_contrib:.4f}",
                            "æ–°çŠ¶æ€è´¡çŒ®": f"{new_contrib:.4f}",
                            "æœ€ç»ˆå€¼": f"{h_t[i]:.4f}",
                        }
                    )
                final_df = pd.DataFrame(final_data)
                st.markdown(final_df.to_markdown(index=False))

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("éšè—çŠ¶æ€èŒƒæ•°", f"{np.linalg.norm(h_t):.4f}")
            with col2:
                avg_reset = np.mean(neuron.forward_history["r_t"])
                st.metric("å¹³å‡é‡ç½®ç‡", f"{avg_reset:.4f}")
            with col3:
                avg_update = np.mean(neuron.forward_history["z_t"])
                st.metric("å¹³å‡æ›´æ–°ç‡", f"{avg_update:.4f}")
            with col4:
                # è®¡ç®—ä¿¡æ¯ä¿ç•™ç‡
                retention = 1 - avg_update
                st.metric("ä¿¡æ¯ä¿ç•™ç‡", f"{retention:.4f}")

            st.markdown("---")

        # å¯è§†åŒ–é—¨çš„æ¼”åŒ–
        with st.expander("ğŸ“Š é—¨æ§å€¼æ¼”åŒ–", expanded=True):
            fig = make_subplots(
                rows=2,
                cols=2,
                subplot_titles=(
                    "é‡ç½®é—¨ r_t",
                    "æ›´æ–°é—¨ z_t",
                    "å€™é€‰çŠ¶æ€ hÌƒ_t",
                    "éšè—çŠ¶æ€èŒƒæ•°",
                ),
            )

            for i in range(hidden_size):
                # é‡ç½®é—¨
                r_values = [g["r_t"][i] for g in gate_history]
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(r_values))),
                        y=r_values,
                        mode="lines+markers",
                        name=f"r[{i}]",
                        showlegend=False,
                    ),
                    row=1,
                    col=1,
                )

                # æ›´æ–°é—¨
                z_values = [g["z_t"][i] for g in gate_history]
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(z_values))),
                        y=z_values,
                        mode="lines+markers",
                        name=f"z[{i}]",
                        showlegend=False,
                    ),
                    row=1,
                    col=2,
                )

                # å€™é€‰çŠ¶æ€
                h_tilde_values = [g["h_tilde"][i] for g in gate_history]
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(h_tilde_values))),
                        y=h_tilde_values,
                        mode="lines+markers",
                        name=f"hÌƒ[{i}]",
                        showlegend=False,
                    ),
                    row=2,
                    col=1,
                )

            # éšè—çŠ¶æ€èŒƒæ•°
            h_norms = [np.linalg.norm(h) for h in hidden_states]
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(h_norms))),
                    y=h_norms,
                    mode="lines+markers",
                    name="||h||",
                    showlegend=False,
                ),
                row=2,
                col=2,
            )

            fig.update_xaxes(title_text="æ—¶é—´æ­¥", row=2, col=1)
            fig.update_xaxes(title_text="æ—¶é—´æ­¥", row=2, col=2)
            fig.update_yaxes(title_text="é—¨å€¼", row=1, col=1)
            fig.update_yaxes(title_text="é—¨å€¼", row=1, col=2)
            fig.update_yaxes(title_text="å€™é€‰å€¼", row=2, col=1)
            fig.update_yaxes(title_text="èŒƒæ•°", row=2, col=2)

            fig.update_layout(height=600, showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

        # åå‘ä¼ æ’­
        st.markdown("---")
        st.subheader("â¬…ï¸ åå‘ä¼ æ’­ï¼ˆæœ€åæ—¶é—´æ­¥ï¼‰")

        upstream_grad = np.ones(hidden_size)
        st.write(f"ä¸Šæ¸¸æ¢¯åº¦: {upstream_grad}")

        gradients = neuron.backward(upstream_grad)

        st.success("âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ")

        with st.expander("ğŸ“‹ æ¢¯åº¦ä¿¡æ¯", expanded=True):
            grad_info = []
            for gate_name in ["r", "z", "h"]:
                W_key = f"grad_W_{gate_name}"
                b_key = f"grad_b_{gate_name}"
                grad_info.append(
                    {
                        "é—¨/çŠ¶æ€": (
                            f"{gate_name}é—¨" if gate_name in ["r", "z"] else "å€™é€‰çŠ¶æ€"
                        ),
                        "Wæ¢¯åº¦èŒƒæ•°": f"{np.linalg.norm(gradients[W_key]):.6f}",
                        "bæ¢¯åº¦èŒƒæ•°": f"{np.linalg.norm(gradients[b_key]):.6f}",
                    }
                )

            grad_df = pd.DataFrame(grad_info)
            st.markdown(grad_df.to_markdown(index=False))

        # ==================== æ•°å€¼ç¨³å®šæ€§æ£€æµ‹ ====================
        st.markdown("---")
        st.subheader("ğŸ”¬ æ•°å€¼ç¨³å®šæ€§è¯Šæ–­")

        st.info("ğŸ’¡ GRUç‰¹æœ‰æ£€æµ‹ï¼šé—¨æ§é¥±å’Œã€æ¢¯åº¦æ¶ˆå¤±ã€åºåˆ—é•¿åº¦å½±å“")

        stability_issues = []

        # 1. æ£€æŸ¥æ¢¯åº¦
        for gate_name in ["r", "z", "h"]:
            W_key = f"grad_W_{gate_name}"
            gate_label = {"r": "é‡ç½®é—¨", "z": "æ›´æ–°é—¨", "h": "å€™é€‰çŠ¶æ€"}[gate_name]
            grad_check = StabilityChecker.check_gradient(
                gradients[W_key].flatten(), f"{gate_label}æƒé‡æ¢¯åº¦"
            )
            stability_issues.append(grad_check)

        # 2. æ£€æŸ¥é—¨æ§é¥±å’Œï¼ˆGRUç‰¹æœ‰ï¼‰
        if gate_history:
            last_gates = gate_history[-1]

            # æ£€æŸ¥é‡ç½®é—¨
            r_check = StabilityChecker.check_gate_saturation(
                last_gates["r_t"], "é‡ç½®é—¨ r_t"
            )
            stability_issues.append(r_check)

            # æ£€æŸ¥æ›´æ–°é—¨
            z_check = StabilityChecker.check_gate_saturation(
                last_gates["z_t"], "æ›´æ–°é—¨ z_t"
            )
            stability_issues.append(z_check)

        # 3. æ£€æŸ¥éšè—çŠ¶æ€
        h_check = StabilityChecker.check_activation(neuron.h, "éšè—çŠ¶æ€")
        stability_issues.append(h_check)

        # 4. æ£€æŸ¥å­¦ä¹ ç‡
        combined_grad_norm = np.sqrt(
            np.linalg.norm(gradients["grad_W_r"]) ** 2
            + np.linalg.norm(gradients["grad_W_z"]) ** 2
            + np.linalg.norm(gradients["grad_W_h"]) ** 2
        )
        combined_param_norm = np.sqrt(
            np.linalg.norm(neuron.W_r) ** 2
            + np.linalg.norm(neuron.W_z) ** 2
            + np.linalg.norm(neuron.W_h) ** 2
        )
        lr_check = StabilityChecker.check_learning_rate(
            learning_rate, combined_grad_norm, combined_param_norm
        )
        stability_issues.append(lr_check)

        # æ˜¾ç¤ºè¯Šæ–­ç»“æœ
        StabilityChecker.display_issues(
            stability_issues, title="ğŸ”¬ GRUç¥ç»å…ƒç¨³å®šæ€§è¯Šæ–­"
        )

        # å‚æ•°æ›´æ–°
        st.markdown("---")
        st.subheader("ğŸ“Š å‚æ•°æ›´æ–°")

        old_W_r = neuron.W_r.copy()
        old_W_z = neuron.W_z.copy()
        old_W_h = neuron.W_h.copy()

        neuron.update_parameters(learning_rate)

        st.success("âœ… å‚æ•°å·²æ›´æ–°")

        change_r = np.linalg.norm(neuron.W_r - old_W_r)
        change_z = np.linalg.norm(neuron.W_z - old_W_z)
        change_h = np.linalg.norm(neuron.W_h - old_W_h)

        st.write("**å„é—¨æƒé‡å˜åŒ–é‡ï¼š**")
        changes_data = [
            {"é—¨/çŠ¶æ€": "é‡ç½®é—¨", "å˜åŒ–é‡": f"{change_r:.6f}"},
            {"é—¨/çŠ¶æ€": "æ›´æ–°é—¨", "å˜åŒ–é‡": f"{change_z:.6f}"},
            {"é—¨/çŠ¶æ€": "å€™é€‰çŠ¶æ€", "å˜åŒ–é‡": f"{change_h:.6f}"},
        ]
        changes_df = pd.DataFrame(changes_data)
        st.markdown(changes_df.to_markdown(index=False))

        st.info(
            f"""
        ğŸ’¡ **GRU ç‰¹æ€§æ€»ç»“**ï¼š
        
        **è®¡ç®—ç»†èŠ‚**ï¼ˆé¡¹ç›®æ ¸å¿ƒï¼‰ï¼š
        - æ¯ä¸ªæ—¶é—´æ­¥è¿›è¡Œ6æ¬¡çŸ©é˜µè¿ç®—
        - 2æ¬¡sigmoidæ¿€æ´»ï¼ˆé—¨æ§ï¼‰
        - 1æ¬¡tanhæ¿€æ´»ï¼ˆå€™é€‰çŠ¶æ€ï¼‰
        - 3æ¬¡é€å…ƒç´ ä¹˜æ³•ï¼ˆé—¨æ§åº”ç”¨ï¼‰
        
        **é—¨æ§æœºåˆ¶**ï¼š
        - **é‡ç½®é—¨** $r_t$: æ§åˆ¶å¦‚ä½•ç»“åˆå†å²ä¿¡æ¯ï¼ˆæ¥è¿‘0â†’å¿½ç•¥å†å²ï¼‰
        - **æ›´æ–°é—¨** $z_t$: æ§åˆ¶æ–°æ—§ä¿¡æ¯æ¯”ä¾‹ï¼ˆæ¥è¿‘1â†’é‡‡ç”¨æ–°ä¿¡æ¯ï¼‰
        
        **ä¼˜åŠ¿å¯¹æ¯”**ï¼š
        - vs RNN: è§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œèƒ½æ•è·é•¿æœŸä¾èµ–
        - vs LSTM: å‚æ•°å°‘25%ï¼Œé€Ÿåº¦å¿«ï¼Œæ€§èƒ½ç›¸å½“
        
        **å®é™…åº”ç”¨**ï¼š
        - æœºå™¨ç¿»è¯‘ï¼ˆGoogle Translateç”¨è¿‡GRUï¼‰
        - è¯­éŸ³è¯†åˆ«
        - æ–‡æœ¬ç”Ÿæˆ
        - æ—¶åºé¢„æµ‹
        
        **æ€»å‚æ•°é‡**: {gru_total_params} ä¸ª
        **æµ®ç‚¹è¿ç®—æ•°**: çº¦ {3 * hidden_size * (hidden_size + input_size) * 2} FLOPs/æ—¶é—´æ­¥
        """
        )


def render_lstm_neuron():
    """æ¸²æŸ“LSTMç¥ç»å…ƒæ¼”ç¤º"""
    st.subheader("ğŸ§  LSTMç¥ç»å…ƒï¼ˆLong Short-Term Memoryï¼‰")

    st.markdown(
        """
    **å·¥ä½œåŸç†**ï¼š
    1. ä½¿ç”¨é—¨æ§æœºåˆ¶æ§åˆ¶ä¿¡æ¯æµåŠ¨
    2. ç»´æŠ¤ç»†èƒçŠ¶æ€ $C_t$ å­˜å‚¨é•¿æœŸè®°å¿†
    3. ä¸‰ä¸ªé—¨ï¼šé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨
    4. è§£å†³RNNçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
    
    **æ•°å­¦è¡¨è¾¾**ï¼ˆé—¨æ§æœºåˆ¶ï¼‰ï¼š
    $$
    \\begin{aligned}
    f_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) && \\text{é—å¿˜é—¨} \\\\
    i_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) && \\text{è¾“å…¥é—¨} \\\\
    \\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) && \\text{å€™é€‰å€¼} \\\\
    C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t && \\text{æ›´æ–°ç»†èƒ} \\\\
    o_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) && \\text{è¾“å‡ºé—¨} \\\\
    h_t &= o_t \\odot \\tanh(C_t) && \\text{è¾“å‡º}
    \\end{aligned}
    $$
    """
    )

    st.markdown("---")
    st.subheader("âš™ï¸ é…ç½®LSTMç¥ç»å…ƒ")

    col1, col2 = st.columns(2)

    with col1:
        input_size = st.slider(
            "è¾“å…¥ç»´åº¦", 1, 5, 3, help="æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥å¤§å°", key="lstm_input_size"
        )
        hidden_size = st.slider(
            "éšè—å±‚ç»´åº¦",
            1,
            5,
            3,
            help="éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€çš„å¤§å°",
            key="lstm_hidden_size",
        )

    with col2:
        sequence_length = st.slider(
            "åºåˆ—é•¿åº¦", 1, 5, 3, help="è¦å¤„ç†çš„æ—¶é—´æ­¥æ•°", key="lstm_seq_len"
        )
        seed = st.number_input(
            "éšæœºç§å­", 0, 100, 42, help="ç”¨äºåˆå§‹åŒ–æƒé‡", key="lstm_seed"
        )
        learning_rate = st.slider(
            "å­¦ä¹ ç‡", 0.001, 0.5, 0.01, 0.001, help="æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿", key="lstm_lr"
        )

    # åˆ›å»ºLSTMç¥ç»å…ƒ
    neuron = LSTMNeuron(input_size=input_size, hidden_size=hidden_size, seed=seed)

    st.markdown("---")

    # æ˜¾ç¤ºå‚æ•°é‡
    st.subheader("ğŸ¯ å‚æ•°é‡åˆ†æ")
    combined_size = hidden_size + input_size
    params_per_gate = hidden_size * combined_size + hidden_size
    total_params = 4 * params_per_gate

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("æ¯ä¸ªé—¨çš„å‚æ•°", f"{params_per_gate}")
    with col2:
        st.metric("é—¨çš„æ•°é‡", "4")
    with col3:
        st.metric("æ€»å‚æ•°é‡", f"{total_params}")

    st.info(
        """
    ğŸ’¡ **LSTMæœ‰4ç»„æƒé‡**ï¼š
    - é—å¿˜é—¨ $W_f, b_f$ï¼šå†³å®šé—å¿˜å¤šå°‘å†å²ä¿¡æ¯
    - è¾“å…¥é—¨ $W_i, b_i$ï¼šå†³å®šæ¥å—å¤šå°‘æ–°ä¿¡æ¯
    - å€™é€‰å€¼ $W_C, b_C$ï¼šç”Ÿæˆå€™é€‰è®°å¿†
    - è¾“å‡ºé—¨ $W_o, b_o$ï¼šå†³å®šè¾“å‡ºå¤šå°‘ä¿¡æ¯
    """
    )

    st.markdown("---")

    # è¾“å…¥åºåˆ—
    st.subheader("ğŸ“¥ è¾“å…¥åºåˆ—")
    st.write(f"è®¾ç½® {sequence_length} ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ï¼š")

    sequence_data = []
    for t in range(sequence_length):
        st.write(f"**æ—¶é—´æ­¥ t={t}:**")
        time_step_input = []
        cols = st.columns(input_size)
        for i in range(input_size):
            with cols[i]:
                val = st.number_input(
                    f"$x_{t}[{i}]$",
                    value=float(np.random.randn() * 0.5),
                    format="%.4f",
                    key=f"lstm_input_t{t}_i{i}",
                )
                time_step_input.append(val)
        sequence_data.append(np.array(time_step_input))

    st.markdown("---")

    # æ‰§è¡Œåºåˆ—å¤„ç†
    if st.button("ğŸš€ å¤„ç†LSTMåºåˆ—", type="primary", key="lstm_compute"):
        st.subheader("â¡ï¸ åºåˆ—å¤„ç†è¿‡ç¨‹")

        # é‡ç½®çŠ¶æ€
        neuron.reset_state()

        outputs = []
        hidden_states = [neuron.h.copy()]
        cell_states = [neuron.C.copy()]
        gate_history = []

        for t, x_t in enumerate(sequence_data):
            st.write(f"### æ—¶é—´æ­¥ t={t}")

            h_t = neuron.forward(x_t)
            outputs.append(h_t.copy())
            hidden_states.append(h_t.copy())
            cell_states.append(neuron.C.copy())

            # ä¿å­˜é—¨çš„å€¼
            gate_history.append(
                {
                    "f_t": neuron.forward_history["f_t"].copy(),
                    "i_t": neuron.forward_history["i_t"].copy(),
                    "o_t": neuron.forward_history["o_t"].copy(),
                    "C_tilde": neuron.forward_history["C_tilde"].copy(),
                }
            )

            # æ˜¾ç¤ºé—¨çš„æ¿€æ´»å€¼
            with st.expander(f"ğŸšª t={t} çš„é—¨æ§å€¼", expanded=False):
                gate_data = []
                for i in range(hidden_size):
                    gate_data.append(
                        {
                            "ç»´åº¦": f"[{i}]",
                            "é—å¿˜é—¨": f'{neuron.forward_history["f_t"][i]:.4f}',
                            "è¾“å…¥é—¨": f'{neuron.forward_history["i_t"][i]:.4f}',
                            "è¾“å‡ºé—¨": f'{neuron.forward_history["o_t"][i]:.4f}',
                            "å€™é€‰å€¼": f'{neuron.forward_history["C_tilde"][i]:.4f}',
                        }
                    )

                gate_df = pd.DataFrame(gate_data)
                st.markdown(gate_df.to_markdown(index=False))

                st.write(f"**ç»†èƒçŠ¶æ€ $C_t$:** {neuron.C}")
                st.write(f"**éšè—çŠ¶æ€ $h_t$:** {h_t}")

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç»†èƒçŠ¶æ€èŒƒæ•°", f"{np.linalg.norm(neuron.C):.4f}")
            with col2:
                st.metric("éšè—çŠ¶æ€èŒƒæ•°", f"{np.linalg.norm(h_t):.4f}")
            with col3:
                avg_forget = np.mean(neuron.forward_history["f_t"])
                st.metric("å¹³å‡é—å¿˜ç‡", f"{avg_forget:.4f}")

            st.markdown("---")

        # å¯è§†åŒ–é—¨çš„æ¼”åŒ–
        with st.expander("ğŸ“Š é—¨æ§å€¼æ¼”åŒ–", expanded=True):
            fig = make_subplots(
                rows=2,
                cols=2,
                subplot_titles=("é—å¿˜é—¨", "è¾“å…¥é—¨", "è¾“å‡ºé—¨", "ç»†èƒçŠ¶æ€èŒƒæ•°"),
            )

            for i in range(hidden_size):
                # é—å¿˜é—¨
                f_values = [g["f_t"][i] for g in gate_history]
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(f_values))),
                        y=f_values,
                        mode="lines+markers",
                        name=f"f[{i}]",
                        showlegend=False,
                    ),
                    row=1,
                    col=1,
                )

                # è¾“å…¥é—¨
                i_values = [g["i_t"][i] for g in gate_history]
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(i_values))),
                        y=i_values,
                        mode="lines+markers",
                        name=f"i[{i}]",
                        showlegend=False,
                    ),
                    row=1,
                    col=2,
                )

                # è¾“å‡ºé—¨
                o_values = [g["o_t"][i] for g in gate_history]
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(o_values))),
                        y=o_values,
                        mode="lines+markers",
                        name=f"o[{i}]",
                        showlegend=False,
                    ),
                    row=2,
                    col=1,
                )

            # ç»†èƒçŠ¶æ€èŒƒæ•°
            c_norms = [np.linalg.norm(c) for c in cell_states]
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(c_norms))),
                    y=c_norms,
                    mode="lines+markers",
                    name="||C||",
                    showlegend=False,
                ),
                row=2,
                col=2,
            )

            fig.update_xaxes(title_text="æ—¶é—´æ­¥", row=2, col=1)
            fig.update_xaxes(title_text="æ—¶é—´æ­¥", row=2, col=2)
            fig.update_yaxes(title_text="é—¨å€¼", row=1, col=1)
            fig.update_yaxes(title_text="é—¨å€¼", row=1, col=2)
            fig.update_yaxes(title_text="é—¨å€¼", row=2, col=1)
            fig.update_yaxes(title_text="èŒƒæ•°", row=2, col=2)

            fig.update_layout(height=600, showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

        # åå‘ä¼ æ’­
        st.markdown("---")
        st.subheader("â¬…ï¸ åå‘ä¼ æ’­ï¼ˆæœ€åæ—¶é—´æ­¥ï¼‰")

        upstream_grad = np.ones(hidden_size)
        st.write(f"ä¸Šæ¸¸æ¢¯åº¦: {upstream_grad}")

        gradients = neuron.backward(upstream_grad)

        st.success("âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ")

        with st.expander("ğŸ“‹ æ¢¯åº¦ä¿¡æ¯", expanded=True):
            grad_info = []
            for gate_name in ["f", "i", "C", "o"]:
                W_key = f"grad_W_{gate_name}"
                b_key = f"grad_b_{gate_name}"
                grad_info.append(
                    {
                        "é—¨": f"{gate_name}é—¨",
                        "Wæ¢¯åº¦èŒƒæ•°": f"{np.linalg.norm(gradients[W_key]):.6f}",
                        "bæ¢¯åº¦èŒƒæ•°": f"{np.linalg.norm(gradients[b_key]):.6f}",
                    }
                )

            grad_df = pd.DataFrame(grad_info)
            st.markdown(grad_df.to_markdown(index=False))

        # ==================== æ•°å€¼ç¨³å®šæ€§æ£€æµ‹ ====================
        st.markdown("---")
        st.subheader("ğŸ”¬ æ•°å€¼ç¨³å®šæ€§è¯Šæ–­")

        st.info("ğŸ’¡ LSTMç‰¹æœ‰æ£€æµ‹ï¼š4ä¸ªé—¨æ§é¥±å’Œã€ç»†èƒçŠ¶æ€çˆ†ç‚¸ã€é•¿æœŸä¾èµ–é—®é¢˜")

        stability_issues = []

        # 1. æ£€æŸ¥æ¢¯åº¦
        gate_labels = {"f": "é—å¿˜é—¨", "i": "è¾“å…¥é—¨", "C": "å€™é€‰å€¼", "o": "è¾“å‡ºé—¨"}
        for gate_name in ["f", "i", "C", "o"]:
            W_key = f"grad_W_{gate_name}"
            grad_check = StabilityChecker.check_gradient(
                gradients[W_key].flatten(), f"{gate_labels[gate_name]}æƒé‡æ¢¯åº¦"
            )
            stability_issues.append(grad_check)

        # 2. æ£€æŸ¥é—¨æ§é¥±å’Œï¼ˆLSTMç‰¹æœ‰ - 4ä¸ªé—¨ï¼‰
        if gate_history:
            last_gates = gate_history[-1]

            # æ£€æŸ¥é—å¿˜é—¨
            f_check = StabilityChecker.check_gate_saturation(
                last_gates["f_t"], "é—å¿˜é—¨ f_t"
            )
            stability_issues.append(f_check)

            # æ£€æŸ¥è¾“å…¥é—¨
            i_check = StabilityChecker.check_gate_saturation(
                last_gates["i_t"], "è¾“å…¥é—¨ i_t"
            )
            stability_issues.append(i_check)

            # æ£€æŸ¥è¾“å‡ºé—¨
            o_check = StabilityChecker.check_gate_saturation(
                last_gates["o_t"], "è¾“å‡ºé—¨ o_t"
            )
            stability_issues.append(o_check)

        # 3. æ£€æŸ¥ç»†èƒçŠ¶æ€å’Œéšè—çŠ¶æ€
        c_check = StabilityChecker.check_activation(neuron.C, "ç»†èƒçŠ¶æ€ C_t")
        stability_issues.append(c_check)

        h_check = StabilityChecker.check_activation(neuron.h, "éšè—çŠ¶æ€ h_t")
        stability_issues.append(h_check)

        # 4. æ£€æŸ¥å­¦ä¹ ç‡
        combined_grad_norm = np.sqrt(
            sum(
                [
                    np.linalg.norm(gradients[f"grad_W_{g}"]) ** 2
                    for g in ["f", "i", "C", "o"]
                ]
            )
        )
        combined_param_norm = np.sqrt(
            sum(
                [
                    np.linalg.norm(getattr(neuron, f"W_{g}")) ** 2
                    for g in ["f", "i", "C", "o"]
                ]
            )
        )
        lr_check = StabilityChecker.check_learning_rate(
            learning_rate, combined_grad_norm, combined_param_norm
        )
        stability_issues.append(lr_check)

        # æ˜¾ç¤ºè¯Šæ–­ç»“æœ
        StabilityChecker.display_issues(
            stability_issues, title="ğŸ”¬ LSTMç¥ç»å…ƒç¨³å®šæ€§è¯Šæ–­"
        )

        # å‚æ•°æ›´æ–°
        st.markdown("---")
        st.subheader("ğŸ“Š å‚æ•°æ›´æ–°")

        old_W_f = neuron.W_f.copy()
        old_W_i = neuron.W_i.copy()
        old_W_C = neuron.W_C.copy()
        old_W_o = neuron.W_o.copy()

        neuron.update_parameters(learning_rate)

        st.success("âœ… å‚æ•°å·²æ›´æ–°")

        change_f = np.linalg.norm(neuron.W_f - old_W_f)
        change_i = np.linalg.norm(neuron.W_i - old_W_i)
        change_C = np.linalg.norm(neuron.W_C - old_W_C)
        change_o = np.linalg.norm(neuron.W_o - old_W_o)

        st.write("**å„é—¨æƒé‡å˜åŒ–é‡ï¼š**")
        changes_data = [
            {"é—¨": "é—å¿˜é—¨", "å˜åŒ–é‡": f"{change_f:.6f}"},
            {"é—¨": "è¾“å…¥é—¨", "å˜åŒ–é‡": f"{change_i:.6f}"},
            {"é—¨": "å€™é€‰å€¼", "å˜åŒ–é‡": f"{change_C:.6f}"},
            {"é—¨": "è¾“å‡ºé—¨", "å˜åŒ–é‡": f"{change_o:.6f}"},
        ]
        changes_df = pd.DataFrame(changes_data)
        st.markdown(changes_df.to_markdown(index=False))

        st.info(
            f"""
        ğŸ’¡ **LSTM ç‰¹æ€§**ï¼š
        - **é—¨æ§æœºåˆ¶**: ç²¾ç¡®æ§åˆ¶ä¿¡æ¯æµåŠ¨
        - **ç»†èƒçŠ¶æ€**: é•¿æœŸè®°å¿†é€šé“ï¼Œæ¢¯åº¦å¯ä»¥ç›´æ¥æµåŠ¨
        - **è§£å†³æ¢¯åº¦æ¶ˆå¤±**: é€šè¿‡åŠ æ³•æ“ä½œè€Œéä¹˜æ³•ï¼Œæ¢¯åº¦æ›´å®¹æ˜“å›ä¼ 
        - **å‚æ•°é‡**: æ˜¯æ™®é€šRNNçš„4å€ï¼Œä½†æ•ˆæœæ˜¾è‘—æå‡
        - **åº”ç”¨**: æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬ç”Ÿæˆ
        
        **æ€»å‚æ•°é‡**: {total_params} ä¸ª
        """
        )


def render_attention_neuron():
    """æ¸²æŸ“æ³¨æ„åŠ›æœºåˆ¶ç¥ç»å…ƒæ¼”ç¤º"""
    st.subheader("ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰")

    st.markdown(
        """
    **å·¥ä½œåŸç†**ï¼š
    1. Queryï¼ˆæŸ¥è¯¢ï¼‰ã€Keyï¼ˆé”®ï¼‰ã€Valueï¼ˆå€¼ï¼‰ä¸‰ä¸ªè§’è‰²
    2. è®¡ç®—Queryä¸æ¯ä¸ªKeyçš„ç›¸ä¼¼åº¦ï¼ˆæ³¨æ„åŠ›åˆ†æ•°ï¼‰
    3. ç”¨Softmaxå½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡
    4. å¯¹Valueè¿›è¡ŒåŠ æƒæ±‚å’Œ
    
    **æ•°å­¦è¡¨è¾¾**ï¼š
    $$
    \\begin{aligned}
    Q &= x_q \\cdot W_Q \\\\
    K_i &= x_{k,i} \\cdot W_K \\\\
    V_i &= x_{v,i} \\cdot W_V \\\\
    \\text{score}_i &= \\frac{Q \\cdot K_i^T}{\\sqrt{d_k}} \\\\
    \\alpha_i &= \\frac{\\exp(\\text{score}_i)}{\\sum_j \\exp(\\text{score}_j)} \\\\
    \\text{output} &= \\sum_i \\alpha_i \\cdot V_i
    \\end{aligned}
    $$
    """
    )

    st.markdown("---")
    st.subheader("âš™ï¸ é…ç½®æ³¨æ„åŠ›æœºåˆ¶")

    col1, col2 = st.columns(2)

    with col1:
        input_size = st.slider(
            "è¾“å…¥ç»´åº¦", 2, 5, 3, help="è¾“å…¥å‘é‡çš„ç»´åº¦", key="attn_input_size"
        )
        d_model = st.slider(
            "æ¨¡å‹ç»´åº¦", 2, 5, 3, help="Qã€Kã€Vçš„ç»´åº¦", key="attn_d_model"
        )

    with col2:
        num_keys = st.slider(
            "Key/Valueæ•°é‡", 2, 5, 3, help="è¦æ³¨æ„çš„ä½ç½®æ•°é‡", key="attn_num_keys"
        )
        seed = st.number_input(
            "éšæœºç§å­", 0, 100, 42, help="ç”¨äºåˆå§‹åŒ–æƒé‡", key="attn_seed"
        )
        learning_rate = st.slider(
            "å­¦ä¹ ç‡", 0.001, 0.5, 0.01, 0.001, help="æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿", key="attn_lr"
        )

    # åˆ›å»ºæ³¨æ„åŠ›ç¥ç»å…ƒ
    neuron = AttentionNeuron(input_size=input_size, d_model=d_model, seed=seed)

    st.markdown("---")

    # å‚æ•°é‡åˆ†æ
    st.subheader("ğŸ¯ å‚æ•°é‡åˆ†æ")
    total_params = 3 * d_model * input_size

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("W_Q å‚æ•°", f"{d_model * input_size}")
    with col2:
        st.metric("W_K å‚æ•°", f"{d_model * input_size}")
    with col3:
        st.metric("W_V å‚æ•°", f"{d_model * input_size}")

    st.info(
        f"""
    ğŸ’¡ **æ³¨æ„åŠ›æœºåˆ¶çš„å‚æ•°**ï¼š
    - $W_Q$: å°†è¾“å…¥æŠ•å½±åˆ°Queryç©ºé—´
    - $W_K$: å°†è¾“å…¥æŠ•å½±åˆ°Keyç©ºé—´
    - $W_V$: å°†è¾“å…¥æŠ•å½±åˆ°Valueç©ºé—´
    - **æ€»å‚æ•°é‡**: {total_params}
    - **æ³¨æ„åŠ›æƒé‡**æ˜¯åŠ¨æ€è®¡ç®—çš„ï¼Œä¸æ˜¯å›ºå®šå‚æ•°ï¼
    """
    )

    st.markdown("---")

    # è¾“å…¥æ•°æ®
    st.subheader("ğŸ“¥ è¾“å…¥æ•°æ®")

    st.write("**Queryï¼ˆæŸ¥è¯¢å‘é‡ï¼‰ï¼š**")
    query_cols = st.columns(input_size)
    query_data = []
    for i in range(input_size):
        with query_cols[i]:
            val = st.number_input(
                f"$q[{i}]$",
                value=float(np.random.randn() * 0.5),
                format="%.4f",
                key=f"attn_query_{i}",
            )
            query_data.append(val)
    query_data = np.array(query_data)

    st.write(f"**Keys & Valuesï¼ˆ{num_keys}ä¸ªé”®å€¼å¯¹ï¼‰ï¼š**")
    keys_data = []
    values_data = []

    for k in range(num_keys):
        st.write(f"*ä½ç½® {k}:*")
        key_cols = st.columns(input_size)
        key_row = []
        for i in range(input_size):
            with key_cols[i]:
                val = st.number_input(
                    f"$k_{k}[{i}]$",
                    value=float(np.random.randn() * 0.5),
                    format="%.4f",
                    key=f"attn_key_{k}_{i}",
                )
                key_row.append(val)
        keys_data.append(np.array(key_row))

        # Valueä½¿ç”¨ä¸Keyç›¸åŒçš„æ•°æ®ï¼ˆç®€åŒ–ï¼‰
        value_cols = st.columns(input_size)
        value_row = []
        for i in range(input_size):
            with value_cols[i]:
                val = st.number_input(
                    f"$v_{k}[{i}]$",
                    value=float(np.random.randn() * 0.5),
                    format="%.4f",
                    key=f"attn_value_{k}_{i}",
                )
                value_row.append(val)
        values_data.append(np.array(value_row))

    keys_data = np.array(keys_data)
    values_data = np.array(values_data)

    st.markdown("---")

    # æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—
    if st.button("ğŸš€ è®¡ç®—æ³¨æ„åŠ›", type="primary", key="attn_compute"):
        st.subheader("â¡ï¸ 1. å‰å‘ä¼ æ’­")

        output, attention_weights = neuron.forward(query_data, keys_data, values_data)

        st.success(f"âœ… æ³¨æ„åŠ›è¾“å‡º: {output}")

        # è¯¦ç»†è®¡ç®—æ­¥éª¤
        with st.expander("ğŸ“‹ è¯¦ç»†è®¡ç®—æ­¥éª¤", expanded=True):
            st.write("**æ­¥éª¤1: æŠ•å½±åˆ°Qã€Kã€Vç©ºé—´**")
            st.write(f"Query: $Q = W_Q \\cdot q$")
            st.write(f"$Q = {neuron.forward_history['Q']}$")
            st.write(f"Keys: $K_i = W_K \\cdot k_i$")
            for i, k in enumerate(neuron.forward_history["K"]):
                st.write(f"$K_{i} = {k}$")

            st.write("**æ­¥éª¤2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**")
            st.write(
                f"$\\text{{score}}_i = \\frac{{Q \\cdot K_i^T}}{{\\sqrt{{{d_model}}}}}$"
            )
            scores_data = []
            for i, score in enumerate(neuron.forward_history["scores"]):
                scores_data.append(
                    {
                        "ä½ç½®": f"{i}",
                        "åŸå§‹åˆ†æ•°": f"{score * np.sqrt(d_model):.4f}",
                        "ç¼©æ”¾å": f"{score:.4f}",
                    }
                )
            scores_df = pd.DataFrame(scores_data)
            st.markdown(scores_df.to_markdown(index=False))

            st.write("**æ­¥éª¤3: Softmaxå½’ä¸€åŒ–**")
            attn_data = []
            for i, weight in enumerate(attention_weights):
                attn_data.append(
                    {
                        "ä½ç½®": f"{i}",
                        "æ³¨æ„åŠ›æƒé‡": f"{weight:.4f}",
                        "ç™¾åˆ†æ¯”": f"{weight*100:.2f}%",
                    }
                )
            attn_df = pd.DataFrame(attn_data)
            st.markdown(attn_df.to_markdown(index=False))
            st.write(f"æƒé‡å’Œ: {np.sum(attention_weights):.6f} (åº”è¯¥=1.0)")

            st.write("**æ­¥éª¤4: åŠ æƒæ±‚å’ŒValue**")
            st.write(f"$\\text{{output}} = \\sum_i \\alpha_i \\cdot V_i$")
            for i, (w, v) in enumerate(
                zip(attention_weights, neuron.forward_history["V"])
            ):
                st.write(f"$\\alpha_{i} \\cdot V_{i} = {w:.4f} \\times {v} = {w * v}$")
            st.write(f"**æœ€ç»ˆè¾“å‡º**: {output}")

        # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
        with st.expander("ğŸ“Š æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–", expanded=True):
            fig = go.Figure()

            # æ¡å½¢å›¾
            fig.add_trace(
                go.Bar(
                    x=[f"ä½ç½®{i}" for i in range(num_keys)],
                    y=attention_weights,
                    text=[f"{w:.3f}" for w in attention_weights],
                    textposition="auto",
                    marker=dict(
                        color=attention_weights,
                        colorscale="Viridis",
                        showscale=True,
                        colorbar=dict(title="æƒé‡"),
                    ),
                )
            )

            fig.update_layout(
                title="æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ",
                xaxis_title="Key/Valueä½ç½®",
                yaxis_title="æ³¨æ„åŠ›æƒé‡",
                height=400,
                yaxis=dict(range=[0, 1]),
            )
            st.plotly_chart(fig, use_container_width=True)

            # çƒ­åŠ›å›¾ï¼šå±•ç¤ºQueryä¸æ¯ä¸ªKeyçš„ç›¸ä¼¼åº¦
            fig2 = go.Figure(
                data=go.Heatmap(
                    z=[attention_weights],
                    x=[f"K{i}" for i in range(num_keys)],
                    y=["Query"],
                    colorscale="RdYlGn",
                    text=[[f"{w:.3f}" for w in attention_weights]],
                    texttemplate="%{text}",
                    textfont={"size": 16},
                    colorbar=dict(title="æ³¨æ„åŠ›"),
                )
            )

            fig2.update_layout(title="Queryå¯¹å„Keyçš„æ³¨æ„åŠ›", height=200)
            st.plotly_chart(fig2, use_container_width=True)

        # åå‘ä¼ æ’­
        st.markdown("---")
        st.subheader("â¬…ï¸ 2. åå‘ä¼ æ’­")

        upstream_grad = np.ones(d_model)
        st.write(f"ä¸Šæ¸¸æ¢¯åº¦: {upstream_grad}")

        gradients = neuron.backward(upstream_grad)

        st.success("âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ")

        with st.expander("ğŸ“‹ æ¢¯åº¦ä¿¡æ¯", expanded=True):
            grad_info = [
                {
                    "å‚æ•°": "W_Q",
                    "å½¢çŠ¶": f"{neuron.W_Q.shape}",
                    "æ¢¯åº¦èŒƒæ•°": f'{np.linalg.norm(gradients["grad_W_Q"]):.6f}',
                },
                {
                    "å‚æ•°": "W_K",
                    "å½¢çŠ¶": f"{neuron.W_K.shape}",
                    "æ¢¯åº¦èŒƒæ•°": f'{np.linalg.norm(gradients["grad_W_K"]):.6f}',
                },
                {
                    "å‚æ•°": "W_V",
                    "å½¢çŠ¶": f"{neuron.W_V.shape}",
                    "æ¢¯åº¦èŒƒæ•°": f'{np.linalg.norm(gradients["grad_W_V"]):.6f}',
                },
            ]
            grad_df = pd.DataFrame(grad_info)
            st.markdown(grad_df.to_markdown(index=False))

        # å‚æ•°æ›´æ–°
        st.markdown("---")
        st.subheader("ğŸ“Š 3. å‚æ•°æ›´æ–°")

        old_W_Q = neuron.W_Q.copy()
        old_W_K = neuron.W_K.copy()
        old_W_V = neuron.W_V.copy()

        neuron.update_parameters(learning_rate)

        st.success("âœ… å‚æ•°å·²æ›´æ–°")

        change_Q = np.linalg.norm(neuron.W_Q - old_W_Q)
        change_K = np.linalg.norm(neuron.W_K - old_W_K)
        change_V = np.linalg.norm(neuron.W_V - old_W_V)

        changes_data = [
            {"å‚æ•°": "W_Q", "å˜åŒ–é‡": f"{change_Q:.6f}"},
            {"å‚æ•°": "W_K", "å˜åŒ–é‡": f"{change_K:.6f}"},
            {"å‚æ•°": "W_V", "å˜åŒ–é‡": f"{change_V:.6f}"},
        ]
        changes_df = pd.DataFrame(changes_data)
        st.markdown(changes_df.to_markdown(index=False))

        # ==================== æ•°å€¼ç¨³å®šæ€§æ£€æµ‹ ====================
        st.markdown("---")
        st.subheader("ğŸ”¬ æ•°å€¼ç¨³å®šæ€§è¯Šæ–­")

        st.info("ğŸ’¡ Attentionç‰¹æœ‰é—®é¢˜ï¼šæ³¨æ„åŠ›æƒé‡åˆ†å¸ƒã€Softmaxæº¢å‡ºã€Query-Keyç›¸ä¼¼åº¦")

        stability_issues = []

        # 1. æ£€æŸ¥æ¢¯åº¦
        grad_Q_check = StabilityChecker.check_gradient(
            gradients["grad_W_Q"].flatten(), "W_Qæ¢¯åº¦"
        )
        stability_issues.append(grad_Q_check)

        grad_K_check = StabilityChecker.check_gradient(
            gradients["grad_W_K"].flatten(), "W_Kæ¢¯åº¦"
        )
        stability_issues.append(grad_K_check)

        grad_V_check = StabilityChecker.check_gradient(
            gradients["grad_W_V"].flatten(), "W_Væ¢¯åº¦"
        )
        stability_issues.append(grad_V_check)

        # 2. æ£€æŸ¥æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒï¼ˆå…³é”®ï¼ï¼‰
        attn_entropy = -np.sum(attention_weights * np.log(attention_weights + 1e-10))
        max_attn_weight = np.max(attention_weights)

        if max_attn_weight > 0.9:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "æ³¨æ„åŠ›æƒé‡è¿‡åº¦é›†ä¸­",
                    "value": f"{max_attn_weight:.4f}",
                    "threshold": "> 0.9",
                    "icon": "ğŸŸ¡",
                    "severity": "medium",
                    "details": {
                        "æœ€å¤§æƒé‡": f"{max_attn_weight:.4f}",
                        "ç†µå€¼": f"{attn_entropy:.4f}",
                        "æƒé‡åˆ†å¸ƒ": ", ".join([f"{w:.3f}" for w in attention_weights]),
                    },
                    "solution": [
                        "æ£€æŸ¥Queryå’ŒKeyçš„åˆå§‹åŒ–",
                        "ä½¿ç”¨temperatureç¼©æ”¾",
                        "æ·»åŠ attention dropout",
                        "æ£€æŸ¥è¾“å…¥æ˜¯å¦è¿‡äºç›¸ä¼¼",
                    ],
                    "explanation": "æ³¨æ„åŠ›æƒé‡è¿‡åº¦é›†ä¸­åœ¨æŸä¸ªä½ç½®ï¼Œå¯èƒ½å¯¼è‡´ä¿¡æ¯ç“¶é¢ˆ",
                }
            )
        elif attn_entropy < 0.5:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "æ³¨æ„åŠ›ç†µå€¼è¿‡ä½",
                    "value": f"{attn_entropy:.4f}",
                    "threshold": "< 0.5",
                    "icon": "ğŸŸ¡",
                    "severity": "low",
                    "details": {
                        "ç†µå€¼": f"{attn_entropy:.4f}",
                        "æœ€å¤§æƒé‡": f"{max_attn_weight:.4f}",
                        "æƒé‡åˆ†å¸ƒ": ", ".join([f"{w:.3f}" for w in attention_weights]),
                    },
                    "solution": [
                        "å¯èƒ½æ˜¯æ­£å¸¸ç°è±¡ï¼ˆæŸä¸ªä½ç½®ç¡®å®æœ€é‡è¦ï¼‰",
                        "å¦‚æœæ€»æ˜¯å¦‚æ­¤ï¼Œæ£€æŸ¥æ¨¡å‹è®¾è®¡",
                        "è€ƒè™‘ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›",
                    ],
                    "explanation": "æ³¨æ„åŠ›åˆ†å¸ƒçš„ä¿¡æ¯ç†µè¾ƒä½ï¼Œå…³æ³¨è¾ƒä¸ºé›†ä¸­",
                }
            )
        else:
            stability_issues.append(
                {
                    "status": "success",
                    "type": "æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ",
                    "value": f"ç†µ={attn_entropy:.4f}",
                    "icon": "ğŸŸ¢",
                    "severity": "none",
                    "details": {
                        "ç†µå€¼": f"{attn_entropy:.4f}",
                        "æœ€å¤§æƒé‡": f"{max_attn_weight:.4f}",
                        "æƒé‡å’Œ": f"{np.sum(attention_weights):.6f}",
                    },
                }
            )

        # 3. æ£€æŸ¥attention scores (softmaxå‰)
        scores = neuron.forward_history["scores"]
        score_max = np.max(np.abs(scores))

        if score_max > 10:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "Attention Scoreè¿‡å¤§",
                    "value": f"{score_max:.2f}",
                    "threshold": "> 10",
                    "icon": "ğŸŸ¡",
                    "severity": "medium",
                    "details": {
                        "æœ€å¤§score": f"{score_max:.2f}",
                        "ScoreèŒƒå›´": f"[{np.min(scores):.2f}, {np.max(scores):.2f}]",
                        "ç¼©æ”¾å› å­": f"âˆš{d_model} = {np.sqrt(d_model):.2f}",
                    },
                    "solution": [
                        "æ£€æŸ¥Queryå’ŒKeyçš„èŒƒæ•°",
                        "å¢å¤§ç¼©æ”¾å› å­ï¼ˆâˆšd_kï¼‰",
                        "ä½¿ç”¨LayerNorm",
                        "æ£€æŸ¥è¾“å…¥æ˜¯å¦å·²å½’ä¸€åŒ–",
                    ],
                    "explanation": "Scoreè¿‡å¤§ä¼šå¯¼è‡´softmaxé¥±å’Œï¼Œæ¢¯åº¦æ¶ˆå¤±",
                }
            )

        # 4. æ£€æŸ¥Q, K, Vçš„èŒƒæ•°
        Q_norm = np.linalg.norm(neuron.forward_history["Q"])
        K_norm = np.linalg.norm(neuron.forward_history["K"])
        V_norm = np.linalg.norm(neuron.forward_history["V"])

        if Q_norm > 10 or K_norm > 10 or V_norm > 10:
            stability_issues.append(
                {
                    "status": "warning",
                    "type": "Q/K/VèŒƒæ•°è¿‡å¤§",
                    "value": f"max={max(Q_norm, K_norm, V_norm):.2f}",
                    "threshold": "> 10",
                    "icon": "ğŸŸ¡",
                    "severity": "medium",
                    "details": {
                        "QèŒƒæ•°": f"{Q_norm:.4f}",
                        "KèŒƒæ•°": f"{K_norm:.4f}",
                        "VèŒƒæ•°": f"{V_norm:.4f}",
                    },
                    "solution": [
                        "ä½¿ç”¨Xavieråˆå§‹åŒ–",
                        "æ·»åŠ LayerNormåœ¨æŠ•å½±å",
                        "æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´",
                    ],
                    "explanation": "Q/K/VèŒƒæ•°è¿‡å¤§ä¼šå¯¼è‡´attention scoreçˆ†ç‚¸",
                }
            )
        else:
            stability_issues.append(
                {
                    "status": "success",
                    "type": "Q/K/VèŒƒæ•°",
                    "value": f"Q={Q_norm:.2f}, K={K_norm:.2f}, V={V_norm:.2f}",
                    "icon": "ğŸŸ¢",
                    "severity": "none",
                }
            )

        # 5. æ£€æŸ¥è¾“å‡º
        output_check = StabilityChecker.check_activation(output, "Attentionè¾“å‡º")
        stability_issues.append(output_check)

        # 6. æ£€æŸ¥å­¦ä¹ ç‡
        combined_param_norm = np.sqrt(
            np.linalg.norm(neuron.W_Q) ** 2
            + np.linalg.norm(neuron.W_K) ** 2
            + np.linalg.norm(neuron.W_V) ** 2
        )
        combined_grad_norm = np.sqrt(
            np.linalg.norm(gradients["grad_W_Q"]) ** 2
            + np.linalg.norm(gradients["grad_W_K"]) ** 2
            + np.linalg.norm(gradients["grad_W_V"]) ** 2
        )
        lr_check = StabilityChecker.check_learning_rate(
            learning_rate, combined_grad_norm, combined_param_norm
        )
        stability_issues.append(lr_check)

        # æ˜¾ç¤ºè¯Šæ–­ç»“æœ
        StabilityChecker.display_issues(
            stability_issues, title="ğŸ”¬ Attentionç¥ç»å…ƒç¨³å®šæ€§è¯Šæ–­"
        )

        st.markdown("---")
        st.info(
            f"""
        ğŸ’¡ **æ³¨æ„åŠ›æœºåˆ¶ç‰¹æ€§ä¸ç¨³å®šæ€§**ï¼š
        
        **åŠ¨æ€æƒé‡**: æ³¨æ„åŠ›æƒé‡æ ¹æ®è¾“å…¥åŠ¨æ€è®¡ç®—ï¼Œä¸æ˜¯å›ºå®šå‚æ•°
        - å½“å‰åˆ†å¸ƒç†µ: {attn_entropy:.4f}
        - æœ€å¤§æƒé‡: {max_attn_weight:.4f}
        
        **é€‰æ‹©æ€§å…³æ³¨**: è‡ªåŠ¨å­¦ä¹ å…³æ³¨é‡è¦çš„ä½ç½®
        - QèŒƒæ•°: {Q_norm:.4f}
        - KèŒƒæ•°: {K_norm:.4f}
        - VèŒƒæ•°: {V_norm:.4f}
        
        **å¯è§£é‡Šæ€§**: æ³¨æ„åŠ›æƒé‡å¯ä»¥å¯è§†åŒ–ï¼Œäº†è§£æ¨¡å‹åœ¨"çœ‹"ä»€ä¹ˆ
        - æƒé‡å’Œ: {np.sum(attention_weights):.6f} (åº”è¯¥=1.0)
        
        **æ— è·ç¦»é™åˆ¶**: å¯ä»¥å…³æ³¨ä»»æ„ä½ç½®ï¼Œä¸å—è·ç¦»å½±å“
        - ScoreèŒƒå›´: [{np.min(scores):.2f}, {np.max(scores):.2f}]
        - ç¼©æ”¾å› å­: âˆš{d_model} = {np.sqrt(d_model):.2f}
        
        **åº”ç”¨**: Transformerã€BERTã€GPTç­‰ç°ä»£NLPæ¨¡å‹çš„æ ¸å¿ƒ
        
        **æ€»å‚æ•°é‡**: {total_params} ä¸ªï¼ˆä¸åŒ…æ‹¬åŠ¨æ€è®¡ç®—çš„æ³¨æ„åŠ›æƒé‡ï¼‰
        
        **å…³é”®æ´å¯Ÿ**: Queryé—®"æˆ‘æƒ³æ‰¾ä»€ä¹ˆ"ï¼ŒKeyå›ç­”"æˆ‘æ˜¯ä»€ä¹ˆ"ï¼Œç›¸ä¼¼åº¦é«˜çš„Keyå¯¹åº”çš„Valueä¼šè¢«æ›´å¤šå…³æ³¨ï¼
        
        **Scaled Dot-Product Attention**:
        - Score = QÂ·K^T / âˆšd_k
        - ç¼©æ”¾çš„ç›®çš„ï¼šé˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´softmaxé¥±å’Œ
        - å½“d_kå¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯å€¼æ–¹å·®ä¸ºd_kï¼Œç¼©æ”¾åæ–¹å·®ä¸º1
        """
        )


def single_neuron_tab(CHINESE_SUPPORTED=True):
    """
    å•ç¥ç»å…ƒå¯è§†åŒ–æ ‡ç­¾é¡µ - ä¸»UIç•Œé¢
    """

    if CHINESE_SUPPORTED:
        st.header("ğŸ§¬ å•ç¥ç»å…ƒï¼šç†è§£ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒ")

        st.markdown(
            """
        ### ğŸ’¡ æ ¸å¿ƒæ€æƒ³
        
        ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„**æœ€å°è®¡ç®—å•å…ƒ**ã€‚ç†è§£å•ä¸ªç¥ç»å…ƒå¦‚ä½•å·¥ä½œï¼Œå°±èƒ½ç†è§£æ•´ä¸ªç¥ç»ç½‘ç»œçš„è¿ä½œåŸç†ã€‚
        
        **é€‰æ‹©ä¸åŒç±»å‹çš„ç¥ç»å…ƒï¼Œå®é™…ä½“éªŒå®ƒä»¬çš„å·¥ä½œæ–¹å¼ï¼**
        """
        )

        st.markdown("---")

        # ç¥ç»å…ƒç±»å‹é€‰æ‹©
        st.subheader("ğŸ¯ é€‰æ‹©ç¥ç»å…ƒç±»å‹")

        neuron_type = st.selectbox(
            "é€‰æ‹©è¦æ¢ç´¢çš„ç¥ç»å…ƒç±»å‹",
            [
                "å…¨è¿æ¥ç¥ç»å…ƒ (Dense/FC)",
                "å·ç§¯ç¥ç»å…ƒ (Conv)",
                "å¾ªç¯ç¥ç»å…ƒ (RNN)",
                "GRUç¥ç»å…ƒ",
                "LSTMç¥ç»å…ƒ",
                "æ³¨æ„åŠ›æœºåˆ¶ (Attention)",
            ],
            help="ä¸åŒç±»å‹çš„ç¥ç»å…ƒé€‚ç”¨äºä¸åŒçš„ä»»åŠ¡",
        )

        st.markdown("---")

        # æ ¹æ®ç¥ç»å…ƒç±»å‹æ˜¾ç¤ºä¸åŒçš„é…ç½®å’Œæ¼”ç¤º
        if neuron_type == "å…¨è¿æ¥ç¥ç»å…ƒ (Dense/FC)":
            render_dense_neuron()
        elif neuron_type == "å·ç§¯ç¥ç»å…ƒ (Conv)":
            render_conv_neuron()
        elif neuron_type == "å¾ªç¯ç¥ç»å…ƒ (RNN)":
            render_rnn_neuron()
        elif neuron_type == "GRUç¥ç»å…ƒ":
            render_gru_neuron()
        elif neuron_type == "LSTMç¥ç»å…ƒ":
            render_lstm_neuron()
        elif neuron_type == "æ³¨æ„åŠ›æœºåˆ¶ (Attention)":
            render_attention_neuron()

        # æ•™å­¦è¯´æ˜
        st.markdown("---")
        with st.expander("ğŸ“š è¯¦ç»†è¯´æ˜ï¼šç¥ç»å…ƒå¦‚ä½•å·¥ä½œ", expanded=False):
            st.markdown(
                """
            ### ğŸ” æ·±å…¥ç†è§£
            
            #### 1. å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰
            
            ç¥ç»å…ƒæ¥æ”¶è¾“å…¥åï¼Œæ‰§è¡Œä¸¤æ­¥è®¡ç®—ï¼š
            
            **æ­¥éª¤1: çº¿æ€§ç»„åˆï¼ˆåŠ æƒå’Œï¼‰**
            - æ¯ä¸ªè¾“å…¥ $x_i$ ä¹˜ä»¥å¯¹åº”çš„æƒé‡ $w_i$
            - å°†æ‰€æœ‰ä¹˜ç§¯ç›¸åŠ ï¼Œå†åŠ ä¸Šåç½® $b$
            - ç»“æœï¼š$z = w_0 x_0 + w_1 x_1 + ... + w_n x_n + b$
            
            **æ­¥éª¤2: éçº¿æ€§æ¿€æ´»**
            - å°† $z$ é€šè¿‡æ¿€æ´»å‡½æ•°è½¬æ¢
            - æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œè®©ç½‘ç»œèƒ½å­¦ä¹ å¤æ‚æ¨¡å¼
            - å¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼š
                - **ReLU**: $f(z) = \\max(0, z)$ - ç®€å•æœ‰æ•ˆï¼Œæœ€å¸¸ç”¨
                - **Sigmoid**: $f(z) = \\frac{1}{1+e^{-z}}$ - è¾“å‡º0åˆ°1ï¼Œç”¨äºæ¦‚ç‡
                - **Tanh**: $f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ - è¾“å‡º-1åˆ°1ï¼Œé›¶ä¸­å¿ƒåŒ–
            
            #### 2. åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰
            
            ä½¿ç”¨**é“¾å¼æ³•åˆ™**è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼š
            
            **æ ¸å¿ƒæ€æƒ³**: ä»è¾“å‡ºå¾€å›ä¼ æ’­è¯¯å·®
            
            - **ä¸Šæ¸¸æ¢¯åº¦** $\\frac{\\partial L}{\\partial y}$: æ¥è‡ªæŸå¤±å‡½æ•°æˆ–ä¸‹ä¸€å±‚
            - **æ¿€æ´»å‡½æ•°å¯¼æ•°** $\\frac{\\partial y}{\\partial z}$: æ¿€æ´»å‡½æ•°åœ¨å½“å‰ç‚¹çš„æ–œç‡
            - **å±€éƒ¨æ¢¯åº¦** $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z}$
            - **å‚æ•°æ¢¯åº¦**: 
                - $\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial z} \\cdot x_i$
                - $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z}$
            
            #### 3. å‚æ•°æ›´æ–°ï¼ˆParameter Updateï¼‰
            
            ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å‚æ•°ï¼š
            
            $$
            w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial w}
            $$
            
            - $\\alpha$ æ˜¯å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ›´æ–°æ­¥é•¿
            - æ¢¯åº¦æŒ‡å‘è¯¯å·®å¢åŠ çš„æ–¹å‘ï¼Œæ‰€ä»¥è¦å‡å»æ¢¯åº¦
            - è¿­ä»£å¤šæ¬¡åï¼Œå‚æ•°ä¼šé€æ¸ä¼˜åŒ–
            
            #### 4. å…³é”®æ´å¯Ÿ
            
            - **æƒé‡**å†³å®šæ¯ä¸ªè¾“å…¥çš„é‡è¦æ€§
            - **åç½®**è°ƒæ•´ç¥ç»å…ƒçš„æ¿€æ´»é˜ˆå€¼
            - **æ¿€æ´»å‡½æ•°**å¼•å…¥éçº¿æ€§ï¼Œæ˜¯æ·±åº¦å­¦ä¹ çš„å…³é”®
            - **æ¢¯åº¦**æŒ‡ç¤ºå‚æ•°åº”è¯¥å¦‚ä½•è°ƒæ•´
            - **å­¦ä¹ ç‡**æ§åˆ¶å­¦ä¹ é€Ÿåº¦ï¼ˆè¿‡å¤§éœ‡è¡ï¼Œè¿‡å°ç¼“æ…¢ï¼‰
            
            #### 5. ä»å•ç¥ç»å…ƒåˆ°ç¥ç»ç½‘ç»œ
            
            - å¤šä¸ªç¥ç»å…ƒå¹¶è¡Œ â†’ **å±‚ï¼ˆLayerï¼‰**
            - å¤šä¸ªå±‚å †å  â†’ **æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰**
            - æ¯ä¸€å±‚åšç±»ä¼¼çš„è®¡ç®—ï¼š$\\text{output} = \\text{activation}(W \\cdot \\text{input} + b)$
            - é€šè¿‡åå‘ä¼ æ’­ï¼Œæ‰€æœ‰å±‚çš„å‚æ•°åŒæ—¶æ›´æ–°
            
            **ç†è§£å•ä¸ªç¥ç»å…ƒ = ç†è§£æ•´ä¸ªç¥ç»ç½‘ç»œï¼**
            """
            )

        # ä¸åŒç±»å‹ç¥ç»ç½‘ç»œçš„ç¥ç»å…ƒå¯¹æ¯”
        st.markdown("---")
        with st.expander("ğŸ”¬ æ‰©å±•ï¼šä¸åŒç±»å‹ç¥ç»ç½‘ç»œçš„ç¥ç»å…ƒå·¥ä½œåŸç†", expanded=False):
            st.markdown(
                """
            ### ğŸ¯ æ ¸å¿ƒé—®é¢˜ï¼šå…¶ä»–ç±»å‹çš„ç¥ç»ç½‘ç»œç¥ç»å…ƒä¹Ÿæ˜¯è¿™æ ·å·¥ä½œçš„å—ï¼Ÿ
            
            **ç­”æ¡ˆï¼šåŸºæœ¬åŸç†ç›¸åŒï¼Œä½†å…·ä½“å®ç°æœ‰æ‰€ä¸åŒï¼**
            
            æ‰€æœ‰ç¥ç»ç½‘ç»œéƒ½éµå¾ªç›¸åŒçš„ä¸‰æ­¥æµç¨‹ï¼š**å‰å‘ä¼ æ’­ â†’ åå‘ä¼ æ’­ â†’ å‚æ•°æ›´æ–°**ï¼Œä½†ä¸åŒç±»å‹çš„ç¥ç»ç½‘ç»œåœ¨"å¦‚ä½•è®¡ç®—"ä¸Šæœ‰æ‰€å·®å¼‚ã€‚
            
            ---
            
            ### 1ï¸âƒ£ **å…¨è¿æ¥ç¥ç»å…ƒï¼ˆFully Connected / Denseï¼‰** - æœ¬é¡µæ¼”ç¤º
            
            **ç‰¹ç‚¹**ï¼šæ¯ä¸ªè¾“å…¥éƒ½ä¸æ¯ä¸ªè¾“å‡ºç›¸è¿
            
            **å‰å‘ä¼ æ’­**ï¼š
            $$
            y = \\text{activation}(w^T x + b)
            $$
            
            **å‚æ•°**ï¼š
            - æƒé‡çŸ©é˜µ $W$ï¼šå¤§å°ä¸º $\\text{output\\_size} \\times \\text{input\\_size}$
            - åç½®å‘é‡ $b$ï¼šå¤§å°ä¸º $\\text{output\\_size}$
            
            **ä¼˜ç‚¹**ï¼šè¡¨è¾¾èƒ½åŠ›å¼ºï¼Œå¯ä»¥å­¦ä¹ ä»»æ„å‡½æ•°
            
            **ç¼ºç‚¹**ï¼šå‚æ•°é‡å¤§ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œä¸é€‚åˆå¤„ç†å›¾åƒç­‰é«˜ç»´æ•°æ®
            
            **åº”ç”¨**ï¼šMLPã€å…¨è¿æ¥å±‚
            
            ---
            
            ### 2ï¸âƒ£ **å·ç§¯ç¥ç»å…ƒï¼ˆConvolutional Neuronï¼‰** 
            
            **ç‰¹ç‚¹**ï¼šå±€éƒ¨è¿æ¥ + æƒå€¼å…±äº«
            
            **å‰å‘ä¼ æ’­**ï¼š
            $$
            y[i,j] = \\text{activation}\\left(\\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} w[m,n] \\cdot x[i+m, j+n] + b\\right)
            $$
            
            **å…³é”®å·®å¼‚**ï¼š
            - ğŸ” **å±€éƒ¨æ„Ÿå—é‡**ï¼šæ¯ä¸ªç¥ç»å…ƒåªå…³æ³¨è¾“å…¥çš„ä¸€å°å—åŒºåŸŸï¼ˆå¦‚ 3Ã—3 æˆ– 5Ã—5ï¼‰
            - ğŸ”„ **æƒå€¼å…±äº«**ï¼šåŒä¸€ä¸ªå·ç§¯æ ¸åœ¨æ•´ä¸ªè¾“å…¥ä¸Šæ»‘åŠ¨ï¼Œå‚æ•°è¢«å¤ç”¨
            - ğŸ“¦ **å‚æ•°é‡**ï¼šå·ç§¯æ ¸å¤§å° $k \\times k \\times C_{in}$ï¼Œä¸è¾“å…¥å¤§å°æ— å…³ï¼
            
            **ä¾‹å­**ï¼š
            - å…¨è¿æ¥ï¼š28Ã—28 å›¾åƒ â†’ 100 ç¥ç»å…ƒ = 78,400 å‚æ•°
            - å·ç§¯ï¼š3Ã—3 å·ç§¯æ ¸ â†’ 100 ä¸ª = 900 å‚æ•°ï¼ˆå‡å°‘ 87 å€ï¼ï¼‰
            
            **åå‘ä¼ æ’­**ï¼šä½¿ç”¨å·ç§¯çš„è½¬ç½®æ“ä½œï¼ˆè½¬ç½®å·ç§¯ï¼‰
            
            **ä¼˜ç‚¹**ï¼šå‚æ•°å°‘ã€å¹³ç§»ä¸å˜æ€§ã€é€‚åˆå›¾åƒ
            
            **åº”ç”¨**ï¼šCNNã€ResNetã€VGG
            
            ---
            
            ### 3ï¸âƒ£ **å¾ªç¯ç¥ç»å…ƒï¼ˆRecurrent Neuronï¼‰**
            
            **ç‰¹ç‚¹**ï¼šæœ‰è®°å¿†ï¼Œå¤„ç†åºåˆ—æ•°æ®
            
            **å‰å‘ä¼ æ’­**ï¼š
            $$
            h_t = \\text{activation}(W_{hh} h_{t-1} + W_{xh} x_t + b)
            $$
            
            **å…³é”®å·®å¼‚**ï¼š
            - ğŸ” **æ—¶é—´å¾ªç¯**ï¼šå½“å‰çŠ¶æ€ $h_t$ ä¾èµ–äºå‰ä¸€æ—¶åˆ»çŠ¶æ€ $h_{t-1}$
            - ğŸ“ **è®°å¿†æœºåˆ¶**ï¼šéšè—çŠ¶æ€ $h_t$ å­˜å‚¨å†å²ä¿¡æ¯
            - â±ï¸ **æ—¶é—´å±•å¼€**ï¼šåœ¨æ—¶é—´ç»´åº¦ä¸Šå±•å¼€æˆå¤šä¸ªæ—¶é—´æ­¥
            
            **å‚æ•°**ï¼š
            - $W_{xh}$ï¼šè¾“å…¥åˆ°éšè—çš„æƒé‡
            - $W_{hh}$ï¼šéšè—åˆ°éšè—çš„æƒé‡ï¼ˆè®°å¿†æƒé‡ï¼‰
            - $W_{hy}$ï¼šéšè—åˆ°è¾“å‡ºçš„æƒé‡
            
            **åå‘ä¼ æ’­**ï¼šBPTTï¼ˆBackpropagation Through Timeï¼‰- æ²¿æ—¶é—´åå‘ä¼ æ’­
            
            **é—®é¢˜**ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ˆé•¿æœŸä¾èµ–éš¾ä»¥å­¦ä¹ ï¼‰
            
            **åº”ç”¨**ï¼šRNNã€è¯­è¨€æ¨¡å‹ã€æ—¶åºé¢„æµ‹
            
            ---
            
            ### 4ï¸âƒ£ **LSTM ç¥ç»å…ƒï¼ˆLong Short-Term Memoryï¼‰**
            
            **ç‰¹ç‚¹**ï¼šæ”¹è¿›çš„ RNNï¼Œè§£å†³é•¿æœŸä¾èµ–é—®é¢˜
            
            **å‰å‘ä¼ æ’­**ï¼ˆå¤æ‚å¾—å¤šï¼ï¼‰ï¼š
            $$
            \\begin{aligned}
            f_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) && \\text{ï¼ˆé—å¿˜é—¨ï¼‰} \\\\
            i_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) && \\text{ï¼ˆè¾“å…¥é—¨ï¼‰} \\\\
            \\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) && \\text{ï¼ˆå€™é€‰è®°å¿†ï¼‰} \\\\
            C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t && \\text{ï¼ˆæ›´æ–°è®°å¿†ï¼‰} \\\\
            o_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) && \\text{ï¼ˆè¾“å‡ºé—¨ï¼‰} \\\\
            h_t &= o_t \\odot \\tanh(C_t) && \\text{ï¼ˆè¾“å‡ºï¼‰}
            \\end{aligned}
            $$
            
            **å…³é”®å·®å¼‚**ï¼š
            - ğŸšª **é—¨æ§æœºåˆ¶**ï¼š3 ä¸ªé—¨ï¼ˆé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨ï¼‰æ§åˆ¶ä¿¡æ¯æµåŠ¨
            - ğŸ’¾ **ç»†èƒçŠ¶æ€** $C_t$ï¼šé•¿æœŸè®°å¿†ï¼Œå¯ä»¥è·¨è¶Šå¾ˆå¤šæ—¶é—´æ­¥
            - ğŸ›ï¸ **ç²¾ç»†æ§åˆ¶**ï¼šå†³å®šä¿ç•™ä»€ä¹ˆã€å¿˜è®°ä»€ä¹ˆã€è¾“å‡ºä»€ä¹ˆ
            
            **å‚æ•°æ•°é‡**ï¼š4 å€äºæ™®é€š RNNï¼ˆå› ä¸ºæœ‰ 4 ç»„æƒé‡ï¼‰
            
            **åå‘ä¼ æ’­**ï¼šæ¢¯åº¦é€šè¿‡ç»†èƒçŠ¶æ€ç›´æ¥æµåŠ¨ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±
            
            **ä¼˜ç‚¹**ï¼šèƒ½å­¦ä¹ é•¿æœŸä¾èµ–
            
            **åº”ç”¨**ï¼šæœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬ç”Ÿæˆ
            
            ---
            
            ### 5ï¸âƒ£ **æ³¨æ„åŠ›æœºåˆ¶ç¥ç»å…ƒï¼ˆAttention Neuronï¼‰**
            
            **ç‰¹ç‚¹**ï¼šåŠ¨æ€åŠ æƒï¼Œå…³æ³¨é‡è¦ä¿¡æ¯
            
            **å‰å‘ä¼ æ’­**ï¼š
            $$
            \\begin{aligned}
            \\text{score}(h_i, s) &= h_i^T W s && \\text{ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰} \\\\
            \\alpha_i &= \\frac{\\exp(\\text{score}(h_i, s))}{\\sum_j \\exp(\\text{score}(h_j, s))} && \\text{ï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰} \\\\
            c &= \\sum_i \\alpha_i h_i && \\text{ï¼ˆåŠ æƒæ±‚å’Œï¼‰}
            \\end{aligned}
            $$
            
            **å…³é”®å·®å¼‚**ï¼š
            - ğŸ¯ **åŠ¨æ€æƒé‡**ï¼šæ³¨æ„åŠ›æƒé‡ $\\alpha_i$ æ ¹æ®è¾“å…¥åŠ¨æ€è®¡ç®—ï¼ˆä¸æ˜¯å›ºå®šå‚æ•°ï¼‰
            - ğŸ” **é€‰æ‹©æ€§å…³æ³¨**ï¼šè‡ªåŠ¨å­¦ä¹ å…³æ³¨å“ªäº›éƒ¨åˆ†é‡è¦
            - ğŸŒ **å…¨å±€è§†é‡**ï¼šå¯ä»¥å…³æ³¨ä»»æ„ä½ç½®ï¼Œæ²¡æœ‰è·ç¦»é™åˆ¶
            
            **åå‘ä¼ æ’­**ï¼šæ¢¯åº¦åŒæ—¶æµå‘æŸ¥è¯¢ã€é”®ã€å€¼
            
            **ä¼˜ç‚¹**ï¼šæ•è·é•¿è·ç¦»ä¾èµ–ã€å¯è§£é‡Šæ€§å¼º
            
            **åº”ç”¨**ï¼šTransformerã€BERTã€GPT
            
            ---
            
            ### 6ï¸âƒ£ **Transformer è‡ªæ³¨æ„åŠ›ç¥ç»å…ƒï¼ˆSelf-Attentionï¼‰**
            
            **ç‰¹ç‚¹**ï¼šå¹¶è¡Œå¤„ç†ï¼Œæ— åºåˆ—é™åˆ¶
            
            **å‰å‘ä¼ æ’­**ï¼š
            $$
            \\begin{aligned}
            Q &= XW_Q, \\quad K = XW_K, \\quad V = XW_V \\\\
            \\text{Attention}(Q,K,V) &= \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V
            \\end{aligned}
            $$
            
            **å…³é”®å·®å¼‚**ï¼š
            - ğŸ”€ **å®Œå…¨å¹¶è¡Œ**ï¼šæ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—ï¼Œä¸åƒ RNN å¿…é¡»æŒ‰é¡ºåº
            - ğŸ­ **å¤šå¤´æ³¨æ„åŠ›**ï¼šå¤šä¸ªæ³¨æ„åŠ›å¤´å…³æ³¨ä¸åŒæ–¹é¢
            - ğŸ“Š **ä½ç½®ç¼–ç **ï¼šéœ€è¦é¢å¤–æ·»åŠ ä½ç½®ä¿¡æ¯
            
            **å‚æ•°**ï¼š$W_Q, W_K, W_V$ ä¸‰ä¸ªæŠ•å½±çŸ©é˜µ
            
            **åå‘ä¼ æ’­**ï¼šæ¢¯åº¦å¯ä»¥ç›´æ¥æµåŠ¨ï¼Œæ²¡æœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
            
            **ä¼˜ç‚¹**ï¼šå¹¶è¡Œé«˜æ•ˆã€é•¿è·ç¦»ä¾èµ–ã€æ€§èƒ½æœ€å¼º
            
            **åº”ç”¨**ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆGPTã€BERTã€LLaMAï¼‰
            
            ---
            
            ### ğŸ“Š **æ€»ç»“å¯¹æ¯”è¡¨**
            
            | ç¥ç»å…ƒç±»å‹ | è¿æ¥æ–¹å¼ | å‚æ•°å…±äº« | é€‚ç”¨åœºæ™¯ | ä¸»è¦ä¼˜åŠ¿ |
            |:----------|:---------|:---------|:---------|:---------|
            | **å…¨è¿æ¥** | å…¨è¿æ¥ | å¦ | è¡¨æ ¼æ•°æ®ã€MLP | è¡¨è¾¾èƒ½åŠ›å¼º |
            | **å·ç§¯** | å±€éƒ¨è¿æ¥ | æ˜¯ï¼ˆå·ç§¯æ ¸ï¼‰ | å›¾åƒã€ç©ºé—´æ•°æ® | å‚æ•°å°‘ã€å¹³ç§»ä¸å˜ |
            | **RNN** | å¾ªç¯è¿æ¥ | æ˜¯ï¼ˆæ—¶é—´ç»´åº¦ï¼‰ | åºåˆ—ã€æ—¶åºæ•°æ® | å¤„ç†å˜é•¿åºåˆ— |
            | **LSTM** | å¾ªç¯+é—¨æ§ | æ˜¯ï¼ˆæ—¶é—´ç»´åº¦ï¼‰ | é•¿åºåˆ— | é•¿æœŸä¾èµ– |
            | **æ³¨æ„åŠ›** | åŠ¨æ€æƒé‡ | å¦ï¼ˆæƒé‡åŠ¨æ€ï¼‰ | åºåˆ—åˆ°åºåˆ— | é€‰æ‹©æ€§å…³æ³¨ |
            | **Transformer** | è‡ªæ³¨æ„åŠ› | å¦ | NLPã€å¤šæ¨¡æ€ | å¹¶è¡Œã€é•¿è·ç¦» |
            
            ---
            
            ### ğŸ“ **å…³é”®æ´å¯Ÿ**
            
            1. **ç»Ÿä¸€æ¡†æ¶**ï¼šæ‰€æœ‰ç¥ç»å…ƒéƒ½éµå¾ª"å‰å‘ â†’ åå‘ â†’ æ›´æ–°"çš„èŒƒå¼
            
            2. **è®¾è®¡å“²å­¦**ï¼š
               - å…¨è¿æ¥ï¼š**é€šç”¨æ€§** - èƒ½å­¦ä»»ä½•ä¸œè¥¿ï¼Œä½†ä»£ä»·é«˜
               - å·ç§¯ï¼š**å½’çº³åç½®** - åˆ©ç”¨å±€éƒ¨æ€§å’Œå¹³ç§»ä¸å˜æ€§
               - RNN/LSTMï¼š**æ—¶åºå½’çº³åç½®** - åˆ©ç”¨æ—¶é—´ä¾èµ–æ€§
               - Transformerï¼š**æœ€å°å½’çº³åç½®** - è®©æ•°æ®è¯´è¯ï¼Œéœ€è¦å¤§é‡æ•°æ®
            
            3. **è®¡ç®—å¤æ‚åº¦**ï¼š
               - å…¨è¿æ¥ï¼š$O(n^2)$ - è¾“å…¥è¾“å‡ºå¤§å°çš„ä¹˜ç§¯
               - å·ç§¯ï¼š$O(k^2 \\cdot C \\cdot H \\cdot W)$ - å·ç§¯æ ¸å¤§å° Ã— é€šé“ Ã— è¾“å‡ºå¤§å°
               - RNNï¼š$O(T \\cdot n^2)$ - æ—¶é—´æ­¥ Ã— çŠ¶æ€å¤§å°å¹³æ–¹
               - Transformerï¼š$O(T^2 \\cdot d)$ - åºåˆ—é•¿åº¦å¹³æ–¹ Ã— ç‰¹å¾ç»´åº¦
            
            4. **æ¢¯åº¦æµåŠ¨**ï¼š
               - å…¨è¿æ¥/å·ç§¯ï¼šç›´æ¥åå‘ä¼ æ’­
               - RNNï¼šBPTTï¼Œå®¹æ˜“æ¢¯åº¦æ¶ˆå¤±
               - LSTMï¼šç»†èƒçŠ¶æ€ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
               - Transformerï¼šè·³è·ƒè¿æ¥ + LayerNorm ä¿æŒæ¢¯åº¦
            
            5. **ä»ç®€å•åˆ°å¤æ‚**ï¼š
               - ç†è§£å…¨è¿æ¥ç¥ç»å…ƒ â†’ ç†è§£ MLP
               - ç†è§£å·ç§¯ç¥ç»å…ƒ â†’ ç†è§£ CNN
               - ç†è§£å¾ªç¯ç¥ç»å…ƒ â†’ ç†è§£ RNN/LSTM
               - ç†è§£æ³¨æ„åŠ›ç¥ç»å…ƒ â†’ ç†è§£ Transformer
            
            **æœ¬é¡µå±•ç¤ºçš„æ˜¯æœ€åŸºç¡€çš„å…¨è¿æ¥ç¥ç»å…ƒï¼Œå®ƒæ˜¯æ‰€æœ‰å…¶ä»–ç¥ç»å…ƒçš„åŸºç¡€ï¼**
            
            ---
            
            ### ğŸ’¡ **å®è·µå»ºè®®**
            
            - ğŸ¯ **ä»ç®€å•å¼€å§‹**ï¼šå…ˆæŒæ¡å…¨è¿æ¥ç¥ç»å…ƒï¼ˆæœ¬é¡µï¼‰
            - ğŸ“š **é€æ­¥æ·±å…¥**ï¼šç†è§£æ¯ç§ç¥ç»å…ƒçš„è®¾è®¡åŠ¨æœº
            - ğŸ§ª **åŠ¨æ‰‹å®éªŒ**ï¼šåœ¨å…¶ä»–æ ‡ç­¾é¡µä¸­æ¢ç´¢ CNNã€RNN ç­‰
            - ğŸ”¬ **å¯¹æ¯”å­¦ä¹ **ï¼šå¯¹æ¯”ä¸åŒç¥ç»å…ƒåœ¨åŒä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°
            - ğŸ“– **é˜…è¯»è®ºæ–‡**ï¼šäº†è§£æ¯ç§æ¶æ„çš„åŸå§‹è®ºæ–‡å’Œå‘å±•å†ç¨‹
            
            **è®°ä½ï¼šç¥ç»å…ƒçš„æ ¸å¿ƒæ˜¯"åŠ æƒæ±‚å’Œ + éçº¿æ€§æ¿€æ´» + æ¢¯åº¦ä¼˜åŒ–"ï¼Œä¸‡å˜ä¸ç¦»å…¶å®—ï¼**
            """
            )

    else:
        st.header("ğŸ§¬ Single Neuron: Understanding the Basic Unit")
        st.info("English version - Coming soon!")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    neuron = SingleNeuron(input_size=3, activation="relu")
    x = np.array([0.5, -0.3, 0.2])

    # å‰å‘ä¼ æ’­
    output = neuron.forward(x)
    print(f"Forward pass output: {output:.6f}")

    # åå‘ä¼ æ’­
    gradients = neuron.backward(upstream_gradient=1.0)
    print(f"Gradients: {gradients}")

    # å‚æ•°æ›´æ–°
    neuron.update_parameters(learning_rate=0.01)
    print("Parameters updated!")
