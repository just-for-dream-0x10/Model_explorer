"""
æ¨¡å‹å‰ªæè®¡ç®—è§£å‰–æ ‡ç­¾é¡µ
Model Pruning Computational Analysis Tab

æ·±å…¥è§£å‰–å‰ªæå¯¹ç¥ç»ç½‘ç»œæ•°å€¼è®¡ç®—çš„å½±å“
æ ¸å¿ƒç†å¿µï¼šè®©ä½ çœ‹åˆ°å‰ªæåæ¯ä¸€æ­¥çš„æ•°å€¼å˜åŒ–ï¼Œä¸ºä»€ä¹ˆæŸäº›å‚æ•°å¯ä»¥å®‰å…¨ç§»é™¤
"""

import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from typing import List, Dict, Tuple, Optional
import copy

from utils.memory_analyzer import get_tensor_memory


def calculate_parameter_importance(model, dataloader=None, num_samples=100):
    """
    è®¡ç®—å‚æ•°é‡è¦æ€§ï¼ˆåŸºäºæ¢¯åº¦æˆ–æ¿€æ´»å€¼ï¼‰

    Args:
        model: PyTorchæ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
        num_samples: æ ·æœ¬æ•°é‡

    Returns:
        dict: æ¯å±‚å‚æ•°çš„é‡è¦æ€§åˆ†æ•°
    """
    importance_scores = {}

    # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºå‚æ•°ç»å¯¹å€¼çš„å¤§å°
    for name, param in model.named_parameters():
        if param.requires_grad:
            # ä½¿ç”¨å‚æ•°ç»å¯¹å€¼ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
            importance = torch.abs(param.data).cpu().numpy()
            importance_scores[name] = importance

    return importance_scores


def structured_prune_layer(layer, pruning_ratio, method="auto"):
    """
    ç»“æ„åŒ–å‰ªæï¼šæ”¯æŒå¤šç§å±‚ç±»å‹çš„å‰ªæ

    Args:
        layer: ç¥ç»ç½‘ç»œå±‚
        pruning_ratio: å‰ªææ¯”ä¾‹
        method: 'auto', 'filter', 'channel', 'neuron'

    Returns:
        å‰ªæåçš„å±‚å’Œå‰ªæä¿¡æ¯
    """
    original_weight = layer.weight.data.clone()
    original_bias = layer.bias.data.clone() if layer.bias is not None else None

    if isinstance(layer, nn.Conv2d):
        # å·ç§¯å±‚å‰ªæ
        if method == "auto":
            method = "filter"  # é»˜è®¤å‰ªæè¿‡æ»¤å™¨

        if method == "filter":
            out_channels = layer.out_channels
            filter_importance = torch.norm(
                original_weight.view(out_channels, -1), dim=1
            )
            num_filters_to_prune = int(pruning_ratio * out_channels)

            if num_filters_to_prune > 0:
                _, indices_to_prune = torch.topk(
                    filter_importance, num_filters_to_prune, largest=False
                )
                mask = torch.ones(out_channels, dtype=torch.bool)
                mask[indices_to_prune] = False
                new_out_channels = mask.sum().item()
                new_weight = original_weight[mask]
                new_bias = original_bias[mask] if original_bias is not None else None

                pruned_layer = nn.Conv2d(
                    layer.in_channels,
                    new_out_channels,
                    layer.kernel_size,
                    stride=layer.stride,
                    padding=layer.padding,
                    bias=layer.bias is not None,
                )
                pruned_layer.weight.data = new_weight
                if new_bias is not None:
                    pruned_layer.bias.data = new_bias

                return pruned_layer, {
                    "method": "filter",
                    "pruned_count": num_filters_to_prune,
                    "remaining_count": new_out_channels,
                    "indices_pruned": indices_to_prune.tolist(),
                    "original_shape": original_weight.shape,
                    "new_shape": new_weight.shape,
                    "type": "conv2d",
                }

    elif isinstance(layer, nn.Linear):
        # å…¨è¿æ¥å±‚å‰ªæ
        if method == "auto":
            method = "neuron"  # é»˜è®¤å‰ªæç¥ç»å…ƒ

        if method == "neuron":
            # å‰ªæè¾“å‡ºç¥ç»å…ƒ
            out_features = layer.out_features
            neuron_importance = torch.norm(original_weight, dim=1)
            num_neurons_to_prune = int(pruning_ratio * out_features)

            if num_neurons_to_prune > 0:
                _, indices_to_prune = torch.topk(
                    neuron_importance, num_neurons_to_prune, largest=False
                )
                mask = torch.ones(out_features, dtype=torch.bool)
                mask[indices_to_prune] = False
                new_out_features = mask.sum().item()
                new_weight = original_weight[mask]
                new_bias = original_bias[mask] if original_bias is not None else None

                pruned_layer = nn.Linear(
                    layer.in_features, new_out_features, bias=layer.bias is not None
                )
                pruned_layer.weight.data = new_weight
                if new_bias is not None:
                    pruned_layer.bias.data = new_bias

                return pruned_layer, {
                    "method": "neuron",
                    "pruned_count": num_neurons_to_prune,
                    "remaining_count": new_out_features,
                    "indices_pruned": indices_to_prune.tolist(),
                    "original_shape": original_weight.shape,
                    "new_shape": new_weight.shape,
                    "type": "linear",
                }

    # ä¸æ”¯æŒçš„å±‚ç±»å‹æˆ–å‰ªææ–¹æ³•
    return layer, {"method": method, "pruned_count": 0, "type": type(layer).__name__}


def unstructured_prune_layer(layer, pruning_ratio, method="magnitude"):
    """
    éç»“æ„åŒ–å‰ªæï¼šå‰ªæå•ä¸ªå‚æ•°

    Args:
        layer: ç¥ç»ç½‘ç»œå±‚
        pruning_ratio: å‰ªææ¯”ä¾‹
        method: 'magnitude' æˆ– 'random'

    Returns:
        å‰ªæåçš„å±‚å’Œæ©ç 
    """
    original_weight = layer.weight.data.clone()
    original_bias = layer.bias.data.clone() if layer.bias is not None else None

    if method == "magnitude":
        # åŸºäºå‚æ•°ç»å¯¹å€¼å¤§å°
        weight_flat = original_weight.view(-1)
        num_params_to_prune = int(pruning_ratio * len(weight_flat))

        if num_params_to_prune > 0:
            # é€‰æ‹©ç»å¯¹å€¼æœ€å°çš„å‚æ•°
            _, indices_to_prune = torch.topk(
                torch.abs(weight_flat), num_params_to_prune, largest=False
            )

            # åˆ›å»ºæ©ç 
            mask = torch.ones_like(weight_flat)
            mask[indices_to_prune] = 0
            mask = mask.view(original_weight.shape)

            # åº”ç”¨å‰ªæ
            pruned_weight = original_weight * mask
            layer.weight.data = pruned_weight

            return layer, mask

    return layer, None


def analyze_pruning_impact(original_model, pruned_model, input_shape, num_samples=10):
    """
    åˆ†æå‰ªæå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

    Args:
        original_model: åŸå§‹æ¨¡å‹
        pruned_model: å‰ªæåæ¨¡å‹
        input_shape: è¾“å…¥å½¢çŠ¶
        num_samples: æµ‹è¯•æ ·æœ¬æ•°

    Returns:
        dict: æ€§èƒ½å¯¹æ¯”åˆ†æ
    """
    # å‚æ•°é‡å¯¹æ¯”
    original_params = sum(p.numel() for p in original_model.parameters())
    pruned_params = sum(p.numel() for p in pruned_model.parameters())

    # å†…å­˜å ç”¨å¯¹æ¯”
    original_memory = original_params * 4 / (1024**2)  # MB
    pruned_memory = pruned_params * 4 / (1024**2)  # MB

    # æ¨ç†æ€§èƒ½æµ‹è¯•
    original_model.eval()
    pruned_model.eval()

    inference_times_original = []
    inference_times_pruned = []
    inference_success = True

    with torch.no_grad():
        for _ in range(num_samples):
            try:
                test_input = torch.randn(input_shape)

                # åŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´
                if torch.cuda.is_available():
                    start_time = torch.cuda.Event(enable_timing=True)
                    end_time = torch.cuda.Event(enable_timing=True)
                    start_time.record()
                    output_orig = original_model(test_input)
                    end_time.record()
                    torch.cuda.synchronize()
                    inference_times_original.append(start_time.elapsed_time(end_time))
                else:
                    import time

                    start = time.time()
                    output_orig = original_model(test_input)
                    end = time.time()
                    inference_times_original.append((end - start) * 1000)  # ms

                # å‰ªææ¨¡å‹æ¨ç†æ—¶é—´
                if torch.cuda.is_available():
                    start_time.record()
                    output_pruned = pruned_model(test_input)
                    end_time.record()
                    torch.cuda.synchronize()
                    inference_times_pruned.append(start_time.elapsed_time(end_time))
                else:
                    start = time.time()
                    output_pruned = pruned_model(test_input)
                    end = time.time()
                    inference_times_pruned.append((end - start) * 1000)

            except Exception as e:
                inference_success = False
                break

    # è¾“å‡ºå·®å¼‚åˆ†æ
    mse_diff = 0.0
    cosine_sim = 1.0

    if inference_success:
        try:
            with torch.no_grad():
                test_input = torch.randn(input_shape)
                output_orig = original_model(test_input)
                output_pruned = pruned_model(test_input)

                # è®¡ç®—è¾“å‡ºå·®å¼‚
                mse_diff = F.mse_loss(output_orig, output_pruned).item()
                cosine_sim = F.cosine_similarity(
                    output_orig.flatten(), output_pruned.flatten(), dim=0
                ).item()
        except Exception as e:
            pass

    return {
        "parameter_reduction": {
            "original": original_params,
            "pruned": pruned_params,
            "reduction_ratio": (
                (original_params - pruned_params) / original_params
                if original_params > 0
                else 0
            ),
            "saved_params": original_params - pruned_params,
        },
        "memory_reduction": {
            "original_mb": original_memory,
            "pruned_mb": pruned_memory,
            "reduction_ratio": (
                (original_memory - pruned_memory) / original_memory
                if original_memory > 0
                else 0
            ),
            "saved_mb": original_memory - pruned_memory,
        },
        "inference_performance": {
            "original_avg_time": (
                np.mean(inference_times_original) if inference_times_original else 0
            ),
            "pruned_avg_time": (
                np.mean(inference_times_pruned) if inference_times_pruned else 0
            ),
            "speedup_ratio": (
                (np.mean(inference_times_original) / np.mean(inference_times_pruned))
                if inference_times_original
                and inference_times_pruned
                and np.mean(inference_times_pruned) > 0
                else 1.0
            ),
            "success": inference_success,
        },
        "output_similarity": {
            "mse_difference": mse_diff,
            "cosine_similarity": cosine_sim,
        },
    }


def visualize_pruning_results(importance_scores, pruning_info=None):
    """å¯è§†åŒ–å‰ªæç»“æœ"""
    if not importance_scores:
        return None

    # å‚æ•°é‡è¦æ€§åˆ†å¸ƒ
    all_importances = []
    layer_names = []

    for name, importance in importance_scores.items():
        all_importances.extend(importance.flatten())
        layer_names.extend([name] * len(importance.flatten()))

    fig = go.Figure()

    # æ·»åŠ é‡è¦æ€§ç›´æ–¹å›¾
    fig.add_trace(
        go.Histogram(
            x=all_importances,
            nbinsx=50,
            name="å‚æ•°é‡è¦æ€§åˆ†å¸ƒ",
            marker_color="lightblue",
        )
    )

    fig.update_layout(
        title="å‚æ•°é‡è¦æ€§åˆ†å¸ƒ",
        xaxis_title="é‡è¦æ€§åˆ†æ•°",
        yaxis_title="å‚æ•°æ•°é‡",
        height=400,
    )

    return fig


def visualize_layer_pruning(layer_name, weight_data, mask=None):
    """å¯è§†åŒ–å•å±‚å‰ªææ•ˆæœ"""
    if len(weight_data.shape) == 4:  # Conv2d
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå·ç§¯æ ¸
        kernel_data = weight_data[0, 0].cpu().numpy()

        fig = go.Figure(data=go.Heatmap(z=kernel_data, colorscale="RdBu", zmid=0))

        title = f"{layer_name} - ç¬¬ä¸€ä¸ªå·ç§¯æ ¸"
        if mask is not None:
            title += " (å‰ªæå)"

        fig.update_layout(title=title, height=300)

        return fig

    elif len(weight_data.shape) == 2:  # Linear
        weight_matrix = weight_data.cpu().numpy()

        fig = go.Figure(data=go.Heatmap(z=weight_matrix, colorscale="RdBu", zmid=0))

        title = f"{layer_name} - æƒé‡çŸ©é˜µ"
        if mask is not None:
            title += " (å‰ªæå)"

        fig.update_layout(title=title, height=400)

        return fig

    return None


def explain_pruning_computation():
    """è§£é‡Šå‰ªæå¯¹æ•°å€¼è®¡ç®—çš„å½±å“"""
    st.markdown(
        """
    ### âœ‚ï¸ å‰ªæå¯¹æ•°å€¼è®¡ç®—çš„å½±å“è¯¦è§£
    
    #### æ ¸å¿ƒé—®é¢˜ï¼šä¸ºä»€ä¹ˆæŸäº›å‚æ•°å¯ä»¥å®‰å…¨ç§»é™¤ï¼Ÿ
    
    **1. å‚æ•°é‡è¦æ€§åˆ†æ**
    ```
    # åŸºäºæƒé‡ç»å¯¹å€¼çš„é‡è¦æ€§è¯„ä¼°
    importance(i,j) = |W[i,j]|
    
    # åŸºäºæ¢¯åº¦çš„é‡è¦æ€§è¯„ä¼°  
    importance(i,j) = |âˆ‚L/âˆ‚W[i,j]|
    
    # åŸºäºæ¿€æ´»å€¼çš„é‡è¦æ€§è¯„ä¼°
    importance(i,j) = mean(|activation[i]|)
    
    æ•°å€¼ä¾‹å­ï¼š
    åŸå§‹æƒé‡çŸ©é˜µ W = [[0.1, -0.8], [0.05, 0.2], [1.2, -0.3]]
    ç»å¯¹å€¼é‡è¦æ€§ = [0.1, 0.8, 0.05, 0.2, 1.2, 0.3]
    
    æ’åºåï¼š[1.2, 0.8, 0.3, 0.2, 0.1, 0.05]
    å‰ªæ50%ï¼šç§»é™¤ [0.1, 0.05, 0.2]
    ä¿ç•™æƒé‡ï¼š[[0, -0.8], [0, 0.2], [1.2, -0.3]]
    ```
    
    **2. å‰ªæåçš„æ•°å€¼å˜åŒ–**
    ```
    # å‰ªæå‰ï¼šy = W Â· x + b
    # å‰ªæåï¼šy' = W' Â· x + b
    
    æ•°å€¼å½±å“åˆ†æï¼š
    x = [0.5, 0.3]
    W = [[0.1, -0.8], [0.05, 0.2]]
    b = [0.1, -0.1]
    
    å‰ªæå‰è¾“å‡ºï¼š
    y[0] = 0.1Ã—0.5 + (-0.8)Ã—0.3 + 0.1 = 0.05 - 0.24 + 0.1 = -0.09
    y[1] = 0.05Ã—0.5 + 0.2Ã—0.3 - 0.1 = 0.025 + 0.06 - 0.1 = -0.015
    
    å‰ªæåï¼ˆç§»é™¤å°æƒé‡ï¼‰ï¼š
    W' = [[0, -0.8], [0, 0.2]]
    y'[0] = 0Ã—0.5 + (-0.8)Ã—0.3 + 0.1 = -0.24 + 0.1 = -0.14
    y'[1] = 0Ã—0.5 + 0.2Ã—0.3 - 0.1 = 0.06 - 0.1 = -0.04
    
    æ•°å€¼å˜åŒ–ï¼šÎ”y = y' - y = [-0.05, -0.025]
    ç›¸å¯¹è¯¯å·®ï¼š|Î”y|/|y| = [55.6%, 166.7%]
    ```
    
    **3. æ¢¯åº¦ä¼ æ’­çš„æ•°å€¼å½±å“**
    ```
    # å‰ªæå‰æ¢¯åº¦è®¡ç®—
    âˆ‚L/âˆ‚W[i,j] = âˆ‚L/âˆ‚y[i] Ã— x[j]
    
    # å‰ªæåæ¢¯åº¦è®¡ç®—
    âˆ‚L/âˆ‚W'[i,j] = âˆ‚L/âˆ‚y'[i] Ã— x[j] (å¦‚æœW'[i,j] â‰  0)
    âˆ‚L/âˆ‚W'[i,j] = 0 (å¦‚æœW'[i,j] = 0)
    
    å…³é”®æ´å¯Ÿï¼š
    - è¢«å‰ªæçš„å‚æ•°ä¸å†è·å¾—æ¢¯åº¦æ›´æ–°
    - å‰©ä½™å‚æ•°çš„æ¢¯åº¦å¯èƒ½å‘ç”Ÿå˜åŒ–
    - å¯èƒ½å¯¼è‡´æ¢¯åº¦æµä¸ç¨³å®š
    ```
    
    **4. æ•°å€¼ç¨³å®šæ€§é—®é¢˜**
    
    **æ¢¯åº¦æ¶ˆå¤±**ï¼š
    - é—®é¢˜ï¼šå‰ªæåæ¢¯åº¦å˜å°
    - ç°è±¡ï¼šâˆ‚L/âˆ‚W' â‰ˆ 0
    - åŸå› ï¼šé‡è¦è¿æ¥è¢«ç§»é™¤
    
    **æ¿€æ´»å€¼åç§»**ï¼š
    - é—®é¢˜ï¼šè¾“å‡ºåˆ†å¸ƒå‘ç”Ÿå˜åŒ–
    - ç°è±¡ï¼šy'çš„å‡å€¼/æ–¹å·®ä¸yä¸åŒ
    - å½±å“ï¼šåç»­å±‚çš„è¾“å…¥åˆ†å¸ƒæ”¹å˜
    
    **æ•°å€¼ç²¾åº¦ç´¯ç§¯**ï¼š
    - é—®é¢˜ï¼šå¤šæ¬¡å‰ªæå¯¼è‡´ç²¾åº¦æŸå¤±ç´¯ç§¯
    - ç°è±¡ï¼šæœ€ç»ˆè¾“å‡ºè¯¯å·®é€æ¸å¢å¤§
    - è§£å†³ï¼šé™åˆ¶å‰ªææ¯”ä¾‹ã€å¾®è°ƒæ¢å¤
    """
    )


def create_sample_model(model_type="cnn"):
    if model_type == "cnn":
        return nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(128, 10),
        )
    elif model_type == "mlp":
        return nn.Sequential(
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10),
        )
    elif model_type == "resnet_like":
        # ç®€åŒ–çš„ResNeté£æ ¼
        return nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(128, 10),
        )
    elif model_type == "transformer_like":
        # ç®€åŒ–çš„Transformeré£æ ¼
        return nn.Sequential(
            nn.Linear(512, 512),  # Self-attentionæ¨¡æ‹Ÿ
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, 256),  # FFNç¬¬ä¸€å±‚
            nn.ReLU(),
            nn.Linear(256, 512),  # FFNç¬¬äºŒå±‚
            nn.LayerNorm(512),
            nn.Linear(512, 10),  # åˆ†ç±»å¤´
        )


def model_pruning_tab(chinese_supported=True):
    """æ¨¡å‹å‰ªæåˆ†æä¸»å‡½æ•°"""

    st.header("âœ‚ï¸ æ¨¡å‹å‰ªæè®¡ç®—è§£å‰–å°")
    st.markdown(
        """
    > **æ ¸å¿ƒç†å¿µ**ï¼šæ·±å…¥è§£å‰–å‰ªæå¯¹æ•°å€¼è®¡ç®—çš„å½±å“
    
    **å…³é”®é—®é¢˜**ï¼š
    - å‰ªæåæƒé‡çŸ©é˜µçš„æ•°å€¼å¦‚ä½•å˜åŒ–ï¼Ÿ
    - ä¸ºä»€ä¹ˆæŸäº›å‚æ•°çš„æ¢¯åº¦æ¥è¿‘é›¶ï¼Ÿ
    - å‰ªæå¦‚ä½•å½±å“æ¿€æ´»å€¼çš„åˆ†å¸ƒï¼Ÿ
    - æ•°å€¼ç²¾åº¦å¦‚ä½•å½±å“å‰ªæå†³ç­–ï¼Ÿ
    """
    )

    st.markdown("---")

    # å‰ªæè®¡ç®—è¿‡ç¨‹è§£æ
    with st.expander("âœ‚ï¸ å‰ªææ•°å€¼è®¡ç®—å½±å“ï¼ˆç‚¹å‡»å±•å¼€ï¼‰", expanded=False):
        explain_pruning_computation()

    st.markdown("---")

    # åˆ†ææ¨¡å¼é€‰æ‹©
    st.subheader("ğŸ”§ é€‰æ‹©åˆ†ææ¨¡å¼")

    analysis_mode = st.radio(
        "åˆ†ææ¨¡å¼",
        ["å‚æ•°é‡è¦æ€§åˆ†æ", "ç»“æ„åŒ–å‰ªæ", "éç»“æ„åŒ–å‰ªæ", "å‰ªææ•ˆæœå¯¹æ¯”"],
        horizontal=True,
    )

    if analysis_mode == "å‚æ•°é‡è¦æ€§åˆ†æ":
        st.markdown("---")
        st.subheader("ğŸ“Š å‚æ•°é‡è¦æ€§åˆ†æ")

        col1, col2 = st.columns(2)

        with col1:
            model_type = st.selectbox(
                "æ¨¡å‹ç±»å‹",
                ["cnn", "mlp", "resnet_like", "transformer_like"],
                key="importance_model_type",
            )
            importance_method = st.selectbox(
                "é‡è¦æ€§è®¡ç®—æ–¹æ³•",
                ["magnitude", "gradient", "activation"],
                key="importance_method",
            )

        with col2:
            if model_type in ["cnn", "resnet_like"]:
                input_size = st.number_input(
                    "å›¾åƒå°ºå¯¸", 32, 224, 32, key="importance_input_size"
                )
                input_shape_desc = f"(1, 3, {input_size}, {input_size})"
            else:  # mlp, transformer_like
                input_size = st.number_input(
                    "å‘é‡ç»´åº¦", 256, 1024, 512, key="importance_input_size"
                )
                input_shape_desc = f"(1, {input_size})"

            num_samples = st.number_input(
                "åˆ†ææ ·æœ¬æ•°", 10, 100, 50, key="importance_samples"
            )

            st.info(f"è¾“å…¥å½¢çŠ¶: {input_shape_desc}")

        if st.button("ğŸ” åˆ†æå‚æ•°é‡è¦æ€§", type="primary"):
            with st.spinner("åˆ†æä¸­..."):
                # åˆ›å»ºæ¨¡å‹
                model = create_sample_model(model_type)

                # è®¡ç®—è¾“å…¥å½¢çŠ¶
                if model_type in ["cnn", "resnet_like"]:
                    input_shape = (1, 3, input_size, input_size)
                else:  # mlp, transformer_like
                    input_shape = (1, input_size)

                # è®¡ç®—å‚æ•°é‡è¦æ€§
                importance_scores = calculate_parameter_importance(
                    model, num_samples=num_samples
                )

            st.success("âœ… åˆ†æå®Œæˆï¼")

            # é‡è¦æ€§ç»Ÿè®¡
            st.markdown("#### ğŸ“Š é‡è¦æ€§ç»Ÿè®¡")

            all_importances = []
            for name, importance in importance_scores.items():
                all_importances.extend(importance.flatten())

            all_importances = np.array(all_importances)

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("æ€»å‚æ•°æ•°", f"{len(all_importances):,}")
            with col2:
                st.metric("å¹³å‡é‡è¦æ€§", f"{np.mean(all_importances):.4f}")
            with col3:
                st.metric("é‡è¦æ€§æ ‡å‡†å·®", f"{np.std(all_importances):.4f}")
            with col4:
                st.metric("æœ€å¤§é‡è¦æ€§", f"{np.max(all_importances):.4f}")

            # å¯è§†åŒ–
            st.markdown("---")
            st.markdown("#### ğŸ“ˆ å¯è§†åŒ–åˆ†æ")

            fig = visualize_pruning_results(importance_scores)
            if fig:
                st.plotly_chart(fig, use_container_width=True)

            # åˆ†å±‚é‡è¦æ€§åˆ†æ
            st.markdown("#### ğŸ” åˆ†å±‚é‡è¦æ€§åˆ†æ")

            layer_data = []
            for name, importance in importance_scores.items():
                layer_data.append(
                    {
                        "å±‚å": name,
                        "å‚æ•°æ•°é‡": importance.size,
                        "å¹³å‡é‡è¦æ€§": np.mean(importance),
                        "é‡è¦æ€§æ ‡å‡†å·®": np.std(importance),
                        "æœ€å°é‡è¦æ€§": np.min(importance),
                        "æœ€å¤§é‡è¦æ€§": np.max(importance),
                    }
                )

            df = pd.DataFrame(layer_data)
            st.dataframe(df, use_container_width=True, hide_index=True)

    elif analysis_mode == "ç»“æ„åŒ–å‰ªæ":
        st.markdown("---")
        st.subheader("ğŸ—ï¸ ç»“æ„åŒ–å‰ªæåˆ†æ")

        st.info("ğŸ’¡ ç»“æ„åŒ–å‰ªæä¼šç§»é™¤æ•´ä¸ªç¥ç»å…ƒ/å·ç§¯æ ¸ï¼Œé€‚åˆå®é™…éƒ¨ç½²åŠ é€Ÿ")

        col1, col2 = st.columns(2)

        with col1:
            model_type = st.selectbox(
                "æ¨¡å‹ç±»å‹", ["cnn", "resnet_like"], key="structured_model"
            )
            pruning_method = st.selectbox(
                "å‰ªææ–¹æ³•",
                ["filter", "channel", "neuron"],
                help="filter: å‰ªææ•´ä¸ªå·ç§¯æ ¸; channel: å‰ªææ•´ä¸ªé€šé“; neuron: å‰ªæç¥ç»å…ƒ",
            )

        with col2:
            pruning_ratio = st.slider(
                "å‰ªææ¯”ä¾‹", 0.1, 0.9, 0.5, 0.1, key="structured_ratio"
            )
            input_size = st.number_input(
                "è¾“å…¥å°ºå¯¸", 32, 224, 32, key="structured_input"
            )

        if st.button("âœ‚ï¸ æ‰§è¡Œç»“æ„åŒ–å‰ªæ", type="primary"):
            with st.spinner("å‰ªæä¸­..."):
                # åˆ›å»ºæ¨¡å‹
                model = create_sample_model(model_type)
                original_model = copy.deepcopy(model)

                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯å‰ªæçš„å±‚
                pruned_layer = None
                pruning_info = None

                for layer in model:
                    if isinstance(layer, (nn.Conv2d, nn.Linear)):
                        pruned_layer, pruning_info = structured_prune_layer(
                            layer, pruning_ratio, pruning_method
                        )
                        break

                if pruning_info.get("pruned_count", 0) > 0:
                    st.success(
                        f"âœ… å‰ªæå®Œæˆï¼å‰ªæäº† {pruning_info['pruned_count']} ä¸ªå•å…ƒ"
                    )

                    # æ˜¾ç¤ºå‰ªæä¿¡æ¯
                    st.markdown("#### ğŸ“‹ å‰ªæä¿¡æ¯")
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("å‰ªææ–¹æ³•", pruning_method)
                    with col2:
                        st.metric("å‰ªææ•°é‡", pruning_info["pruned_count"])
                    with col3:
                        st.metric("å‰©ä½™æ•°é‡", pruning_info["remaining_count"])

                    # å½¢çŠ¶å˜åŒ–
                    st.markdown(
                        f"""
                    **å½¢çŠ¶å˜åŒ–**ï¼š
                    - åŸå§‹å½¢çŠ¶ï¼š`{pruning_info['original_shape']}`
                    - å‰ªæåå½¢çŠ¶ï¼š`{pruning_info['new_shape']}`
                    """
                    )

                    # å¯è§†åŒ–
                    if isinstance(pruned_layer, (nn.Conv2d, nn.Linear)):
                        st.markdown("#### ğŸ“ˆ æƒé‡å¯è§†åŒ–")
                        col1, col2 = st.columns(2)

                        with col1:
                            st.markdown("**åŸå§‹æƒé‡**")
                            original_fig = visualize_layer_pruning(
                                "åŸå§‹å±‚", layer.weight.data
                            )
                            if original_fig:
                                st.plotly_chart(original_fig, use_container_width=True)

                        with col2:
                            st.markdown("**å‰ªæåæƒé‡**")
                            pruned_fig = visualize_layer_pruning(
                                "å‰ªæåå±‚", pruned_layer.weight.data
                            )
                            if pruned_fig:
                                st.plotly_chart(pruned_fig, use_container_width=True)
                else:
                    st.warning("æ²¡æœ‰è¿›è¡Œå‰ªæï¼Œå¯èƒ½æ˜¯å› ä¸ºå‰ªææ¯”ä¾‹è¿‡å°")

    elif analysis_mode == "éç»“æ„åŒ–å‰ªæ":
        st.markdown("---")
        st.subheader("ğŸ¯ éç»“æ„åŒ–å‰ªæåˆ†æ")

        st.info("ğŸ’¡ éç»“æ„åŒ–å‰ªæç§»é™¤å•ä¸ªå‚æ•°ï¼Œç²¾åº¦æŸå¤±å°ä½†éœ€è¦ç‰¹æ®Šç¡¬ä»¶æ”¯æŒ")

        col1, col2 = st.columns(2)

        with col1:
            model_type = st.selectbox(
                "æ¨¡å‹ç±»å‹", ["cnn", "mlp", "transformer_like"], key="unstructured_model"
            )
            pruning_method = st.selectbox(
                "å‰ªæç­–ç•¥", ["magnitude", "random"], key="unstructured_method"
            )

        with col2:
            pruning_ratio = st.slider(
                "å‰ªææ¯”ä¾‹", 0.1, 0.9, 0.5, 0.1, key="unstructured_ratio"
            )
            if model_type in ["cnn", "resnet_like"]:
                input_size = st.number_input(
                    "å›¾åƒå°ºå¯¸", 32, 128, 32, key="unstructured_input"
                )
            else:
                input_size = st.number_input(
                    "å‘é‡ç»´åº¦", 256, 1024, 512, key="unstructured_input"
                )

        if st.button("ğŸ¯ æ‰§è¡Œéç»“æ„åŒ–å‰ªæ", type="primary"):
            with st.spinner("å‰ªæä¸­..."):
                # åˆ›å»ºæ¨¡å‹
                model = create_sample_model(model_type)
                original_model = copy.deepcopy(model)

                # åº”ç”¨éç»“æ„åŒ–å‰ªæ
                pruned_count = 0
                for layer in model.modules():
                    if isinstance(layer, (nn.Conv2d, nn.Linear)):
                        _, mask = unstructured_prune_layer(
                            layer, pruning_ratio, pruning_method
                        )
                        if mask is not None:
                            pruned_count += (mask == 0).sum().item()

                # è®¡ç®—è¾“å…¥å½¢çŠ¶
                if model_type in ["cnn", "resnet_like"]:
                    input_shape = (1, 3, input_size, input_size)
                else:
                    input_shape = (1, input_size)

                # åˆ†æå‰ªæå½±å“
                impact_analysis = analyze_pruning_impact(
                    original_model, model, input_shape
                )

            st.success(f"âœ… å‰ªæå®Œæˆï¼å‰ªæäº† {pruned_count:,} ä¸ªå‚æ•°")

            # å‰ªæç»Ÿè®¡
            st.markdown("#### ğŸ“Š å‰ªæç»Ÿè®¡")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                reduction = impact_analysis["parameter_reduction"]["reduction_ratio"]
                st.metric("å‚æ•°å‡å°‘", f"{reduction:.1%}")
            with col2:
                saved = impact_analysis["parameter_reduction"]["saved_params"]
                st.metric("èŠ‚çœå‚æ•°", f"{saved:,}")
            with col3:
                mem_reduction = impact_analysis["memory_reduction"]["reduction_ratio"]
                st.metric("å†…å­˜å‡å°‘", f"{mem_reduction:.1%}")
            with col4:
                if impact_analysis["inference_performance"]["success"]:
                    speedup = impact_analysis["inference_performance"]["speedup_ratio"]
                    st.metric("ç†è®ºåŠ é€Ÿ", f"{speedup:.2f}x")
                else:
                    st.metric("æ¨ç†çŠ¶æ€", "âŒ å¤±è´¥")

    else:  # å‰ªææ•ˆæœå¯¹æ¯”
        st.markdown("---")
        st.subheader("ğŸ”¬ å‰ªææ•ˆæœå¯¹æ¯”åˆ†æ")

        st.info("ğŸ’¡ å¯¹æ¯”ä¸åŒå‰ªæç­–ç•¥çš„æ•ˆæœï¼Œæ‰¾åˆ°æœ€é€‚åˆçš„é…ç½®")

        # é…ç½®é¢æ¿
        col1, col2, col3 = st.columns(3)

        with col1:
            model_type = st.selectbox("æ¨¡å‹ç±»å‹", ["cnn", "mlp"], key="compare_model")
            if model_type == "cnn":
                input_size = st.number_input(
                    "å›¾åƒå°ºå¯¸", 32, 128, 32, key="compare_input"
                )
                input_shape_desc = f"(1, 3, {input_size}, {input_size})"
            else:
                input_size = st.number_input(
                    "å‘é‡ç»´åº¦", 256, 1024, 512, key="compare_input"
                )
                input_shape_desc = f"(1, {input_size})"
            st.info(f"è¾“å…¥å½¢çŠ¶: {input_shape_desc}")

        with col2:
            pruning_ratios = st.multiselect(
                "å‰ªææ¯”ä¾‹",
                [0.1, 0.3, 0.5, 0.7, 0.9],
                default=[0.3, 0.5, 0.7],
                key="compare_ratios",
            )

        with col3:
            methods = st.multiselect(
                "å‰ªææ–¹æ³•",
                ["structured", "unstructured"],
                default=["structured", "unstructured"],
                key="compare_methods",
            )

        if st.button("ğŸš€ å¼€å§‹å¯¹æ¯”åˆ†æ", type="primary"):
            if not pruning_ratios or not methods:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªå‰ªææ¯”ä¾‹å’Œä¸€ä¸ªå‰ªææ–¹æ³•")
                return

            with st.spinner("åˆ†æä¸­..."):
                # åˆ›å»ºæ¨¡å‹
                model = create_sample_model(model_type)
                original_model = copy.deepcopy(model)

                # è®¡ç®—è¾“å…¥å½¢çŠ¶
                if model_type == "cnn":
                    input_shape = (1, 3, input_size, input_size)
                else:
                    input_shape = (1, input_size)

                # å¯¹æ¯”ç»“æœ
                comparison_results = {}

                for method in methods:
                    method_results = {}

                    for ratio in pruning_ratios:
                        test_model = copy.deepcopy(original_model)

                        if method == "structured":
                            # ç»“æ„åŒ–å‰ªæï¼ˆç®€åŒ–ç‰ˆï¼‰
                            for i, layer in enumerate(test_model):
                                if isinstance(layer, (nn.Conv2d, nn.Linear)):
                                    pruned_layer, _ = structured_prune_layer(
                                        layer, ratio, "auto"
                                    )
                                    test_model[i] = pruned_layer
                                    break  # åªå‰ªæç¬¬ä¸€ä¸ªå¯å‰ªæçš„å±‚

                        else:  # unstructured
                            # éç»“æ„åŒ–å‰ªæ
                            for layer in test_model.modules():
                                if isinstance(layer, (nn.Conv2d, nn.Linear)):
                                    unstructured_prune_layer(layer, ratio, "magnitude")

                        # åˆ†æå½±å“
                        impact = analyze_pruning_impact(
                            original_model, test_model, input_shape
                        )

                        method_results[f"{ratio:.1f}"] = impact

                    comparison_results[method] = method_results

            st.success("âœ… å¯¹æ¯”åˆ†æå®Œæˆï¼")

            # ç»“æœè¡¨æ ¼
            st.markdown("#### ğŸ“Š å¯¹æ¯”ç»“æœæ€»è§ˆ")

            table_data = {
                "å‰ªææ–¹æ³•": [],
                "å‰ªææ¯”ä¾‹": [],
                "å‚æ•°å‡å°‘": [],
                "å†…å­˜å‡å°‘": [],
                "æ¨ç†åŠ é€Ÿ": [],
                "MSEå·®å¼‚": [],
                "ä½™å¼¦ç›¸ä¼¼åº¦": [],
            }

            for method, ratios_data in comparison_results.items():
                for ratio, impact in ratios_data.items():
                    table_data["å‰ªææ–¹æ³•"].append(method)
                    table_data["å‰ªææ¯”ä¾‹"].append(f"{ratio}")
                    table_data["å‚æ•°å‡å°‘"].append(
                        f"{impact['parameter_reduction']['reduction_ratio']:.1%}"
                    )
                    table_data["å†…å­˜å‡å°‘"].append(
                        f"{impact['memory_reduction']['reduction_ratio']:.1%}"
                    )

                    if impact["inference_performance"]["success"]:
                        table_data["æ¨ç†åŠ é€Ÿ"].append(
                            f"{impact['inference_performance']['speedup_ratio']:.2f}x"
                        )
                    else:
                        table_data["æ¨ç†åŠ é€Ÿ"].append("âŒ")

                    table_data["MSEå·®å¼‚"].append(
                        f"{impact['output_similarity']['mse_difference']:.6f}"
                    )
                    table_data["ä½™å¼¦ç›¸ä¼¼åº¦"].append(
                        f"{impact['output_similarity']['cosine_similarity']:.6f}"
                    )

            df = pd.DataFrame(table_data)
            st.dataframe(df, use_container_width=True, hide_index=True)

    # æ€»ç»“
    st.markdown("---")
    st.subheader("ğŸ’¡ å‰ªææœ€ä½³å®è·µ")

    st.markdown(
        """
    ### ğŸ¯ å‰ªæç­–ç•¥é€‰æ‹©
    
    1. **ç»“æ„åŒ–å‰ªæ**
       - é€‚ç”¨ï¼šç§»åŠ¨ç«¯éƒ¨ç½²ã€å®æ—¶æ¨ç†
       - ä¼˜ç‚¹ï¼šç¡¬ä»¶å‹å¥½ã€å®é™…åŠ é€Ÿ
       - ç¼ºç‚¹ï¼šç²¾åº¦æŸå¤±è¾ƒå¤§
    
    2. **éç»“æ„åŒ–å‰ªæ**
       - é€‚ç”¨ï¼šç ”ç©¶å®éªŒã€ç²¾åº¦æ•æ„Ÿåœºæ™¯
       - ä¼˜ç‚¹ï¼šç²¾åº¦æŸå¤±å°ã€çµæ´»åº¦é«˜
       - ç¼ºç‚¹ï¼šéœ€è¦ç‰¹æ®Šç¡¬ä»¶æ”¯æŒ
    
    ### âš ï¸ å¸¸è§é™·é˜±
    
    - **è¿‡åº¦å‰ªæ**ï¼šå‰ªææ¯”ä¾‹è¿‡é«˜å¯¼è‡´ä¸¥é‡ç²¾åº¦æŸå¤±
    - **ä¸å‡åŒ€å‰ªæ**ï¼šæŸäº›å±‚å‰ªæè¿‡å¤šç ´åç½‘ç»œç»“æ„
    - **ç¼ºä¹å¾®è°ƒ**ï¼šå‰ªæåä¸è¿›è¡Œæ¢å¤è®­ç»ƒ
    - **å¿½ç•¥ç¡¬ä»¶é™åˆ¶**ï¼šé€‰æ‹©ä¸å…¼å®¹çš„å‰ªææ–¹æ³•
    
    ### ğŸ”§ ä¼˜åŒ–å»ºè®®
    
    - **æ¸è¿›å¼å‰ªæ**ï¼šåˆ†é˜¶æ®µé€æ­¥å¢åŠ å‰ªææ¯”ä¾‹
    - **åˆ†å±‚ç­–ç•¥**ï¼šä¸åŒå±‚é‡‡ç”¨ä¸åŒå‰ªææ¯”ä¾‹
    - **å¾®è°ƒæ¢å¤**ï¼šå‰ªæåè¿›è¡ŒçŸ­æ—¶é—´è®­ç»ƒ
    - **éªŒè¯è¯„ä¼°**ï¼šåœ¨éªŒè¯é›†ä¸Šæµ‹è¯•å‰ªææ•ˆæœ
    """
    )


if __name__ == "__main__":
    # æµ‹è¯•è¿è¡Œ
    model_pruning_tab()
