"""
æ•°å€¼ç¨³å®šæ€§åˆ†æå™¨
Numerical Stability Analyzer

æ£€æµ‹ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜
åŒ…æ‹¬ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ã€æ¿€æ´»å€¼å¼‚å¸¸ã€æƒé‡å¼‚å¸¸

æ–°å¢åŠŸèƒ½ï¼ˆPhase 3ï¼‰:
- å®æ—¶æ¢¯åº¦æ£€æµ‹
- åˆå§‹åŒ–æ–¹æ¡ˆæ¨è
- å³°å€¼å†…å­˜é¢„æµ‹
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional, Any


class LayerStabilityInfo:
    """å•å±‚ç¨³å®šæ€§ä¿¡æ¯"""

    def __init__(self, name: str, layer_type: str):
        self.name = name
        self.layer_type = layer_type

        # æ¿€æ´»å€¼ç»Ÿè®¡
        self.activation_mean = 0.0
        self.activation_std = 0.0
        self.activation_min = 0.0
        self.activation_max = 0.0
        self.activation_range = 0.0

        # æ¢¯åº¦ç»Ÿè®¡
        self.gradient_mean = 0.0
        self.gradient_std = 0.0
        self.gradient_norm = 0.0
        self.gradient_max = 0.0

        # æƒé‡ç»Ÿè®¡
        self.weight_mean = 0.0
        self.weight_std = 0.0
        self.weight_norm = 0.0

        # ç¨³å®šæ€§çŠ¶æ€
        self.activation_status = "æœªæ£€æµ‹"  # æ­£å¸¸ã€å¼‚å¸¸å¤§ã€å¼‚å¸¸å°ã€åŒ…å«NaN/Inf
        self.gradient_status = "æœªæ£€æµ‹"  # æ­£å¸¸ã€æ¶ˆå¤±ã€çˆ†ç‚¸ã€åŒ…å«NaN/Inf
        self.weight_status = "æœªæ£€æµ‹"  # æ­£å¸¸ã€å¼‚å¸¸ã€æœªåˆå§‹åŒ–

        # é—®é¢˜æè¿°å’Œå»ºè®®
        self.issues = []
        self.recommendations = []


def check_activation_stability(
    activations: torch.Tensor,
    threshold_large: float = 100.0,
    threshold_small: float = 1e-3,
) -> Dict:
    """
    æ£€æŸ¥æ¿€æ´»å€¼çš„ç¨³å®šæ€§

    Args:
        activations: æ¿€æ´»å€¼å¼ é‡
        threshold_large: å¼‚å¸¸å¤§çš„é˜ˆå€¼
        threshold_small: å¼‚å¸¸å°çš„é˜ˆå€¼

    Returns:
        result: ç¨³å®šæ€§æ£€æŸ¥ç»“æœ
    """
    result = {
        "mean": 0.0,
        "std": 0.0,
        "min": 0.0,
        "max": 0.0,
        "range": 0.0,
        "has_nan": False,
        "has_inf": False,
        "status": "æ­£å¸¸",
        "issues": [],
        "recommendations": [],
    }

    # æ£€æŸ¥NaNå’ŒInf
    if torch.isnan(activations).any():
        result["has_nan"] = True
        result["status"] = "åŒ…å«NaN"
        result["issues"].append("æ¿€æ´»å€¼åŒ…å«NaNï¼ˆNot a Numberï¼‰")
        result["recommendations"].append("æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦æœ‰NaN")
        result["recommendations"].append("æ£€æŸ¥æƒé‡åˆå§‹åŒ–")
        result["recommendations"].append("é™ä½å­¦ä¹ ç‡")
        return result

    if torch.isinf(activations).any():
        result["has_inf"] = True
        result["status"] = "åŒ…å«Inf"
        result["issues"].append("æ¿€æ´»å€¼åŒ…å«Infï¼ˆæ— ç©·å¤§ï¼‰")
        result["recommendations"].append("æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´æ•°å€¼æº¢å‡º")
        result["recommendations"].append("ä½¿ç”¨æ¢¯åº¦è£å‰ª")
        result["recommendations"].append("é™ä½å­¦ä¹ ç‡")
        return result

    # ç»Ÿè®¡ä¿¡æ¯
    result["mean"] = activations.mean().item()
    result["std"] = activations.std().item()
    result["min"] = activations.min().item()
    result["max"] = activations.max().item()
    result["range"] = result["max"] - result["min"]

    # æ£€æŸ¥å¼‚å¸¸
    if abs(result["max"]) > threshold_large or abs(result["min"]) > threshold_large:
        result["status"] = "å¼‚å¸¸å¤§"
        result["issues"].append(
            f"æ¿€æ´»å€¼èŒƒå›´è¿‡å¤§: [{result['min']:.2f}, {result['max']:.2f}]"
        )
        result["recommendations"].append("æ·»åŠ BatchNormæˆ–LayerNorm")
        result["recommendations"].append("ä½¿ç”¨ReLUä»£æ›¿Sigmoid/Tanh")
        result["recommendations"].append("æ£€æŸ¥æƒé‡åˆå§‹åŒ–æ–¹æ¡ˆ")

    elif abs(result["mean"]) < threshold_small and result["std"] < threshold_small:
        result["status"] = "å¼‚å¸¸å°"
        result["issues"].append(
            f"æ¿€æ´»å€¼è¿‡å°: mean={result['mean']:.2e}, std={result['std']:.2e}"
        )
        result["recommendations"].append("å¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±")
        result["recommendations"].append("æ£€æŸ¥æ¿€æ´»å‡½æ•°ï¼ˆé¿å…Sigmoidï¼‰")
        result["recommendations"].append("ä½¿ç”¨æ®‹å·®è¿æ¥")

    return result


def check_gradient_stability(
    gradients: torch.Tensor,
    threshold_vanish: float = 1e-7,
    threshold_explode: float = 10.0,
) -> Dict:
    """
    æ£€æŸ¥æ¢¯åº¦çš„ç¨³å®šæ€§

    Args:
        gradients: æ¢¯åº¦å¼ é‡
        threshold_vanish: æ¢¯åº¦æ¶ˆå¤±é˜ˆå€¼
        threshold_explode: æ¢¯åº¦çˆ†ç‚¸é˜ˆå€¼

    Returns:
        result: ç¨³å®šæ€§æ£€æŸ¥ç»“æœ
    """
    result = {
        "mean": 0.0,
        "std": 0.0,
        "norm": 0.0,
        "max": 0.0,
        "has_nan": False,
        "has_inf": False,
        "status": "æ­£å¸¸",
        "issues": [],
        "recommendations": [],
    }

    # æ£€æŸ¥NaNå’ŒInf
    if torch.isnan(gradients).any():
        result["has_nan"] = True
        result["status"] = "åŒ…å«NaN"
        result["issues"].append("æ¢¯åº¦åŒ…å«NaN")
        result["recommendations"].append("å­¦ä¹ ç‡å¯èƒ½è¿‡å¤§")
        result["recommendations"].append("æ£€æŸ¥æŸå¤±å‡½æ•°")
        result["recommendations"].append("ä½¿ç”¨æ¢¯åº¦è£å‰ª")
        return result

    if torch.isinf(gradients).any():
        result["has_inf"] = True
        result["status"] = "åŒ…å«Inf"
        result["issues"].append("æ¢¯åº¦åŒ…å«Inf")
        result["recommendations"].append("æ¢¯åº¦çˆ†ç‚¸")
        result["recommendations"].append("ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆclip_grad_normï¼‰")
        result["recommendations"].append("é™ä½å­¦ä¹ ç‡")
        return result

    # ç»Ÿè®¡ä¿¡æ¯
    result["mean"] = gradients.mean().item()
    result["std"] = gradients.std().item()
    result["norm"] = gradients.norm().item()
    result["max"] = gradients.abs().max().item()

    # æ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±
    if result["norm"] < threshold_vanish:
        result["status"] = "æ¢¯åº¦æ¶ˆå¤±"
        result["issues"].append(f"æ¢¯åº¦èŒƒæ•°è¿‡å°: {result['norm']:.2e}")
        result["recommendations"].append("ä½¿ç”¨æ®‹å·®è¿æ¥ï¼ˆResNetï¼‰")
        result["recommendations"].append("ä½¿ç”¨ReLU/GELUæ¿€æ´»å‡½æ•°")
        result["recommendations"].append("æ£€æŸ¥æƒé‡åˆå§‹åŒ–ï¼ˆä½¿ç”¨Xavier/Heåˆå§‹åŒ–ï¼‰")
        result["recommendations"].append("æ·»åŠ BatchNorm")

    # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸
    elif result["norm"] > threshold_explode:
        result["status"] = "æ¢¯åº¦çˆ†ç‚¸"
        result["issues"].append(f"æ¢¯åº¦èŒƒæ•°è¿‡å¤§: {result['norm']:.2f}")
        result["recommendations"].append(
            "ä½¿ç”¨æ¢¯åº¦è£å‰ª: torch.nn.utils.clip_grad_norm_()"
        )
        result["recommendations"].append("é™ä½å­¦ä¹ ç‡ï¼ˆå½“å‰å­¦ä¹ ç‡Ã—0.1ï¼‰")
        result["recommendations"].append("ä½¿ç”¨BatchNormç¨³å®šè®­ç»ƒ")

    return result


def check_weight_stability(weights: torch.Tensor) -> Dict:
    """
    æ£€æŸ¥æƒé‡çš„ç¨³å®šæ€§

    Args:
        weights: æƒé‡å¼ é‡

    Returns:
        result: ç¨³å®šæ€§æ£€æŸ¥ç»“æœ
    """
    result = {
        "mean": 0.0,
        "std": 0.0,
        "norm": 0.0,
        "has_nan": False,
        "has_inf": False,
        "status": "æ­£å¸¸",
        "issues": [],
        "recommendations": [],
    }

    # æ£€æŸ¥NaNå’ŒInf
    if torch.isnan(weights).any():
        result["has_nan"] = True
        result["status"] = "åŒ…å«NaN"
        result["issues"].append("æƒé‡åŒ…å«NaN")
        result["recommendations"].append("é‡æ–°åˆå§‹åŒ–æ¨¡å‹")
        result["recommendations"].append("æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹æ˜¯å¦ç¨³å®š")
        return result

    if torch.isinf(weights).any():
        result["has_inf"] = True
        result["status"] = "åŒ…å«Inf"
        result["issues"].append("æƒé‡åŒ…å«Inf")
        result["recommendations"].append("è®­ç»ƒä¸ç¨³å®šå¯¼è‡´æƒé‡æº¢å‡º")
        result["recommendations"].append("é™ä½å­¦ä¹ ç‡")
        result["recommendations"].append("ä½¿ç”¨æƒé‡è¡°å‡ï¼ˆweight decayï¼‰")
        return result

    # ç»Ÿè®¡ä¿¡æ¯
    result["mean"] = weights.mean().item()
    result["std"] = weights.std().item()
    result["norm"] = weights.norm().item()

    # æ£€æŸ¥æƒé‡æ˜¯å¦åˆç†
    if result["std"] < 1e-6:
        result["status"] = "æœªåˆå§‹åŒ–æˆ–å¼‚å¸¸"
        result["issues"].append(f"æƒé‡æ ‡å‡†å·®è¿‡å°: {result['std']:.2e}")
        result["recommendations"].append("æ£€æŸ¥æƒé‡æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–")
        result["recommendations"].append("ä½¿ç”¨Xavieræˆ–Heåˆå§‹åŒ–")

    elif result["std"] > 10.0:
        result["status"] = "å¼‚å¸¸å¤§"
        result["issues"].append(f"æƒé‡æ ‡å‡†å·®è¿‡å¤§: {result['std']:.2f}")
        result["recommendations"].append("æƒé‡å¯èƒ½å¢é•¿å¤±æ§")
        result["recommendations"].append("æ·»åŠ æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰")
        result["recommendations"].append("é™ä½å­¦ä¹ ç‡")

    return result


def analyze_model_stability(
    model: nn.Module, input_data: torch.Tensor, num_steps: int = 10
) -> Dict:
    """
    åˆ†ææ•´ä¸ªæ¨¡å‹çš„æ•°å€¼ç¨³å®šæ€§

    Args:
        model: PyTorchæ¨¡å‹
        input_data: è¾“å…¥æ•°æ®
        num_steps: æ¨¡æ‹Ÿè®­ç»ƒæ­¥æ•°

    Returns:
        result: ç¨³å®šæ€§åˆ†æç»“æœ
    """
    model.train()

    layers_info = []

    # æ³¨å†Œhookæ”¶é›†æ¿€æ´»å€¼å’Œæ¢¯åº¦
    activations = {}
    gradients = {}

    def get_activation(name):
        def hook(module, input, output):
            if isinstance(output, torch.Tensor):
                activations[name] = output.detach()

        return hook

    def get_gradient(name):
        def hook(module, grad_input, grad_output):
            if isinstance(grad_output[0], torch.Tensor):
                gradients[name] = grad_output[0].detach()

        return hook

    # æ³¨å†Œhooks
    hooks = []
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.LayerNorm)):
            hooks.append(module.register_forward_hook(get_activation(name)))
            hooks.append(module.register_backward_hook(get_gradient(name)))

    # æ¨¡æ‹Ÿå¤šæ­¥è®­ç»ƒ
    for step in range(num_steps):
        model.zero_grad()

        # å‰å‘ä¼ æ’­
        output = model(input_data)

        # æ„é€ æŸå¤±
        target = torch.randn_like(output)
        loss = ((output - target) ** 2).mean()

        # åå‘ä¼ æ’­
        loss.backward()

    # ç§»é™¤hooks
    for hook in hooks:
        hook.remove()

    # åˆ†ææ¯ä¸€å±‚
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            info = LayerStabilityInfo(name, type(module).__name__)

            # æ£€æŸ¥æ¿€æ´»å€¼
            if name in activations:
                act_result = check_activation_stability(activations[name])
                info.activation_mean = act_result["mean"]
                info.activation_std = act_result["std"]
                info.activation_min = act_result["min"]
                info.activation_max = act_result["max"]
                info.activation_range = act_result["range"]
                info.activation_status = act_result["status"]
                info.issues.extend(act_result["issues"])
                info.recommendations.extend(act_result["recommendations"])

            # æ£€æŸ¥æ¢¯åº¦
            if name in gradients:
                grad_result = check_gradient_stability(gradients[name])
                info.gradient_mean = grad_result["mean"]
                info.gradient_std = grad_result["std"]
                info.gradient_norm = grad_result["norm"]
                info.gradient_max = grad_result["max"]
                info.gradient_status = grad_result["status"]
                info.issues.extend(grad_result["issues"])
                info.recommendations.extend(grad_result["recommendations"])

            # æ£€æŸ¥æƒé‡
            if hasattr(module, "weight") and module.weight is not None:
                weight_result = check_weight_stability(module.weight)
                info.weight_mean = weight_result["mean"]
                info.weight_std = weight_result["std"]
                info.weight_norm = weight_result["norm"]
                info.weight_status = weight_result["status"]
                info.issues.extend(weight_result["issues"])
                info.recommendations.extend(weight_result["recommendations"])

            layers_info.append(info)

    # æ±‡æ€»é—®é¢˜
    total_issues = sum(len(info.issues) for info in layers_info)
    problem_layers = [info for info in layers_info if len(info.issues) > 0]

    # åˆ†ç±»é—®é¢˜
    gradient_vanish_layers = [
        info for info in layers_info if info.gradient_status == "æ¢¯åº¦æ¶ˆå¤±"
    ]
    gradient_explode_layers = [
        info for info in layers_info if info.gradient_status == "æ¢¯åº¦çˆ†ç‚¸"
    ]
    activation_issue_layers = [
        info for info in layers_info if info.activation_status not in ["æ­£å¸¸", "æœªæ£€æµ‹"]
    ]

    result = {
        "layers": layers_info,
        "summary": {
            "total_layers": len(layers_info),
            "total_issues": total_issues,
            "problem_layers": len(problem_layers),
            "gradient_vanish_count": len(gradient_vanish_layers),
            "gradient_explode_count": len(gradient_explode_layers),
            "activation_issue_count": len(activation_issue_layers),
        },
        "problem_layers": problem_layers,
        "gradient_vanish_layers": gradient_vanish_layers,
        "gradient_explode_layers": gradient_explode_layers,
        "activation_issue_layers": activation_issue_layers,
    }

    return result


if __name__ == "__main__":
    print("=" * 60)
    print("æ•°å€¼ç¨³å®šæ€§åˆ†æå™¨æµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯•æ¿€æ´»å€¼æ£€æŸ¥
    print("\n### æ¿€æ´»å€¼ç¨³å®šæ€§æµ‹è¯• ###")

    # æ­£å¸¸æ¿€æ´»å€¼
    normal_act = torch.randn(100) * 0.5
    result = check_activation_stability(normal_act)
    print(
        f"æ­£å¸¸æ¿€æ´»å€¼: çŠ¶æ€={result['status']}, mean={result['mean']:.4f}, std={result['std']:.4f}"
    )

    # å¼‚å¸¸å¤§çš„æ¿€æ´»å€¼
    large_act = torch.randn(100) * 100
    result = check_activation_stability(large_act)
    print(f"å¼‚å¸¸å¤§æ¿€æ´»å€¼: çŠ¶æ€={result['status']}, é—®é¢˜æ•°={len(result['issues'])}")

    # æµ‹è¯•æ¢¯åº¦æ£€æŸ¥
    print("\n### æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯• ###")

    # æ­£å¸¸æ¢¯åº¦
    normal_grad = torch.randn(100) * 0.1
    result = check_gradient_stability(normal_grad)
    print(f"æ­£å¸¸æ¢¯åº¦: çŠ¶æ€={result['status']}, norm={result['norm']:.4f}")

    # æ¢¯åº¦æ¶ˆå¤±
    vanish_grad = torch.randn(100) * 1e-8
    result = check_gradient_stability(vanish_grad)
    print(f"æ¢¯åº¦æ¶ˆå¤±: çŠ¶æ€={result['status']}, norm={result['norm']:.2e}")

    # æ¢¯åº¦çˆ†ç‚¸
    explode_grad = torch.randn(100) * 100
    result = check_gradient_stability(explode_grad)
    print(f"æ¢¯åº¦çˆ†ç‚¸: çŠ¶æ€={result['status']}, norm={result['norm']:.2f}")

    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)


# ==================== Phase 3 æ–°å¢åŠŸèƒ½ ====================


def detect_gradient_flow_realtime(
    model: nn.Module, sample_input: torch.Tensor, loss_fn: Optional[nn.Module] = None
) -> Dict[str, Any]:
    """
    å®æ—¶æ£€æµ‹æ¢¯åº¦æµåŠ¨æƒ…å†µ

    å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        sample_input: æ ·æœ¬è¾“å…¥
        loss_fn: æŸå¤±å‡½æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨è¾“å‡ºçš„sumï¼‰

    è¿”å›:
        åŒ…å«æ¢¯åº¦ç»Ÿè®¡å’Œè¯Šæ–­ä¿¡æ¯çš„å­—å…¸
    """
    model.train()

    # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
    model.zero_grad()

    # å‰å‘ä¼ æ’­
    output = model(sample_input)

    # è®¡ç®—æŸå¤±
    if loss_fn is not None:
        if output.dim() > 1 and output.size(-1) > 1:
            # åˆ†ç±»ä»»åŠ¡ï¼Œåˆ›å»ºå‡æ ‡ç­¾
            target = torch.zeros(output.size(0), dtype=torch.long)
            loss = loss_fn(output, target)
        else:
            loss = output.sum()
    else:
        loss = output.sum()

    # åå‘ä¼ æ’­
    loss.backward()

    # æ”¶é›†æ¢¯åº¦ä¿¡æ¯
    gradient_info = {}
    gradient_norms = {}
    layer_gradients = {}

    for name, param in model.named_parameters():
        if param.grad is not None:
            grad = param.grad.detach()
            grad_norm = grad.norm().item()
            grad_mean = grad.mean().item()
            grad_std = grad.std().item()
            grad_max = grad.abs().max().item()

            gradient_norms[name] = grad_norm
            layer_gradients[name] = {
                "norm": grad_norm,
                "mean": grad_mean,
                "std": grad_std,
                "max": grad_max,
                "shape": tuple(grad.shape),
                "has_nan": torch.isnan(grad).any().item(),
                "has_inf": torch.isinf(grad).any().item(),
            }

    # æ£€æµ‹æ¢¯åº¦é—®é¢˜
    vanishing_threshold = 1e-7
    exploding_threshold = 100.0

    vanishing_layers = {
        k: v for k, v in gradient_norms.items() if v < vanishing_threshold and v > 0
    }

    exploding_layers = {
        k: v for k, v in gradient_norms.items() if v > exploding_threshold
    }

    nan_inf_layers = {
        k: v for k, v in layer_gradients.items() if v["has_nan"] or v["has_inf"]
    }

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if gradient_norms:
        grad_norms_list = list(gradient_norms.values())
        gradient_info["statistics"] = {
            "mean_norm": np.mean(grad_norms_list),
            "std_norm": np.std(grad_norms_list),
            "min_norm": np.min(grad_norms_list),
            "max_norm": np.max(grad_norms_list),
            "median_norm": np.median(grad_norms_list),
        }
    else:
        gradient_info["statistics"] = {}

    # è¯Šæ–­ç»“æœ
    gradient_info["all_gradients"] = layer_gradients
    gradient_info["gradient_norms"] = gradient_norms
    gradient_info["vanishing"] = vanishing_layers
    gradient_info["exploding"] = exploding_layers
    gradient_info["nan_inf"] = nan_inf_layers
    gradient_info["healthy"] = (
        len(vanishing_layers) == 0
        and len(exploding_layers) == 0
        and len(nan_inf_layers) == 0
    )

    # ç”Ÿæˆå»ºè®®
    recommendations = []

    if vanishing_layers:
        recommendations.append(
            {
                "issue": "æ¢¯åº¦æ¶ˆå¤±",
                "affected_layers": list(vanishing_layers.keys()),
                "severity": "high",
                "suggestions": [
                    "ä½¿ç”¨ ReLU æˆ– LeakyReLU æ¿€æ´»å‡½æ•°",
                    "ä½¿ç”¨æ®‹å·®è¿æ¥ï¼ˆResNetï¼‰",
                    "ä½¿ç”¨ BatchNorm æˆ– LayerNorm",
                    "å‡å°ç½‘ç»œæ·±åº¦",
                    "ä½¿ç”¨ Xavier/He åˆå§‹åŒ–",
                ],
            }
        )

    if exploding_layers:
        recommendations.append(
            {
                "issue": "æ¢¯åº¦çˆ†ç‚¸",
                "affected_layers": list(exploding_layers.keys()),
                "severity": "critical",
                "suggestions": [
                    "é™ä½å­¦ä¹ ç‡",
                    "ä½¿ç”¨æ¢¯åº¦è£å‰ª (gradient clipping)",
                    "ä½¿ç”¨ BatchNorm",
                    "æ£€æŸ¥æƒé‡åˆå§‹åŒ–",
                    "ä½¿ç”¨æ›´å°çš„æƒé‡åˆå§‹åŒ–æ ‡å‡†å·®",
                ],
            }
        )

    if nan_inf_layers:
        recommendations.append(
            {
                "issue": "æ•°å€¼æº¢å‡º (NaN/Inf)",
                "affected_layers": list(nan_inf_layers.keys()),
                "severity": "critical",
                "suggestions": [
                    "æ˜¾è‘—é™ä½å­¦ä¹ ç‡",
                    "ä½¿ç”¨æ¢¯åº¦è£å‰ª",
                    "æ£€æŸ¥æ•°æ®é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–ï¼‰",
                    "ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ",
                    "æ£€æŸ¥æŸå¤±å‡½æ•°å®ç°",
                ],
            }
        )

    gradient_info["recommendations"] = recommendations

    return gradient_info


def recommend_initialization(
    layer: nn.Module, layer_name: str = "", activation: str = "relu"
) -> Dict[str, Any]:
    """
    æ¨èåˆé€‚çš„åˆå§‹åŒ–æ–¹æ¡ˆ

    å‚æ•°:
        layer: ç¥ç»ç½‘ç»œå±‚
        layer_name: å±‚åç§°
        activation: æ¿€æ´»å‡½æ•°ç±»å‹

    è¿”å›:
        åˆå§‹åŒ–æ¨èä¿¡æ¯
    """
    layer_type = layer.__class__.__name__
    recommendation = {
        "layer_name": layer_name or layer_type,
        "layer_type": layer_type,
        "activation": activation,
    }

    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):
        # æ ¹æ®æ¿€æ´»å‡½æ•°æ¨èåˆå§‹åŒ–
        if activation.lower() in ["relu", "leakyrelu", "elu"]:
            recommendation["method"] = "kaiming_normal"
            recommendation["reason"] = "ReLUç³»åˆ—æ¿€æ´»å‡½æ•°çš„æœ€ä½³å®è·µ"
            recommendation["code"] = (
                f"nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')"
            )
            recommendation["description"] = "Heåˆå§‹åŒ–ï¼Œè€ƒè™‘äº†ReLUä¼šå°†è´Ÿå€¼ç½®é›¶çš„ç‰¹æ€§"

        elif activation.lower() in ["sigmoid", "tanh"]:
            recommendation["method"] = "xavier_uniform"
            recommendation["reason"] = "Sigmoid/Tanhçš„æœ€ä½³å®è·µ"
            recommendation["code"] = f"nn.init.xavier_uniform_(layer.weight)"
            recommendation["description"] = (
                "Xavieråˆå§‹åŒ–ï¼Œä¿æŒæ–¹å·®åœ¨å‰å‘å’Œåå‘ä¼ æ’­ä¸­ä¸€è‡´"
            )

        elif activation.lower() in ["gelu", "silu", "swish"]:
            recommendation["method"] = "xavier_normal"
            recommendation["reason"] = "å¹³æ»‘æ¿€æ´»å‡½æ•°çš„æ¨èæ–¹æ¡ˆ"
            recommendation["code"] = f"nn.init.xavier_normal_(layer.weight)"
            recommendation["description"] = "Xavieråˆå§‹åŒ–çš„æ­£æ€åˆ†å¸ƒç‰ˆæœ¬"

        else:
            recommendation["method"] = "default"
            recommendation["reason"] = "ä½¿ç”¨PyTorché»˜è®¤åˆå§‹åŒ–"
            recommendation["code"] = "# ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–"
            recommendation["description"] = "PyTorchçš„é»˜è®¤uniformåˆå§‹åŒ–"

        # åç½®åˆå§‹åŒ–
        if hasattr(layer, "bias") and layer.bias is not None:
            recommendation["bias_init"] = {
                "method": "zeros",
                "code": "nn.init.zeros_(layer.bias)",
                "reason": "åç½®é€šå¸¸åˆå§‹åŒ–ä¸º0",
            }

    elif isinstance(layer, (nn.BatchNorm2d, nn.BatchNorm1d)):
        recommendation["method"] = "ones_and_zeros"
        recommendation["reason"] = "BatchNormçš„æ ‡å‡†åˆå§‹åŒ–"
        recommendation["code"] = (
            "nn.init.ones_(layer.weight)\n" "nn.init.zeros_(layer.bias)"
        )
        recommendation["description"] = "weight(gamma)åˆå§‹åŒ–ä¸º1ï¼Œbias(beta)åˆå§‹åŒ–ä¸º0"

    elif isinstance(layer, (nn.LSTM, nn.GRU, nn.RNN)):
        recommendation["method"] = "orthogonal"
        recommendation["reason"] = "RNNçš„æœ€ä½³å®è·µ"
        recommendation["code"] = (
            "for name, param in layer.named_parameters():\n"
            "    if 'weight_ih' in name:\n"
            "        nn.init.xavier_uniform_(param)\n"
            "    elif 'weight_hh' in name:\n"
            "        nn.init.orthogonal_(param)"
        )
        recommendation["description"] = "è¾“å…¥æƒé‡ç”¨Xavierï¼Œéšè—æƒé‡ç”¨æ­£äº¤åˆå§‹åŒ–"

    else:
        recommendation["method"] = "not_applicable"
        recommendation["reason"] = "è¯¥å±‚ç±»å‹é€šå¸¸ä¸éœ€è¦ç‰¹æ®Šåˆå§‹åŒ–"
        recommendation["code"] = "# ä¸éœ€è¦ç‰¹æ®Šåˆå§‹åŒ–"
        recommendation["description"] = f"{layer_type}å±‚é€šå¸¸ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–å³å¯"

    return recommendation


def predict_peak_memory(
    model: nn.Module,
    input_shape: Tuple[int, ...],
    batch_size: int = 1,
    optimizer_type: str = "adam",
    dtype: torch.dtype = torch.float32,
) -> Dict[str, Any]:
    """
    é¢„æµ‹è®­ç»ƒæ—¶çš„å³°å€¼å†…å­˜ä½¿ç”¨

    å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        input_shape: è¾“å…¥å½¢çŠ¶ï¼ˆä¸åŒ…å«batchç»´åº¦ï¼‰
        batch_size: æ‰¹å¤§å°
        optimizer_type: ä¼˜åŒ–å™¨ç±»å‹ ('sgd', 'adam', 'adamw')
        dtype: æ•°æ®ç±»å‹

    è¿”å›:
        å†…å­˜é¢„æµ‹ä¿¡æ¯
    """
    bytes_per_element = {
        torch.float32: 4,
        torch.float16: 2,
        torch.float64: 8,
        torch.int32: 4,
        torch.int64: 8,
    }.get(dtype, 4)

    # è®¡ç®—å‚æ•°å†…å­˜
    param_count = sum(p.numel() for p in model.parameters())
    param_memory = param_count * bytes_per_element / (1024**2)  # MB

    # è®¡ç®—æ¢¯åº¦å†…å­˜ï¼ˆä¸å‚æ•°ç›¸åŒï¼‰
    gradient_memory = param_memory

    # è®¡ç®—ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜
    if optimizer_type.lower() in ["adam", "adamw"]:
        # Adam éœ€è¦ä¸¤ä¸ªçŠ¶æ€ï¼šmomentum å’Œ varianceï¼ˆæ¯ä¸ªä¸å‚æ•°å¤§å°ç›¸åŒï¼‰
        optimizer_memory = param_memory * 2
    elif optimizer_type.lower() == "sgd":
        # SGD with momentum éœ€è¦ä¸€ä¸ªçŠ¶æ€
        optimizer_memory = param_memory
    else:
        optimizer_memory = 0

    # ä¼°ç®—å‰å‘ä¼ æ’­æ¿€æ´»å€¼å†…å­˜
    # ç®€åŒ–ä¼°ç®—ï¼šå‡è®¾æ¯å±‚çš„æ¿€æ´»å€¼å¤§å°é€æ¸å‡å°
    try:
        # åˆ›å»ºæ ·æœ¬è¾“å…¥
        full_input_shape = (batch_size,) + input_shape
        sample_input = torch.randn(full_input_shape, dtype=dtype)

        # ç»Ÿè®¡æ¿€æ´»å€¼
        activation_memory = 0
        hooks = []

        def hook_fn(module, input, output):
            nonlocal activation_memory
            if isinstance(output, torch.Tensor):
                activation_memory += output.numel() * bytes_per_element / (1024**2)

        # æ³¨å†Œhooks
        for module in model.modules():
            if len(list(module.children())) == 0:  # åªå¤„ç†å¶å­æ¨¡å—
                hooks.append(module.register_forward_hook(hook_fn))

        # å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            _ = model(sample_input)

        # æ¸…ç†hooks
        for hook in hooks:
            hook.remove()

    except Exception as e:
        # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨ç»éªŒå…¬å¼
        input_elements = batch_size * np.prod(input_shape)
        activation_memory = (
            input_elements * bytes_per_element * 10 / (1024**2)
        )  # ç²—ç•¥ä¼°è®¡

    # è®¡ç®—åå‘ä¼ æ’­å†…å­˜ï¼ˆé€šå¸¸æ˜¯å‰å‘çš„2-3å€ï¼‰
    backward_memory = activation_memory * 2.5

    # å³°å€¼å†…å­˜ = å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ + å‰å‘æ¿€æ´» + åå‘æ¿€æ´»
    peak_memory = (
        param_memory
        + gradient_memory
        + optimizer_memory
        + activation_memory
        + backward_memory
    )

    memory_info = {
        "total_peak": peak_memory,
        "breakdown": {
            "parameters": param_memory,
            "gradients": gradient_memory,
            "optimizer_states": optimizer_memory,
            "forward_activations": activation_memory,
            "backward_activations": backward_memory,
        },
        "parameter_count": param_count,
        "batch_size": batch_size,
        "optimizer_type": optimizer_type,
        "dtype": str(dtype),
        "bytes_per_element": bytes_per_element,
    }

    # ç”Ÿæˆå»ºè®®
    recommendations = []

    if peak_memory > 1000:  # > 1GB
        recommendations.append(
            {
                "issue": "å†…å­˜å ç”¨è¾ƒå¤§",
                "severity": "medium",
                "suggestions": [
                    f"å‡å°æ‰¹å¤§å°ï¼ˆå½“å‰: {batch_size}ï¼‰",
                    "ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯",
                    "ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰",
                    "ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰",
                ],
            }
        )

    if peak_memory > 4000:  # > 4GB
        recommendations.append(
            {
                "issue": "å†…å­˜å ç”¨å¾ˆå¤§",
                "severity": "high",
                "suggestions": [
                    "å¼ºçƒˆå»ºè®®å‡å°æ‰¹å¤§å°",
                    "ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¯èŠ‚çœ50%å†…å­˜ï¼‰",
                    "è€ƒè™‘ä½¿ç”¨æ¨¡å‹å¹¶è¡Œ",
                    "ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹",
                ],
            }
        )

    if optimizer_type.lower() in ["adam", "adamw"]:
        recommendations.append(
            {
                "issue": "Adamä¼˜åŒ–å™¨å†…å­˜å¼€é”€å¤§",
                "severity": "info",
                "suggestions": [
                    f"Adaméœ€è¦2å€å‚æ•°å†…å­˜å­˜å‚¨çŠ¶æ€ï¼ˆ{optimizer_memory:.1f} MBï¼‰",
                    "å¯ä»¥è€ƒè™‘ä½¿ç”¨SGDï¼ˆå†…å­˜å‡åŠï¼‰",
                    "æˆ–ä½¿ç”¨Adafactorç­‰å†…å­˜ä¼˜åŒ–çš„ä¼˜åŒ–å™¨",
                ],
            }
        )

    memory_info["recommendations"] = recommendations

    # ç”Ÿæˆä¸åŒé…ç½®ä¸‹çš„å†…å­˜å¯¹æ¯”ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…é€’å½’ï¼‰
    memory_info["memory_comparison"] = {
        "current": peak_memory,
        "half_batch": peak_memory * 0.5 if batch_size > 1 else peak_memory,
        "mixed_precision": peak_memory * 0.5,  # æ··åˆç²¾åº¦çº¦èŠ‚çœ50%
        "sgd_optimizer": peak_memory
        - optimizer_memory
        + param_memory,  # SGDåªéœ€1å€å‚æ•°å†…å­˜
    }

    return memory_info


def analyze_numerical_stability(
    model: nn.Module, sample_input: torch.Tensor
) -> Dict[str, Any]:
    """
    ç»¼åˆåˆ†ææ•°å€¼ç¨³å®šæ€§

    ç»“åˆæ¢¯åº¦æ£€æµ‹ã€åˆå§‹åŒ–æ¨èå’Œå†…å­˜é¢„æµ‹

    å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        sample_input: æ ·æœ¬è¾“å…¥

    è¿”å›:
        ç»¼åˆåˆ†æç»“æœ
    """
    analysis = {}

    # 1. æ¢¯åº¦æµåŠ¨æ£€æµ‹
    try:
        gradient_info = detect_gradient_flow_realtime(model, sample_input)
        analysis["gradient_flow"] = gradient_info
    except Exception as e:
        analysis["gradient_flow"] = {"error": str(e)}

    # 2. åˆå§‹åŒ–æ¨è
    initialization_recommendations = []
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.LSTM)):
            rec = recommend_initialization(module, name, activation="relu")
            initialization_recommendations.append(rec)

    analysis["initialization"] = initialization_recommendations

    # 3. å†…å­˜é¢„æµ‹
    try:
        input_shape = tuple(sample_input.shape[1:])  # å»æ‰batchç»´åº¦
        batch_size = sample_input.shape[0]
        memory_info = predict_peak_memory(model, input_shape, batch_size)
        analysis["memory"] = memory_info
    except Exception as e:
        analysis["memory"] = {"error": str(e)}

    # 4. æ•´ä½“å¥åº·è¯„åˆ†
    health_score = 100
    issues = []

    if "gradient_flow" in analysis and not analysis["gradient_flow"].get(
        "healthy", True
    ):
        health_score -= 30
        issues.append("æ¢¯åº¦æµåŠ¨å¼‚å¸¸")

    if "memory" in analysis:
        peak_mem = analysis["memory"].get("total_peak", 0)
        if peak_mem > 4000:
            health_score -= 20
            issues.append("å†…å­˜å ç”¨è¿‡å¤§")
        elif peak_mem > 1000:
            health_score -= 10
            issues.append("å†…å­˜å ç”¨è¾ƒå¤§")

    analysis["overall"] = {
        "health_score": max(0, health_score),
        "status": (
            "healthy"
            if health_score >= 80
            else ("warning" if health_score >= 60 else "critical")
        ),
        "issues": issues,
    }

    return analysis


# ==================== è¾…åŠ©å‡½æ•° ====================


def format_memory_size(size_mb: float) -> str:
    """æ ¼å¼åŒ–å†…å­˜å¤§å°æ˜¾ç¤º"""
    if size_mb < 1:
        return f"{size_mb * 1024:.1f} KB"
    elif size_mb < 1024:
        return f"{size_mb:.1f} MB"
    else:
        return f"{size_mb / 1024:.2f} GB"


def get_gradient_health_emoji(gradient_info: Dict[str, Any]) -> str:
    """è·å–æ¢¯åº¦å¥åº·çŠ¶æ€çš„emoji"""
    if gradient_info.get("healthy", False):
        return "âœ…"
    elif gradient_info.get("nan_inf"):
        return "ğŸ”´"
    elif gradient_info.get("exploding"):
        return "ğŸŸ "
    elif gradient_info.get("vanishing"):
        return "ğŸŸ¡"
    else:
        return "âšª"
